Comprehensive Plan for a Dynamic Knowledge Corpus Research Assistant
1. System Architecture Overview
CopyUser Interface <---> Orchestrator <---> Intelligence Layer
      ^                   ^                    ^
      |                   |                    |
      v                   v                    v
   Web UI/API      Context Manager      Document Processors
                         ^                    ^
                         |                    |
                         v                    v
                   Memory System         Knowledge Base
                                              ^
                                              |
                                              v
                                        Vector Database
2. Core Components and Responsibilities
A. Orchestrator

Central coordination of all system components
Query analysis and execution strategy determination
Dynamic parameter adjustment based on query type and system state
Conversation flow management and context tracking

B. Intelligence Layer

Research Agent: Multi-step information discovery and knowledge retrieval
Parameter Optimizer: Dynamic adjustment of search and processing parameters
Document Analyzer: Content-aware metadata extraction and classification
Response Generator: Converting research findings into coherent answers

C. Knowledge Base

Vector Database: Similarity search for relevant information
Document Processing Pipeline: Converting raw documents to searchable chunks
Metadata System: Rich classification of documents and chunks
Multi-level Chunk Storage: Hierarchical representation of content

D. Memory System

Conversation History: Tracking user interactions
Knowledge Context: Managing active information relevant to current queries
Notes Repository: System observations about documents and queries
Performance Metrics: Tracking system behavior for optimization

E. User Interface

Chat Interface: Conversational access to the system
Admin Dashboard: System monitoring and control
Feedback Mechanisms: Learning from user interactions

3. Document Processing Pipeline
3.1 Document Analysis and Metadata Extraction
pythonCopyclass DocumentAnalyzer:
    """
    Analyzes documents to extract metadata and determine optimal processing parameters.
    Uses LLMs to understand document structure and content characteristics.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()
        self.metadata_schema = self._load_metadata_schema()
        self.analysis_notes = {}

    async def analyze_document(self, document_path, document_content=None):
        """Analyze a document to extract metadata and determine processing parameters."""
        # Load content if not provided
        if document_content is None:
            document_content = self._load_document(document_path)

        # Take representative sample for analysis
        sample = self._create_representative_sample(document_content)

        # Create analysis prompt with JSON form
        analysis_prompt = self._create_analysis_prompt(document_path, sample)

        # Get LLM analysis
        analysis_result = await self.llm_service.generate_completion(
            prompt=analysis_prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        # Parse the response and extract processing parameters
        analysis = self._parse_analysis_result(analysis_result)

        # Store analysis notes
        if "notes" in analysis:
            self.analysis_notes[document_path] = analysis["notes"]

        return {
            "metadata": analysis.get("metadata", {}),
            "processing_parameters": analysis.get("processing_parameters", {}),
            "content_type": analysis.get("content_type", "unknown"),
            "structural_analysis": analysis.get("structural_analysis", {})
        }

    def _create_analysis_prompt(self, document_path, sample):
        """Create a prompt for document analysis with JSON form."""
        filename = os.path.basename(document_path)

        return f"""
        You are a document analyzer specialized in understanding text corpora. Analyze this document sample and extract metadata.

        DOCUMENT: {filename}
        SAMPLE CONTENT:
        {sample}

        Provide your analysis in the following JSON format:

        {{
            "metadata": {{
                "title": "Document title",
                "author": "Author name if identified",
                "publication_year": "Year if identified, otherwise null",
                "language": "Primary language",
                "content_type": "Type of text (e.g., academic, legal, religious, etc.)",
                "category": "Subject category",
                "domain": "Knowledge domain"
            }},
            "structural_analysis": {{
                "has_chapters": true/false,
                "has_sections": true/false,
                "has_paragraphs": true/false,
                "structural_patterns": "Description of structural patterns in the text"
            }},
            "processing_parameters": {{
                "recommended_chunk_size": Integer (100-1000),
                "recommended_chunk_overlap": Integer (10-100),
                "chunking_strategy": "paragraph"|"section"|"sentence",
                "key_terms_to_preserve": ["List", "of", "important", "terms"],
                "special_handling_required": true/false
            }},
            "notes": "Any additional observations or recommendations for processing this document"
        }}

        Analyze the sample thoroughly to identify as many metadata fields as possible. If you cannot determine a value with confidence, use null.
        """
3.2 Content Chunking with Structural Awareness
pythonCopyclass ChunkProcessor:
    """
    Processes documents into chunks with LLM-enhanced metadata extraction
    for each chunk.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()
        self.chunk_notes = {}

    async def process_document(self, document_path, content, document_analysis):
        """Process a document into chunks with extracted metadata."""
        # Extract processing parameters
        params = document_analysis["processing_parameters"]
        chunk_size = params.get("recommended_chunk_size", 512)
        overlap = params.get("recommended_chunk_overlap", 50)
        strategy = params.get("chunking_strategy", "paragraph")

        # Create initial chunks based on strategy
        raw_chunks = self._create_raw_chunks(content, strategy, chunk_size, overlap)

        # Process chunks in parallel with asyncio tasks
        import asyncio

        # Create tasks for all chunks
        tasks = []
        for i, chunk in enumerate(raw_chunks):
            task = self._process_chunk(
                chunk_text=chunk,
                chunk_index=i,
                document_path=document_path,
                document_analysis=document_analysis
            )
            tasks.append(task)

        # Process in batches to avoid overwhelming API
        batch_size = 5
        processed_chunks = []

        for i in range(0, len(tasks), batch_size):
            batch_tasks = tasks[i:i+batch_size]
            batch_results = await asyncio.gather(*batch_tasks)
            processed_chunks.extend(batch_results)

            # Small delay between batches
            await asyncio.sleep(0.5)

        return processed_chunks

    async def _process_chunk(self, chunk_text, chunk_index, document_path, document_analysis):
        """Process a single chunk to extract metadata and create Level 0 chunk."""
        # Create a document ID from path
        doc_id = f"doc_{hash(document_path) % 10000}"

        # Create chunk ID
        chunk_id = f"{doc_id}_chunk_{chunk_index}"

        # Get structural analysis from document
        structure = document_analysis.get("structural_analysis", {})

        # Create metadata extraction prompt with JSON form
        prompt = self._create_chunk_metadata_prompt(
            chunk_text=chunk_text,
            document_metadata=document_analysis["metadata"],
            structure=structure
        )

        # Get LLM analysis
        metadata_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.1,
            response_format={"type": "json_object"}
        )

        # Parse the response
        metadata = self._parse_metadata_result(metadata_result)

        # Store notes if present
        if "notes" in metadata:
            self.chunk_notes[chunk_id] = metadata["notes"]
            del metadata["notes"]

        # Create the chunk object
        chunk = {
            "id": chunk_id,
            "text": chunk_text,
            "metadata": {
                # Basic metadata
                "doc_id": doc_id,
                "document_path": os.path.relpath(document_path),
                "document_name": os.path.basename(document_path),
                "chunk_id": chunk_id,
                "chunk_level": 0,
                "chunk_index": chunk_index,
                "contained_chunks": [chunk_id],

                # Position in document (approximate)
                "position": {
                    "start": content.find(chunk_text) if chunk_text in content else -1,
                    "end": content.find(chunk_text) + len(chunk_text) if chunk_text in content else -1
                },

                # Document metadata
                "document_type": document_analysis["metadata"].get("content_type", "unknown"),
                "document_language": document_analysis["metadata"].get("language", "unknown"),

                # LLM-extracted chunk-specific metadata
                **metadata
            }
        }

        return chunk
3.3 Multi-level Chunk Building
pythonCopyclass MultilevelChunkBuilder:
    """
    Builds higher-level chunks from Level 0 chunks with dynamic parameter adjustment.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()
        self.building_notes = {}

    async def build_multilevel_chunks(self, level0_chunks, max_embedding_tokens):
        """Build higher-level chunks from Level 0 chunks with dynamic parameters."""
        # Analyze chunk characteristics to determine optimal parameters
        chunking_params = await self._determine_optimal_parameters(level0_chunks[:20])

        # Store all chunks by level
        all_chunks = {0: level0_chunks}

        # Process each level
        current_level = 0
        while True:
            current_level += 1

            # Check if we've reached the maximum level from parameters
            if current_level > chunking_params["max_levels"]:
                break

            # Get chunks from previous level
            prev_level_chunks = all_chunks[current_level - 1]

            # If we have very few chunks at previous level, stop building
            if len(prev_level_chunks) < chunking_params["min_chunks_to_combine"]:
                break

            # Build chunks for this level
            current_level_chunks = await self._build_level_chunks(
                prev_level_chunks,
                current_level,
                max_embedding_tokens,
                chunking_params
            )

            # If no new chunks were created, stop
            if not current_level_chunks:
                break

            # Store chunks for this level
            all_chunks[current_level] = current_level_chunks

            # Update notes
            self.building_notes[f"level_{current_level}"] = {
                "chunks_created": len(current_level_chunks),
                "average_size": sum(len(c["text"]) for c in current_level_chunks) / len(current_level_chunks),
                "parameters_used": chunking_params
            }

        return all_chunks

    async def _determine_optimal_parameters(self, sample_chunks):
        """Use LLM to determine optimal parameters for multilevel chunking."""
        # Create a sample of chunk texts and their characteristics
        sample_text = ""
        for i, chunk in enumerate(sample_chunks[:5]):
            sample_text += f"Chunk {i+1} (length: {len(chunk['text'])} chars):\n{chunk['text'][:200]}...\n\n"

        # Calculate some statistics
        avg_chunk_length = sum(len(c["text"]) for c in sample_chunks) / len(sample_chunks)
        max_chunk_length = max(len(c["text"]) for c in sample_chunks)
        min_chunk_length = min(len(c["text"]) for c in sample_chunks)

        # Create the prompt with JSON form
        prompt = f"""
        You are a chunking optimization expert. Analyze these sample chunks and recommend optimal parameters.

        CHUNK STATISTICS:
        - Number of chunks: {len(sample_chunks)}
        - Average chunk length: {avg_chunk_length:.2f} characters
        - Maximum chunk length: {max_chunk_length} characters
        - Minimum chunk length: {min_chunk_length} characters

        SAMPLE CHUNKS:
        {sample_text}

        Based on these samples, determine the optimal parameters for building multi-level chunks.
        Provide your recommendation in the following JSON format:

        {{
            "chunks_per_level": Number of chunks to combine per level (2-5),
            "max_levels": Maximum levels to build (1-10),
            "overlap_strategy": "none"|"adjacent"|"sliding_window",
            "min_chunks_to_combine": Minimum chunks needed to build a higher level (2-5),
            "prioritize_semantic_coherence": true/false,
            "coherence_boundary_check": true/false,
            "notes": "Explanation of your recommendations"
        }}

        Consider the nature of the text and importance of preserving semantic boundaries.
        """

        # Get LLM recommendation
        recommendation = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        # Parse the response
        params = self._parse_parameter_result(recommendation)

        # Store notes
        if "notes" in params:
            self.building_notes["parameter_determination"] = params["notes"]

        return params
4. Vector Search and Retrieval System
4.1 Embedding Service with Dynamic Batching
pythonCopyclass EmbeddingService:
    """
    Service for generating embeddings with dynamic optimization and note-taking.
    """

    def __init__(self, model="text-embedding-3-large", resource_monitor=None):
        self.model = model
        self.resource_monitor = resource_monitor or ResourceMonitor()
        self.stats = {
            "total_embeddings": 0,
            "total_tokens": 0,
            "avg_latency_ms": 0
        }
        self.notes = {}

    async def embed_documents(self, documents, batch_size=None, optimize=True):
        """Generate embeddings for documents with dynamic batching."""
        import time
        import asyncio
        from openai import AsyncOpenAI

        # Document ID extraction
        doc_ids = []
        doc_texts = []

        for doc in documents:
            if isinstance(doc, dict):
                doc_texts.append(doc["text"])
                doc_ids.append(doc["id"])
            else:
                # Assume Document object
                doc_texts.append(doc.page_content)
                doc_ids.append(doc.metadata.get("chunk_id", f"doc_{len(doc_ids)}"))

        # Determine optimal batch size
        if batch_size is None:
            if optimize and self.resource_monitor:
                # Get system resource state
                resource_state = self.resource_monitor.get_current_state()
                optimizations = self.resource_monitor.optimize_resources()
                batch_size = optimizations.get("embedding_batch_size", 500)

                # Record optimization notes
                self.notes["batch_optimization"] = (
                    f"Optimized batch size to {batch_size} based on "
                    f"CPU: {resource_state.get('cpu_usage')}%, "
                    f"Memory: {resource_state.get('memory_usage')}%, "
                    f"API usage: {resource_state.get('tokens_used_this_minute', 0)} tokens"
                )
            else:
                # Use model default
                model_info = await self.get_model_info()
                batch_size = model_info.get("recommended_batch_size", 1000)

        # Split into batches
        num_documents = len(doc_texts)
        num_batches = (num_documents + batch_size - 1) // batch_size

        # Create batches
        text_batches = [doc_texts[i:i+batch_size] for i in range(0, num_documents, batch_size)]
        id_batches = [doc_ids[i:i+batch_size] for i in range(0, num_documents, batch_size)]

        # Initialize OpenAI client
        client = AsyncOpenAI()

        # Process batches
        embeddings = {}

        for batch_idx, (text_batch, id_batch) in enumerate(zip(text_batches, id_batches)):
            # Record batch start time
            batch_start = time.time()

            # Check if we need to throttle due to rate limits
            if batch_idx > 0 and self.resource_monitor:
                resource_state = self.resource_monitor.get_current_state()
                if resource_state.get("throttle_embeddings", False):
                    await asyncio.sleep(resource_state.get("min_time_between_calls", 0.1))

            try:
                # Generate embeddings for this batch
                response = await client.embeddings.create(
                    model=self.model,
                    input=text_batch
                )

                # Calculate latency
                batch_latency = (time.time() - batch_start) * 1000

                # Extract embeddings
                batch_embeddings = [item.embedding for item in response.data]

                # Update embeddings dictionary
                for i, doc_id in enumerate(id_batch):
                    embeddings[doc_id] = batch_embeddings[i]

                # Update stats and record API call
                self._update_stats(response, batch_latency, len(text_batch))

            except Exception as e:
                # Handle errors with exponential backoff and batch splitting
                await self._handle_embedding_error(e, batch_idx, text_batch, id_batch, embeddings)

        return embeddings
4.2 Vector Database Integration
pythonCopyclass VectorDatabaseManager:
    """
    Manages interactions with the vector database.
    """

    def __init__(self, persist_directory, collection_name="knowledge_corpus"):
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.db = None
        self.embedding_function = None

    def initialize(self, embedding_function=None):
        """Initialize the vector database."""
        from langchain.vectorstores import Chroma
        from langchain.embeddings import OpenAIEmbeddings

        # Set embedding function
        self.embedding_function = embedding_function or OpenAIEmbeddings(model="text-embedding-3-large")

        # Check if database exists
        if os.path.exists(self.persist_directory):
            # Load existing database
            self.db = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embedding_function,
                collection_name=self.collection_name
            )
        else:
            # Create new database
            os.makedirs(self.persist_directory, exist_ok=True)
            self.db = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embedding_function,
                collection_name=self.collection_name
            )

    def add_documents(self, documents, embeddings=None, batch_size=500):
        """
        Add documents to the vector database.

        Args:
            documents: List of Document objects or dictionaries
            embeddings: Optional pre-computed embeddings
            batch_size: Batch size for adding documents
        """
        from langchain.schema import Document

        # Convert dictionaries to Document objects if needed
        docs = []
        for doc in documents:
            if isinstance(doc, dict):
                docs.append(Document(
                    page_content=doc["text"],
                    metadata=doc["metadata"]
                ))
            else:
                docs.append(doc)

        # Add documents in batches
        for i in range(0, len(docs), batch_size):
            end_idx = min(i + batch_size, len(docs))
            batch_docs = docs[i:end_idx]

            if embeddings:
                batch_embeddings = embeddings[i:end_idx]
                self.db.add_documents(
                    documents=batch_docs,
                    embeddings=batch_embeddings
                )
            else:
                self.db.add_documents(documents=batch_docs)

            print(f"Added {end_idx}/{len(docs)} documents to vector database")

        # Persist changes
        self.db.persist()

    async def similarity_search(self, query, top_k=5, threshold=0.7, filter_dict=None):
        """
        Perform a similarity search.

        Args:
            query: Search query
            top_k: Number of results to return
            threshold: Minimum similarity score
            filter_dict: Optional metadata filters

        Returns:
            List of search results with relevance scores
        """
        # Perform search
        results = self.db.similarity_search_with_relevance_scores(
            query,
            k=top_k,
            score_threshold=threshold,
            filter=filter_dict
        )

        return results
5. Research Agent Implementation
5.1 Core Research Agent
pythonCopyclass ResearchAgent:
    """
    Agent for conducting research in the knowledge corpus with dynamic parameter
    adjustment and note-taking capabilities.
    """

    def __init__(self, vector_db, llm_service=None, resource_monitor=None):
        self.vector_db = vector_db
        self.llm_service = llm_service or LLMService(default_model="gpt-4")
        self.resource_monitor = resource_monitor or ResourceMonitor()
        self.research_memory = {}
        self.notes = {}

    async def research(self, query, context=None, initial_params=None):
        """Conduct research with dynamic parameter adjustment."""
        # Generate a unique ID for this research session
        research_id = f"research_{int(time.time())}"

        # Determine initial parameters
        params = initial_params or await self._determine_parameters(query, context)

        # Record parameters for this research
        self.notes[f"{research_id}_params"] = params

        # Initial search
        initial_results = await self._perform_search(query, params)

        # Record initial results
        self.notes[f"{research_id}_initial_results_count"] = len(initial_results)

        # Analyze results to determine next steps
        analysis = await self._analyze_results(query, initial_results, params, context)

        # Record analysis
        self.notes[f"{research_id}_analysis"] = analysis

        # Check if we need further research
        if analysis["information_sufficiency"] >= params["sufficiency_threshold"] or not analysis["follow_up_queries"]:
            # Initial research is sufficient
            self.notes[f"{research_id}_conclusion"] = "Initial research sufficient"
            return self._prepare_final_results(research_id, initial_results, analysis, params)

        # Conduct iterative research
        all_results = initial_results.copy()
        seen_chunk_ids = {result.metadata["chunk_id"] for result in all_results}
        iterations = 1

        # Store in research memory
        self.research_memory[research_id] = {
            "query": query,
            "iterations": iterations,
            "all_results": all_results,
            "seen_chunk_ids": seen_chunk_ids,
            "params": params,
            "analysis": analysis
        }

        # Iterative deepening
        while (iterations < params["max_iterations"] and
               analysis["information_sufficiency"] < params["sufficiency_threshold"] and
               analysis["follow_up_queries"]):

            iterations += 1

            # Adjust parameters based on previous results
            adjusted_params = await self._adjust_parameters(params, analysis, iterations)

            # Record parameter adjustments
            self.notes[f"{research_id}_iteration_{iterations}_params"] = adjusted_params

            # Execute follow-up queries
            iteration_results = []
            for follow_up_query in analysis["follow_up_queries"]:
                # Execute query
                query_results = await self._perform_search(follow_up_query, adjusted_params)

                # Filter out already seen chunks
                new_results = [result for result in query_results
                              if result.metadata["chunk_id"] not in seen_chunk_ids]

                # Update seen chunks
                for result in new_results:
                    seen_chunk_ids.add(result.metadata["chunk_id"])

                # Add to iteration results
                iteration_results.extend(new_results)

            # Record iteration results
            self.notes[f"{research_id}_iteration_{iterations}_results_count"] = len(iteration_results)

            # If no new results, break
            if not iteration_results:
                self.notes[f"{research_id}_conclusion"] = "No new results found in iteration"
                break

            # Add new results to overall collection
            all_results.extend(iteration_results)

            # Analyze combined results
            analysis = await self._analyze_results(query, all_results, adjusted_params, context)

            # Record analysis
            self.notes[f"{research_id}_iteration_{iterations}_analysis"] = analysis

            # Update research memory
            self.research_memory[research_id].update({
                "iterations": iterations,
                "all_results": all_results,
                "seen_chunk_ids": seen_chunk_ids,
                "latest_analysis": analysis
            })

        # Final conclusion
        self.notes[f"{research_id}_conclusion"] = (
            f"Research completed after {iterations} iterations with "
            f"information sufficiency: {analysis['information_sufficiency']}"
        )

        # Prepare final results
        return self._prepare_final_results(research_id, all_results, analysis, params)
5.2 Parameter Determination and Adjustment
pythonCopy    async def _determine_parameters(self, query, context=None):
        """Determine optimal research parameters for the query."""
        # Create prompt for parameter determination with JSON form
        context_text = self._format_context(context) if context else "No previous context"

        prompt = f"""
        You are a research strategist for a knowledge corpus question answering system.
        Determine the optimal research parameters for this query.

        QUERY: {query}

        CONVERSATION CONTEXT:
        {context_text}

        Based on this query, determine the optimal research approach.
        Provide your recommendations in the following JSON format:

        {{
            "query_type": "factual"|"interpretive"|"comparative"|"historical"|"technical",
            "complexity": 1-10 (1 = simple, 10 = complex),
            "expected_search_breadth": 1-10 (1 = narrow, 10 = broad),
            "top_k": Number of initial results to retrieve (5-30),
            "max_iterations": Maximum research iterations (1-5),
            "similarity_threshold": Minimum similarity score (0.0-1.0),
            "sufficiency_threshold": Information sufficiency threshold (0.0-1.0),
            "metadata_filters": {{
                Optional metadata filters to apply
            }},
            "notes": "Explanation of your parameter choices"
        }}

        Choose values that will lead to optimal research efficiency and accuracy.
        """

        # Get LLM recommendation
        param_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"},
            note_id=f"param_determination_{int(time.time())}"
        )

        # Parse the response
        try:
            parameters = json.loads(param_result)
        except json.JSONDecodeError:
            # Fallback to default parameters
            parameters = {
                "query_type": "factual",
                "complexity": 5,
                "expected_search_breadth": 5,
                "top_k": 10,
                "max_iterations": 3,
                "similarity_threshold": 0.7,
                "sufficiency_threshold": 0.8,
                "metadata_filters": {},
                "notes": "Using default parameters due to parsing failure."
            }

        return parameters

    async def _adjust_parameters(self, current_params, analysis, iteration):
        """Adjust search parameters based on analysis."""
        # Start with current parameters
        adjusted_params = current_params.copy()

        # Create prompt for parameter adjustment with JSON form
        prompt = f"""
        You are a research optimization expert. Adjust search parameters based on current analysis results.

        CURRENT PARAMETERS:
        {json.dumps(current_params, indent=2)}

        ANALYSIS RESULTS:
        {json.dumps(analysis, indent=2)}

        CURRENT ITERATION: {iteration}

        Based on the analysis results, recommend adjusted parameters for the next search iteration.
        Provide your recommendations in the following JSON format:

        {{
            "top_k": Number of results to retrieve (5-30),
            "similarity_threshold": Minimum similarity score (0.0-1.0),
            "metadata_filters": {{
                Optional metadata filters to apply based on analysis
            }},
            "parameter_adjustments": [
                "List of specific parameter changes made and why"
            ],
            "notes": "Explanation of your adjustment strategy"
        }}

        Focus on addressing the information gaps identified in the analysis.
        """

        # Get LLM recommendation
        adjustment_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"},
            note_id=f"param_adjustment_iter_{iteration}_{int(time.time())}"
        )

        # Apply adjustments
        adjustments = self._parse_adjustment_result(adjustment_result)
        self._apply_parameter_adjustments(adjusted_params, adjustments, iteration)

        return adjusted_params
5.3 Result Analysis
pythonCopy    async def _analyze_results(self, query, results, params, context=None):
        """Analyze search results to determine next steps."""
        # Format results for analysis
        formatted_results = self._format_results_for_analysis(results)
        context_text = self._format_context(context) if context else "No previous context"

        # Create analysis prompt with JSON form
        prompt = f"""
        You are a research analyst for a knowledge corpus. Analyze these search results and determine next steps.

        ORIGINAL QUERY: {query}

        CONVERSATION CONTEXT:
        {context_text}

        SEARCH RESULTS:
        {formatted_results}

        Analyze these results carefully and provide your assessment in the following JSON format:

        {{
            "information_sufficiency": 0.0-1.0 (how well these results answer the query),
            "key_findings": [
                "List of key information found in the results"
            ],
            "information_gaps": [
                "List of missing information needed to fully answer the query"
            ],
            "follow_up_queries": [
                "List of specific follow-up queries to fill the information gaps"
            ],
            "irrelevant_results": [
                Indices of results that seem irrelevant to the query (0-based)
            ],
            "metadata_recommendations": {{
                Recommended metadata filters to focus subsequent searches
            }},
            "notes": "Your observations about the results and recommendations"
        }}

        Be thorough in identifying information gaps and formulating precise follow-up queries.
        """

        # Get LLM analysis
        analysis_result = await self.llm_service.generate_completion(
            prompt=prompt,RetryClaude hit the max length for a message and has paused its response. You can write Continue to keep the chat going.EScontinueEdit5. Research Agent Implementation (continued)
5.3 Result Analysis (continued)
pythonCopy        # Get LLM analysis
        analysis_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"},
            note_id=f"results_analysis_{int(time.time())}"
        )

        # Parse the response
        try:
            analysis = json.loads(analysis_result)
        except json.JSONDecodeError:
            # Fallback to basic analysis
            analysis = {
                "information_sufficiency": 0.5,
                "key_findings": ["Unable to parse key findings"],
                "information_gaps": ["Unable to parse information gaps"],
                "follow_up_queries": [],
                "irrelevant_results": [],
                "metadata_recommendations": {},
                "notes": "Analysis parsing failed, using default values."
            }

        return analysis

    def _prepare_final_results(self, research_id, all_results, analysis, params):
        """Prepare final results object."""
        # Deduplicate and organize chunks
        processed_results = self._deduplicate_and_organize_chunks(all_results)

        # Get research memory
        memory = self.research_memory.get(research_id, {})

        # Build final result
        final_result = {
            "research_id": research_id,
            "original_query": memory.get("query", "Unknown"),
            "chunks": processed_results,
            "chunk_count": len(processed_results),
            "key_findings": analysis.get("key_findings", []),
            "information_gaps": analysis.get("information_gaps", []),
            "information_sufficiency": analysis.get("information_sufficiency", 0),
            "iterations": memory.get("iterations", 1),
            "parameters": params,
            "notes": self.notes.get(f"{research_id}_conclusion", "")
        }

        return final_result
6. Orchestrator Implementation
6.1 Core Orchestrator
pythonCopyclass OrchestratorAgent:
    """
    Central orchestrator that coordinates the entire knowledge corpus chat system,
    with dynamic parameter adjustment and note-taking capabilities.
    """

    def __init__(self, vector_db, llm_service=None, research_agent=None, resource_monitor=None):
        self.vector_db = vector_db
        self.llm_service = llm_service or LLMService(default_model="gpt-4")
        self.research_agent = research_agent or ResearchAgent(vector_db, self.llm_service)
        self.resource_monitor = resource_monitor or ResourceMonitor()

        # Conversation tracking
        self.conversations = {}
        self.knowledge_context = {}

        # Notes storage
        self.notes = {}

    async def process_query(self, query, session_id, conversation_history=None):
        """Process a user query."""
        # Initialize session if new
        if session_id not in self.conversations:
            self.conversations[session_id] = []
            self.knowledge_context[session_id] = []

        # Update conversation with user query
        if conversation_history:
            self.conversations[session_id] = conversation_history
        else:
            self.conversations[session_id].append({"role": "user", "content": query})

        # Create a unique ID for this query
        query_id = f"{session_id}_{len(self.conversations[session_id])}"

        # Analyze query to determine processing strategy
        strategy = await self._determine_strategy(query, session_id)

        # Record strategy
        self.notes[f"query_{query_id}_strategy"] = strategy

        # Research phase
        if strategy["requires_research"]:
            # Conduct research with the strategy parameters
            research_results = await self.research_agent.research(
                query=query,
                context=self.conversations[session_id],
                initial_params=strategy["research_params"]
            )

            # Update knowledge context
            await self._update_knowledge_context(session_id, research_results, strategy)

        # Generate response
        response = await self._generate_response(query, session_id, strategy)

        # Update conversation with response
        self.conversations[session_id].append({"role": "assistant", "content": response})

        # Record performance metrics
        self._record_performance_metrics(query_id, strategy)

        return response
6.2 Query Strategy Determination
pythonCopy    async def _determine_strategy(self, query, session_id):
        """Determine processing strategy for the query."""
        # Get conversation context
        conversation = self.conversations[session_id]

        # Format context for prompt
        context_text = self._format_conversation(conversation)

        # Summarize current knowledge context
        knowledge_summary = self._summarize_knowledge_context(session_id)

        # Create strategy determination prompt with JSON form
        prompt = f"""
        You are the orchestrator for a knowledge corpus question-answering system.
        Determine the optimal processing strategy for this query.

        USER QUERY: {query}

        CONVERSATION HISTORY:
        {context_text}

        CURRENT KNOWLEDGE CONTEXT SUMMARY:
        {knowledge_summary}

        Based on this query and context, determine the optimal processing strategy.
        Provide your recommendation in the following JSON format:

        {{
            "query_type": "factual"|"interpretive"|"comparative"|"clarification"|"follow_up",
            "requires_research": true/false,
            "is_follow_up_to_previous": true/false,
            "requires_new_knowledge": true/false,
            "research_params": {{
                Optional parameters for research if required:
                "top_k": Number of results to retrieve,
                "max_iterations": Maximum research iterations,
                etc.
            }},
            "context_relevance": {{
                "conversation_importance": 1-10,
                "knowledge_importance": 1-10,
                "knowledge_sufficiency": 0.0-1.0
            }},
            "response_params": {{
                "detail_level": 1-10,
                "include_citations": true/false,
                "include_sources": true/false
            }},
            "notes": "Explanation of your strategy recommendation"
        }}

        If the query is a follow-up or can be answered from existing context, set requires_research to false.
        """

        # Get LLM recommendation
        strategy_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"},
            note_id=f"strategy_{session_id}_{len(conversation)}"
        )

        # Parse the response
        try:
            strategy = json.loads(strategy_result)
        except json.JSONDecodeError:
            # Fallback to default strategy
            strategy = {
                "query_type": "factual",
                "requires_research": True,
                "is_follow_up_to_previous": False,
                "requires_new_knowledge": True,
                "research_params": {
                    "top_k": 10,
                    "max_iterations": 2
                },
                "context_relevance": {
                    "conversation_importance": 5,
                    "knowledge_importance": 8,
                    "knowledge_sufficiency": 0.2
                },
                "response_params": {
                    "detail_level": 7,
                    "include_citations": True,
                    "include_sources": True
                },
                "notes": "Using default strategy due to parsing failure."
            }

        return strategy
6.3 Knowledge Context Management
pythonCopy    async def _update_knowledge_context(self, session_id, research_results, strategy):
        """Update the knowledge context with new research findings."""
        # Get current knowledge context
        current_knowledge = self.knowledge_context.get(session_id, [])

        # Extract new chunks from research
        new_chunks = research_results.get("chunks", [])

        # Determine how to update knowledge context based on strategy
        if strategy["query_type"] == "follow_up" and current_knowledge:
            # For follow-up queries, merge new knowledge with existing
            # Create prompt for knowledge integration with JSON form
            knowledge_text = self._format_knowledge_chunks(current_knowledge)
            new_knowledge_text = self._format_knowledge_chunks(new_chunks)

            prompt = f"""
            You are a knowledge integrator for a question-answering system.
            Determine how to combine existing knowledge with new findings.

            EXISTING KNOWLEDGE:
            {knowledge_text}

            NEW FINDINGS:
            {new_knowledge_text}

            STRATEGY:
            {json.dumps(strategy, indent=2)}

            Provide your recommendation for knowledge integration in the following JSON format:

            {{
                "keep_existing": [List of indices of existing knowledge to keep],
                "add_new": [List of indices of new findings to add],
                "knowledge_priority": "existing"|"new"|"balanced",
                "notes": "Explanation of your integration strategy"
            }}
            """

            # Get LLM recommendation
            integration_result = await self.llm_service.generate_completion(
                prompt=prompt,
                temperature=0.2,
                response_format={"type": "json_object"},
                note_id=f"knowledge_integration_{session_id}_{len(self.conversations[session_id])}"
            )

            try:
                integration = json.loads(integration_result)

                # Apply integration strategy
                updated_knowledge = []

                # Keep selected existing knowledge
                if "keep_existing" in integration:
                    for i in integration["keep_existing"]:
                        if 0 <= i < len(current_knowledge):
                            updated_knowledge.append(current_knowledge[i])

                # Add selected new knowledge
                if "add_new" in integration:
                    for i in integration["add_new"]:
                        if 0 <= i < len(new_chunks):
                            updated_knowledge.append(new_chunks[i])

                # Store notes if provided
                if "notes" in integration:
                    self.notes[f"knowledge_integration_{session_id}_{len(self.conversations[session_id])}"] = integration["notes"]

                self.knowledge_context[session_id] = updated_knowledge

            except json.JSONDecodeError:
                # Fallback: keep all existing and add all new
                self.knowledge_context[session_id] = current_knowledge + new_chunks

        else:
            # For new topics, replace knowledge context
            self.knowledge_context[session_id] = new_chunks

        # Ensure context doesn't exceed model limits
        self._prune_knowledge_context(session_id)
6.4 Response Generation
pythonCopy    async def _generate_response(self, query, session_id, strategy):
        """Generate a response to the user query."""
        # Format knowledge context
        knowledge_text = self._format_knowledge_chunks(
            self.knowledge_context.get(session_id, []),
            include_citations=strategy["response_params"]["include_citations"]
        )

        # Format conversation history
        conversation_text = self._format_conversation(
            self.conversations[session_id],
            importance=strategy["context_relevance"]["conversation_importance"]
        )

        # Create system instruction
        system_instruction = self._create_system_instruction(strategy)

        # Following Claude's preferred format: knowledge first, then conversation, then query
        prompt = f"""{system_instruction}

KNOWLEDGE CONTEXT:
{knowledge_text}

CONVERSATION HISTORY:
{conversation_text}

USER QUERY:
{query}
"""

        # Generate response
        response = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.7,  # Slightly higher for more natural response
            max_tokens=None,  # Allow model to determine length
            note_id=f"response_{session_id}_{len(self.conversations[session_id])}"
        )

        return response

    def _create_system_instruction(self, strategy):
        """Create system instruction based on strategy."""
        # Base instruction
        instruction = "You are a knowledgeable assistant with expertise in researching and explaining information from a knowledge corpus."

        # Add detail level
        detail_level = strategy["response_params"]["detail_level"]
        if detail_level <= 3:
            instruction += " Provide concise, direct answers."
        elif detail_level <= 7:
            instruction += " Provide balanced, informative answers with appropriate detail."
        else:
            instruction += " Provide comprehensive, detailed explanations with nuance and depth."

        # Add citation instructions
        if strategy["response_params"]["include_citations"]:
            instruction += " Cite sources using the reference numbers provided in the knowledge context [1], [2], etc."

        # Add query-specific instructions
        query_type = strategy["query_type"]
        if query_type == "factual":
            instruction += " Focus on factual accuracy and clarity."
        elif query_type == "interpretive":
            instruction += " Acknowledge various interpretations and perspectives."
        elif query_type == "comparative":
            instruction += " Compare and contrast different views or sources."
        elif query_type == "follow_up":
            instruction += " Build upon the previous conversation while addressing the new question."

        # Add knowledge usage instruction
        instruction += " Base your answer on the provided knowledge context. If the information needed is not in the context, acknowledge this limitation."

        return instruction
7. LLM Service Implementation
pythonCopyclass LLMService:
    """
    Service for interfacing with LLMs, with note-taking capabilities.
    """

    def __init__(self, default_model="gpt-4", resource_monitor=None):
        self.default_model = default_model
        self.resource_monitor = resource_monitor or ResourceMonitor()
        self.call_history = []
        self.notes = {}

    async def generate_completion(self, prompt, model=None, temperature=0.7, max_tokens=None,
                               response_format=None, note_id=None):
        """Generate a completion from the LLM."""
        # Import OpenAI client
        from openai import AsyncOpenAI

        # Record call start time
        start_time = time.time()

        # Use default model if not specified
        model = model or self.default_model

        # Check resource monitor for rate limits
        if self.resource_monitor:
            resources = self.resource_monitor.get_current_state()
            if resources.get("warnings"):
                # If approaching rate limits, use less capable but faster model
                if "Approaching API RPM limit" in resources["warnings"]:
                    model = model.replace("gpt-4", "gpt-3.5-turbo")

        # Configure request parameters
        params = {
            "model": model,
            "temperature": temperature,
            "messages": [{"role": "user", "content": prompt}]
        }

        if max_tokens:
            params["max_tokens"] = max_tokens

        if response_format:
            params["response_format"] = response_format

        # Create OpenAI client
        client = AsyncOpenAI()

        # Generate completion
        try:
            response = await client.chat.completions.create(**params)

            # Extract response text
            completion_text = response.choices[0].message.content

            # Record completion time
            end_time = time.time()
            latency = (end_time - start_time) * 1000

            # Record API call
            if self.resource_monitor:
                self.resource_monitor.record_api_call(
                    api_type="completion",
                    tokens=response.usage.total_tokens,
                    latency=latency
                )

            # Record call in history
            call_record = {
                "timestamp": end_time,
                "model": model,
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "latency_ms": latency,
                "success": True
            }

            self.call_history.append(call_record)

            # Check if this is a JSON response and contains notes
            if response_format and response_format.get("type") == "json_object":
                try:
                    content = json.loads(completion_text)
                    if "notes" in content and note_id:
                        self.notes[note_id] = content["notes"]
                except:
                    pass

            return completion_text

        except Exception as e:
            # Record error
            end_time = time.time()
            latency = (end_time - start_time) * 1000

            call_record = {
                "timestamp": end_time,
                "model": model,
                "error": str(e),
                "latency_ms": latency,
                "success": False
            }

            self.call_history.append(call_record)

            # Add note about error
            if note_id:
                self.notes[f"{note_id}_error"] = f"Error generating completion: {str(e)}"

            # Fallback to simpler model if this was a capacity error
            if "capacity" in str(e).lower() and "gpt-4" in model:
                return await self.generate_completion(
                    prompt=prompt,
                    model="gpt-3.5-turbo",
                    temperature=temperature,
                    max_tokens=max_tokens,
                    response_format=response_format,
                    note_id=note_id
                )

            # Re-raise error
            raise
8. Resource Monitoring System
pythonCopyclass ResourceMonitor:
    """
    Monitors system resources and provides feedback for dynamic resource allocation.
    """

    def __init__(self):
        self.history = []
        self.warning_thresholds = {
            "cpu": 85,  # Percentage
            "memory": 80,  # Percentage
            "embedding_tpm": 4000000,  # Tokens per minute (out of 5M limit)
            "api_rpm": 8000  # Requests per minute (out of 10K limit)
        }
        self.start_time = time.time()

    def get_current_state(self):
        """Get current system resource state."""
        import psutil

        # Get CPU and memory usage
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()

        # Calculate API usage rates based on history
        current_minute = int((time.time() - self.start_time) / 60)
        recent_api_calls = [
            entry for entry in self.history
            if entry["type"] == "api_call" and entry["minute"] == current_minute
        ]

        recent_tokens = sum(entry.get("tokens", 0) for entry in recent_api_calls)

        # Calculate available resources
        state = {
            "cpu_usage": cpu_percent,
            "cpu_available": 100 - cpu_percent,
            "memory_usage": memory.percent,
            "memory_available": 100 - memory.percent,
            "tokens_used_this_minute": recent_tokens,
            "tokens_available": self.warning_thresholds["embedding_tpm"] - recent_tokens,
            "api_calls_this_minute": len(recent_api_calls),
            "api_calls_available": self.warning_thresholds["api_rpm"] - len(recent_api_calls)
        }

        # Add warnings if thresholds are approached
        state["warnings"] = []

        if state["cpu_usage"] > self.warning_thresholds["cpu"]:
            state["warnings"].append("CPU usage critical")

        if state["memory_usage"] > self.warning_thresholds["memory"]:
            state["warnings"].append("Memory usage critical")

        if state["tokens_used_this_minute"] > self.warning_thresholds["embedding_tpm"] * 0.9:
            state["warnings"].append("Approaching embedding TPM limit")

        if state["api_calls_this_minute"] > self.warning_thresholds["api_rpm"] * 0.9:
            state["warnings"].append("Approaching API RPM limit")

        # Record current state in history
        self.history.append({
            "timestamp": time.time(),
            "minute": current_minute,
            "type": "system_state",
            "state": state
        })

        return state

    def record_api_call(self, api_type, tokens=0, latency=0):
        """Record an API call for rate monitoring."""
        current_minute = int((time.time() - self.start_time) / 60)

        self.history.append({
            "timestamp": time.time(),
            "minute": current_minute,
            "type": "api_call",
            "api_type": api_type,
            "tokens": tokens,
            "latency": latency
        })

    def optimize_resources(self):
        """Suggest resource optimizations based on current state."""
        state = self.get_current_state()
        optimizations = {}

        # Optimize batch sizes based on current usage
        if state["cpu_available"] < 20 or state["memory_available"] < 20:
            # System under heavy load, reduce batch sizes
            optimizations["embedding_batch_size"] = 100
            optimizations["processing_batch_size"] = 5
            optimizations["max_parallel_processes"] = 4
        elif state["cpu_available"] > 50 and state["memory_available"] > 50:
            # System has plenty of resources, increase throughput
            optimizations["embedding_batch_size"] = 1000
            optimizations["processing_batch_size"] = 20
            optimizations["max_parallel_processes"] = 12
        else:
            # Moderate resource availability
            optimizations["embedding_batch_size"] = 500
            optimizations["processing_batch_size"] = 10
            optimizations["max_parallel_processes"] = 8

        # Adjust API call strategies based on rate limits
        if state["tokens_available"] < 1000000 or state["api_calls_available"] < 2000:
            # Approaching rate limits, implement throttling
            optimizations["throttle_embeddings"] = True
            optimizations["min_time_between_calls"] = 0.1  # seconds
        else:
            optimizations["throttle_embeddings"] = False
            optimizations["min_time_between_calls"] = 0

        return optimizations
9. Web Interface Implementation
pythonCopyasync def initialize_and_run_system(input_folder="./input", output_folder="./output"):
    """Initialize and run the complete knowledge corpus chat system."""
    import os
    import time
    import asyncio
    import uuid
    from fastapi import FastAPI, WebSocket, HTTPException
    from fastapi.responses import HTMLResponse
    from fastapi.staticfiles import StaticFiles
    import uvicorn
    import json

    # Create output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Initialize components
    print("Initializing resource monitor...")
    resource_monitor = ResourceMonitor()

    print("Initializing LLM service...")
    llm_service = LLMService(resource_monitor=resource_monitor)

    print("Initializing embedding service...")
    embedding_service = EmbeddingService(resource_monitor=resource_monitor)

    # Check if vector store already exists
    vector_store_path = os.path.join(output_folder, "vector_store")
    vector_store_exists = os.path.exists(vector_store_path)

    if not vector_store_exists:
        print("Vector store not found. Processing documents...")
        await process_documents(input_folder, output_folder, llm_service, embedding_service, resource_monitor)

    # Load vector database
    from langchain.vectorstores import Chroma
    from langchain.embeddings import OpenAIEmbeddings

    print("Loading vector database...")
    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")
    vector_db = Chroma(
        collection_name="knowledge_corpus",
        persist_directory=vector_store_path,
        embedding_function=embedding_function
    )

    # Initialize research agent
    print("Initializing research agent...")
    research_agent = ResearchAgent(vector_db, llm_service, resource_monitor)

    # Initialize orchestrator
    print("Initializing orchestrator...")
    orchestrator = OrchestratorAgent(vector_db, llm_service, research_agent, resource_monitor)

    # Create FastAPI app
    app = FastAPI(title="Knowledge Corpus Chat System")

    # Mount static files
    app.mount("/static", StaticFiles(directory="static"), name="static")

    # Define routes
    @app.get("/", response_class=HTMLResponse)
    async def get_home():
        with open("static/index.html", "r") as f:
            return f.read()

    @app.websocket("/chat")
    async def websocket_endpoint(websocket: WebSocket):
        await websocket.accept()

        # Initialize session
        session_id = str(uuid.uuid4())
        conversation = []

        try:
            while True:
                # Receive message
                message = await websocket.receive_text()
                data = json.loads(message)

                if data["type"] == "query":
                    # Process query
                    query = data["content"]

                    # Send acknowledgment
                    await websocket.send_json({
                        "type": "status",
                        "content": "Processing your question..."
                    })

                    # Process with orchestrator
                    response = await orchestrator.process_query(query, session_id, conversation)

                    # Update conversation
                    conversation.append({"role": "user", "content": query})
                    conversation.append({"role": "assistant", "content": response})

                    # Send response
                    await websocket.send_json({
                        "type": "response",
                        "content": response
                    })

                elif data["type"] == "reset":
                    # Reset conversation
                    conversation = []
                    session_id = str(uuid.uuid4())
                    await websocket.send_json({
                        "type": "status",
                        "content": "Conversation reset."
                    })

        except Exception as e:
            print(f"Error in websocket: {e}")
            try:
                await websocket.send_json({
                    "type": "error",
                    "content": f"An error occurred: {str(e)}"
                })
            except:
                pass
            finally:
                await websocket.close()

    # Start the server
    print("Starting server...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
10. Document Processing Pipeline
pythonCopyasync def process_documents(input_folder, output_folder, llm_service, embedding_service, resource_monitor):
    """Process all documents in the input folder."""
    import os
    import time
    import asyncio
    from concurrent.futures import ProcessPoolExecutor

    start_time = time.time()

    # Initialize document analyzer
    document_analyzer = DocumentAnalyzer(llm_service)

    # Initialize chunk processor
    chunk_processor = ChunkProcessor(llm_service)

    # Initialize multilevel chunk builder
    chunk_builder = MultilevelChunkBuilder(llm_service)

    # Discover all documents
    documents = []
    for root, _, files in os.walk(input_folder):
        for filename in files:
            if filename.endswith(('.txt', '.pdf', '.html', '.md')):
                file_path = os.path.join(root, filename)
                rel_path = os.path.relpath(file_path, input_folder)

                documents.append({
                    "path": file_path,
                    "rel_path": rel_path,
                    "name": filename
                })

    print(f"Found {len(documents)} documents")

    # Analyze documents in parallel
    print("Analyzing documents...")

    # Use ProcessPoolExecutor for CPU-bound tasks
    max_workers = min(len(documents), os.cpu_count())

    # Create tasks for document analysis
    analysis_tasks = []
    for doc in documents:
        task = document_analyzer.analyze_document(doc["path"])
        analysis_tasks.append((doc, task))

    # Process tasks in batches to avoid overwhelming API
    analysis_results = []
    batch_size = 5

    for i in range(0, len(analysis_tasks), batch_size):
        batch = analysis_tasks[i:i+batch_size]
        batch_docs = [item[0] for item in batch]
        batch_tasks = [item[1] for item in batch]

        # Wait for batch to complete
        batch_results = await asyncio.gather(*batch_tasks)

        # Combine results with documents
        for j, result in enumerate(batch_results):
            analysis_results.append((batch_docs[j], result))

        print(f"Analyzed {len(analysis_results)}/{len(documents)} documents")

        # Small delay between batches
        await asyncio.sleep(0.5)

    # Process documents into chunks
    print("Processing documents into chunks...")
    all_level0_chunks = []

    for doc, analysis in analysis_results:
        # Load document content if needed
        with open(doc["path"], 'r', encoding='utf-8') as f:
            content = f.read()

        # Process document into chunks
        doc_chunks = await chunk_processor.process_document(doc["path"], content, analysis)
        all_level0_chunks.extend(doc_chunks)

        print(f"Created {len(doc_chunks)} Level 0 chunks for {doc['name']}")

    print(f"Created total of {len(all_level0_chunks)} Level 0 chunks")

    # Get model information for embedding
    embedding_model_info = await embedding_service.get_model_info()

    # Build multilevel chunks
    print("Building multilevel chunks...")
    all_chunks = await chunk_builder.build_multilevel_chunks(
        all_level0_chunks,
        embedding_model_info["max_tokens"]
    )

    # Count total chunks
    total_chunks = sum(len(chunks) for level, chunks in all_chunks.items())
    print(f"Created total of {total_chunks} chunks across {len(all_chunks)} levels")

    # Flatten chunks for embedding
    flat_chunks = []
    for level_chunks in all_chunks.values():
        flat_chunks.extend(level_chunks)

    # Generate embeddings
    print(f"Generating embeddings for {len(flat_chunks)} chunks...")
    embeddings = await embedding_service.embed_documents(flat_chunks)
    print(f"Generated {len(embeddings)} embeddings")

    # Initialize vector database
    from langchain.vectorstores import Chroma
    from langchain.schema import Document

    vector_store_path = os.path.join(output_folder, "vector_store")

    # Convert to langchain Documents
    documents = []
    embeddings_list = []
    metadatas = []
    ids = []

    for chunk in flat_chunks:
        chunk_id = chunk["id"]

        # Skip if we don't have an embedding
        if chunk_id not in embeddings:
            continue

        # Create Document
        doc = Document(
            page_content=chunk["text"],
            metadata=chunk["metadata"]
        )

        documents.append(doc)
        embeddings_list.append(embeddings[chunk_id])
        metadatas.append(RetryClaude hit the max length for a message and has paused its response. You can write Continue to keep the chat going.ESclaudeEditI'll continue with the document processing pipeline code:
pythonCopy    documents.append(doc)
    embeddings_list.append(embeddings[chunk_id])
    metadatas.append(chunk["metadata"])
    ids.append(chunk_id)

# Create vector database
print("Creating vector database...")
vector_db = Chroma(
    collection_name="knowledge_corpus",
    persist_directory=vector_store_path
)

# Add documents in batches
batch_size = 500
for i in range(0, len(documents), batch_size):
    end_idx = min(i + batch_size, len(documents))

    vector_db.add_documents(
        documents=documents[i:end_idx],
        embeddings=embeddings_list[i:end_idx],
        metadatas=metadatas[i:end_idx],
        ids=ids[i:end_idx]
    )

    print(f"Added {end_idx}/{len(documents)} documents to vector database")

# Persist database
vector_db.persist()

# Record completion time
end_time = time.time()
print(f"Document processing completed in {end_time - start_time:.2f} seconds")
11. Advanced Search Capabilities
11.1 Progressive Search Refinement
pythonCopyclass ProgressiveSearcher:
    """
    Implements progressive search refinement for complex queries.
    """

    def __init__(self, vector_db, llm_service):
        self.vector_db = vector_db
        self.llm_service = llm_service

    async def progressive_search(self, query, max_iterations=3, initial_k=10):
        """
        Conduct a progressive search that refines results through multiple iterations.

        Args:
            query: Initial search query
            max_iterations: Maximum number of iterations
            initial_k: Initial number of results to retrieve

        Returns:
            Final set of results and search path
        """
        search_path = []
        current_results = await self.vector_db.similarity_search(query, top_k=initial_k)
        search_path.append({
            "iteration": 1,
            "query": query,
            "results_count": len(current_results)
        })

        for iteration in range(2, max_iterations + 1):
            # Analyze current results to refine search
            analysis = await self._analyze_search_results(query, current_results)

            # If results are sufficient, stop
            if analysis["sufficiency_score"] > 0.8:
                search_path.append({
                    "iteration": iteration,
                    "status": "sufficient",
                    "sufficiency_score": analysis["sufficiency_score"]
                })
                break

            # Generate refined query
            refined_query = await self._generate_refined_query(query, current_results, analysis)

            # Execute refined query
            new_results = await self.vector_db.similarity_search(
                refined_query,
                top_k=initial_k,
                threshold=0.6  # Lower threshold to get more diverse results
            )

            # Merge and deduplicate results
            seen_ids = {doc.metadata["chunk_id"] for doc in current_results}
            for doc in new_results:
                if doc.metadata["chunk_id"] not in seen_ids:
                    current_results.append(doc)
                    seen_ids.add(doc.metadata["chunk_id"])

            search_path.append({
                "iteration": iteration,
                "query": refined_query,
                "new_results_count": len(new_results),
                "total_results_count": len(current_results)
            })

        return current_results, search_path

    async def _analyze_search_results(self, query, results):
        """Analyze search results to determine next steps."""
        # Format results for analysis
        formatted_results = self._format_results(results)

        prompt = f"""
        Analyze these search results for the query: "{query}"

        SEARCH RESULTS:
        {formatted_results}

        Determine how well these results answer the query and what information is still missing.
        Return your analysis as JSON:

        {{
            "sufficiency_score": 0.0-1.0 (how well the results answer the query),
            "covered_aspects": ["list", "of", "aspects", "covered", "well"],
            "missing_aspects": ["list", "of", "aspects", "missing", "or", "inadequate"],
            "key_terms_for_refinement": ["list", "of", "terms", "to", "focus", "on"]
        }}
        """

        # Get LLM analysis
        analysis_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        # Parse and return analysis
        try:
            return json.loads(analysis_result)
        except:
            return {
                "sufficiency_score": 0.5,
                "covered_aspects": [],
                "missing_aspects": ["unknown due to parsing error"],
                "key_terms_for_refinement": [query]
            }

    async def _generate_refined_query(self, original_query, current_results, analysis):
        """Generate a refined query based on analysis of current results."""
        prompt = f"""
        Original query: "{original_query}"

        Current search results cover these aspects well:
        {', '.join(analysis["covered_aspects"])}

        But these aspects are still missing or inadequate:
        {', '.join(analysis["missing_aspects"])}

        Generate a refined search query that will help find information about the missing aspects.
        The query should be specific and focused on filling the gaps in current results.
        """

        refined_query = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3
        )

        return refined_query.strip()
11.2 Multi-Strategy Search
pythonCopyclass MultiStrategySearcher:
    """
    Implements multiple search strategies to handle different query types.
    """

    def __init__(self, vector_db, llm_service):
        self.vector_db = vector_db
        self.llm_service = llm_service

    async def search(self, query, query_type="auto", max_results=20):
        """
        Conduct search using the optimal strategy for the query type.

        Args:
            query: Search query
            query_type: Type of query (factual, conceptual, exploratory, comparative, auto)
            max_results: Maximum number of results to return

        Returns:
            Search results and search metadata
        """
        # If auto, determine query type
        if query_type == "auto":
            query_type = await self._determine_query_type(query)

        # Choose search strategy based on query type
        if query_type == "factual":
            return await self._factual_search(query, max_results)
        elif query_type == "conceptual":
            return await self._conceptual_search(query, max_results)
        elif query_type == "exploratory":
            return await self._exploratory_search(query, max_results)
        elif query_type == "comparative":
            return await self._comparative_search(query, max_results)
        else:
            # Default to hybrid search
            return await self._hybrid_search(query, max_results)

    async def _determine_query_type(self, query):
        """Determine the type of query using LLM analysis."""
        prompt = f"""
        Analyze this query and determine its type:

        QUERY: {query}

        Choose one of the following types:
        - "factual": Seeking specific facts or definitions
        - "conceptual": Exploring abstract concepts or principles
        - "exploratory": Broad information gathering on a topic
        - "comparative": Comparing different entities or viewpoints

        Return only the query type as a single word with no additional text.
        """

        query_type = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.1
        )

        # Clean up response
        query_type = query_type.strip().lower().replace('"', '').replace('.', '')

        # Map to valid types
        type_map = {
            "factual": "factual",
            "conceptual": "conceptual",
            "exploratory": "exploratory",
            "comparative": "comparative"
        }

        return type_map.get(query_type, "hybrid")

    async def _factual_search(self, query, max_results):
        """Optimized search for factual queries - high precision."""
        # Extract key entities from query
        entities = await self._extract_key_entities(query)

        # Generate high-precision query
        precision_query = f"{query} {' '.join(entities)}"

        # Search with high threshold
        results = await self.vector_db.similarity_search(
            precision_query,
            top_k=max_results,
            threshold=0.75
        )

        return results, {"strategy": "factual", "entities": entities}

    async def _conceptual_search(self, query, max_results):
        """Optimized search for conceptual queries - semantic focus."""
        # Extract key concepts from query
        concepts = await self._extract_key_concepts(query)

        # Search for each concept and combine results
        all_results = []
        seen_ids = set()

        for concept in concepts:
            concept_query = f"{concept} {query}"
            concept_results = await self.vector_db.similarity_search(
                concept_query,
                top_k=max_results // len(concepts),
                threshold=0.65
            )

            # Add unique results
            for result in concept_results:
                chunk_id = result.metadata["chunk_id"]
                if chunk_id not in seen_ids:
                    all_results.append(result)
                    seen_ids.add(chunk_id)

        return all_results, {"strategy": "conceptual", "concepts": concepts}

    async def _exploratory_search(self, query, max_results):
        """Optimized search for exploratory queries - broad coverage."""
        # Create diverse sub-queries
        sub_queries = await self._generate_exploratory_sub_queries(query)

        # Search for each sub-query and combine results
        all_results = []
        seen_ids = set()

        for sub_query in sub_queries:
            sub_results = await self.vector_db.similarity_search(
                sub_query,
                top_k=max_results // len(sub_queries),
                threshold=0.6  # Lower threshold for broader results
            )

            # Add unique results
            for result in sub_results:
                chunk_id = result.metadata["chunk_id"]
                if chunk_id not in seen_ids:
                    all_results.append(result)
                    seen_ids.add(chunk_id)

        return all_results, {"strategy": "exploratory", "sub_queries": sub_queries}

    async def _comparative_search(self, query, max_results):
        """Optimized search for comparative queries - multiple perspectives."""
        # Extract entities being compared
        comparison_entities = await self._extract_comparison_entities(query)

        # Search for each entity and the comparison context
        all_results = []
        seen_ids = set()

        # Search for each entity
        for entity in comparison_entities:
            entity_query = f"{entity} {query}"
            entity_results = await self.vector_db.similarity_search(
                entity_query,
                top_k=max_results // (len(comparison_entities) + 1),
                threshold=0.7
            )

            # Add unique results
            for result in entity_results:
                chunk_id = result.metadata["chunk_id"]
                if chunk_id not in seen_ids:
                    all_results.append(result)
                    seen_ids.add(chunk_id)

        # Also search for comparison terms
        comparison_query = f"comparison between {' and '.join(comparison_entities)}"
        comparison_results = await self.vector_db.similarity_search(
            comparison_query,
            top_k=max_results // (len(comparison_entities) + 1),
            threshold=0.65
        )

        # Add unique comparison results
        for result in comparison_results:
            chunk_id = result.metadata["chunk_id"]
            if chunk_id not in seen_ids:
                all_results.append(result)
                seen_ids.add(chunk_id)

        return all_results, {"strategy": "comparative", "entities": comparison_entities}
12. Advanced Memory Management
12.1 Conversation Memory Manager
pythonCopyclass ConversationMemoryManager:
    """
    Manages conversation history with dynamic compression and context-aware retrieval.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()
        self.memory_by_session = {}
        self.summary_by_session = {}

    def add_message(self, session_id, message):
        """Add a message to conversation memory."""
        if session_id not in self.memory_by_session:
            self.memory_by_session[session_id] = []

        self.memory_by_session[session_id].append(message)

        # Check if we need to compress memory
        if len(self.memory_by_session[session_id]) > 10:
            self._compress_memory(session_id)

    async def get_relevant_history(self, session_id, query, max_messages=5):
        """Get the most relevant messages from conversation history."""
        if session_id not in self.memory_by_session:
            return []

        # Always include the last few messages for continuity
        recent_messages = self.memory_by_session[session_id][-3:]

        # If we have few messages or don't need relevance filtering, return them
        if len(self.memory_by_session[session_id]) <= max_messages:
            return self.memory_by_session[session_id]

        # Get summary if available
        summary = self.summary_by_session.get(session_id, "")

        # Evaluate relevance of older messages
        older_messages = self.memory_by_session[session_id][:-3]
        if not older_messages:
            return recent_messages

        # Use LLM to select relevant messages
        relevant_indices = await self._select_relevant_messages(
            query, older_messages, summary, max_messages - len(recent_messages)
        )

        # Combine relevant older messages with recent ones
        relevant_older = [older_messages[i] for i in relevant_indices]
        return relevant_older + recent_messages

    async def _compress_memory(self, session_id):
        """Compress conversation memory by summarizing older messages."""
        messages = self.memory_by_session[session_id]

        # Keep the most recent messages intact
        recent_count = 5
        if len(messages) <= recent_count:
            return

        # Messages to summarize
        to_summarize = messages[:-recent_count]

        # Format messages for summarization
        formatted_messages = "\n".join([
            f"{'User' if msg['role']=='user' else 'Assistant'}: {msg['content']}"
            for msg in to_summarize
        ])

        # Generate summary with LLM
        prompt = f"""
        Summarize the following conversation exchanges concisely while preserving key information:

        {formatted_messages}

        Provide a short summary that captures the essential points of this conversation history.
        Focus especially on any persistent topics or important details that might be relevant for future questions.
        """

        summary = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3
        )

        # Store summary
        self.summary_by_session[session_id] = summary

        # Replace older messages with summary
        self.memory_by_session[session_id] = [
            {"role": "system", "content": f"Previous conversation summary: {summary}"}
        ] + messages[-recent_count:]

    async def _select_relevant_messages(self, query, messages, summary, max_count):
        """Select the most relevant messages for the current query."""
        # Format messages
        formatted_messages = []
        for i, msg in enumerate(messages):
            if msg["role"] == "system" and "summary" in msg["content"]:
                continue  # Skip system summaries
            formatted_messages.append(f"[{i}] {'User' if msg['role']=='user' else 'Assistant'}: {msg['content']}")

        # Create prompt
        prompt = f"""
        CONVERSATION SUMMARY: {summary}

        MESSAGES:
        {chr(10).join(formatted_messages)}

        CURRENT QUERY: {query}

        Select up to {max_count} message indices that are most relevant to the current query.
        Consider:
        1. Messages that discuss topics related to the current query
        2. Messages that provide context needed to understand the query
        3. Previous questions similar to the current query

        Return only the message indices as a comma-separated list, e.g., "1,4,7"
        """

        # Get LLM recommendations
        result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2
        )

        # Parse indices
        try:
            indices = [int(idx.strip()) for idx in result.strip().split(",")]
            return [idx for idx in indices if 0 <= idx < len(messages)]
        except:
            # Fallback to most recent messages
            return list(range(max(0, len(messages) - max_count), len(messages)))
12.2 Knowledge Context Manager
pythonCopyclass KnowledgeContextManager:
    """
    Manages the active knowledge context with dynamic prioritization and relevance tracking.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()
        self.knowledge_by_session = {}
        self.relevance_scores = {}

    def update_knowledge(self, session_id, new_chunks, query):
        """Update knowledge context with new chunks."""
        if session_id not in self.knowledge_by_session:
            self.knowledge_by_session[session_id] = []
            self.relevance_scores[session_id] = {}

        # Add new chunks
        for chunk in new_chunks:
            chunk_id = chunk.metadata["chunk_id"]

            # Check if chunk already exists
            existing_ids = {c.metadata["chunk_id"] for c in self.knowledge_by_session[session_id]}
            if chunk_id not in existing_ids:
                self.knowledge_by_session[session_id].append(chunk)

                # Initial relevance score based on position in search results
                position_score = 1.0 - (new_chunks.index(chunk) / len(new_chunks))
                self.relevance_scores[session_id][chunk_id] = position_score

        # Prune knowledge if needed
        self._prune_knowledge_context(session_id)

    async def get_relevant_knowledge(self, session_id, query, max_chunks=10):
        """Get the most relevant knowledge chunks for the current query."""
        if session_id not in self.knowledge_by_session:
            return []

        # If we have few chunks, return them all
        if len(self.knowledge_by_session[session_id]) <= max_chunks:
            return self.knowledge_by_session[session_id]

        # Update relevance scores based on current query
        await self._update_relevance_scores(session_id, query)

        # Sort chunks by relevance score
        chunks = self.knowledge_by_session[session_id]
        chunks_with_scores = [
            (chunk, self.relevance_scores[session_id].get(chunk.metadata["chunk_id"], 0))
            for chunk in chunks
        ]
        sorted_chunks = [chunk for chunk, score in sorted(
            chunks_with_scores, key=lambda x: x[1], reverse=True
        )]

        # Return top chunks
        return sorted_chunks[:max_chunks]

    async def _update_relevance_scores(self, session_id, query):
        """Update relevance scores based on current query."""
        chunks = self.knowledge_by_session[session_id]

        # Batch chunks for analysis
        batch_size = 5
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]

            # Format chunks for analysis
            formatted_chunks = []
            for j, chunk in enumerate(batch):
                chunk_id = chunk.metadata["chunk_id"]
                formatted_chunks.append(f"[{chunk_id}] {chunk.page_content[:200]}...")

            # Create prompt
            prompt = f"""
            QUERY: {query}

            KNOWLEDGE CHUNKS:
            {chr(10).join(formatted_chunks)}

            Rate each chunk's relevance to the query on a scale from 0.0 to 1.0,
            where 1.0 means highly relevant and 0.0 means not relevant at all.

            Return your ratings as a JSON object where keys are chunk IDs and values are relevance scores:
            {{
                "chunk_id_1": 0.7,
                "chunk_id_2": 0.3,
                ...
            }}
            """

            # Get LLM ratings
            result = await self.llm_service.generate_completion(
                prompt=prompt,
                temperature=0.2,
                response_format={"type": "json_object"}
            )

            # Parse and update scores
            try:
                ratings = json.loads(result)
                for chunk_id, score in ratings.items():
                    if chunk_id in self.relevance_scores[session_id]:
                        # Average with existing score for stability
                        old_score = self.relevance_scores[session_id][chunk_id]
                        self.relevance_scores[session_id][chunk_id] = (old_score + score) / 2
                    else:
                        self.relevance_scores[session_id][chunk_id] = score
            except:
                # Fallback: keep existing scores
                pass

    def _prune_knowledge_context(self, session_id):
        """Ensure knowledge context doesn't exceed token limits."""
        chunks = self.knowledge_by_session[session_id]

        # Use approximate token counting
        import tiktoken
        enc = tiktoken.get_encoding("cl100k_base")

        # Calculate current token usage
        total_tokens = sum(len(enc.encode(chunk.page_content)) for chunk in chunks)

        # Maximum tokens (70% of context window)
        max_tokens = 32000 * 0.7

        # If we're under the limit, no need to prune
        if total_tokens <= max_tokens:
            return

        # Sort chunks by relevance score
        chunks_with_scores = [
            (chunk, self.relevance_scores[session_id].get(chunk.metadata["chunk_id"], 0))
            for chunk in chunks
        ]
        sorted_chunks = [chunk for chunk, score in sorted(
            chunks_with_scores, key=lambda x: x[1], reverse=True
        )]

        # Keep adding chunks until we hit the token limit
        kept_chunks = []
        current_tokens = 0

        for chunk in sorted_chunks:
            chunk_tokens = len(enc.encode(chunk.page_content))
            if current_tokens + chunk_tokens <= max_tokens:
                kept_chunks.append(chunk)
                current_tokens += chunk_tokens
            else:
                # Skip this chunk and continue checking others that might fit
                continue

        # Update knowledge context
        self.knowledge_by_session[session_id] = kept_chunks
13. Response Generation System
13.1 Response Generator
pythonCopyclass ResponseGenerator:
    """
    Generates responses using a structured approach with research findings.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()

    async def generate_response(self, query, knowledge_chunks, conversation_history, response_params):
        """Generate a response based on knowledge and conversation context."""
        # Format knowledge context
        knowledge_text = self._format_knowledge_chunks(
            knowledge_chunks,
            include_citations=response_params.get("include_citations", True)
        )

        # Format conversation history
        conversation_text = self._format_conversation(
            conversation_history,
            importance=response_params.get("conversation_importance", 5)
        )

        # Create system instruction
        system_instruction = self._create_system_instruction(response_params)

        # Create prompt with knowledge first (Claude's preference)
        prompt = f"""{system_instruction}

KNOWLEDGE CONTEXT:
{knowledge_text}

CONVERSATION HISTORY:
{conversation_text}

USER QUERY:
{query}
"""

        # Generate response
        response = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=response_params.get("temperature", 0.7),
            max_tokens=response_params.get("max_tokens", None)
        )

        return response

    def _create_system_instruction(self, response_params):
        """Create system instruction based on response parameters."""
        # Base instruction
        instruction = "You are a knowledgeable assistant with expertise in researching and explaining information from a knowledge corpus."

        # Add detail level
        detail_level = response_params.get("detail_level", 7)
        if detail_level <= 3:
            instruction += " Provide concise, direct answers."
        elif detail_level <= 7:
            instruction += " Provide balanced, informative answers with appropriate detail."
        else:
            instruction += " Provide comprehensive, detailed explanations with nuance and depth."

        # Add citation instructions
        if response_params.get("include_citations", True):
            instruction += " Cite sources using the reference numbers provided in the knowledge context [1], [2], etc."

        # Add query-specific instructions
        query_type = response_params.get("query_type", "factual")
        if query_type == "factual":
            instruction += " Focus on factual accuracy and clarity."
        elif query_type == "interpretive":
            instruction += " Acknowledge various interpretations and perspectives."
        elif query_type == "comparative":
            instruction += " Compare and contrast different views or sources."
        elif query_type == "follow_up":
            instruction += " Build upon the previous conversation while addressing the new question."

        # Add knowledge usage instruction
        instruction += " Base your answer on the provided knowledge context. If the information needed is not in the context, acknowledge this limitation."

        return instruction

    def _format_knowledge_chunks(self, chunks, include_citations=True):
        """Format knowledge chunks for inclusion in prompts."""
        if not chunks:
            return "No relevant knowledge available."

        formatted = ""
        for i, chunk in enumerate(chunks):
            # Format source information
            metadata = chunk.metadata
            source_info = f"{metadata.get('document_name', 'Unknown')}"

            # Add specific location information if available
            location_info = []
            if "chapter" in metadata and metadata["chapter"]:
                location_info.append(f"Chapter: {metadata['chapter']}")
            if "section" in metadata and metadata["section"]:
                location_info.append(f"Section: {metadata['section']}")
            if "page" in metadata and metadata["page"]:
                location_info.append(f"Page: {metadata['page']}")

            if location_info:
                source_info += f" ({', '.join(location_info)})"

            # Format the chunk
            if include_citations:
                formatted += f"[{i+1}] {source_info}\n{chunk.page_content}\n\n"
            else:
                formatted += f"{source_info}\n{chunk.page_content}\n\n"

        return formatted

    def _format_conversation(self, conversation, importance=5):
        """Format conversation history based on importance."""
        if not conversation:
            return "No previous conversation."

        # Determine how many messages to include based on importance
        if importance <= 3:
            # Low importance: just the last exchange
            messages = conversation[-2:] if len(conversation) >= 2 else conversation
        elif importance <= 7:
            # Medium importance: last 3 exchanges
            messages = conversation[-6:] if len(conversation) >= 6 else conversation
        else:
            # High importance: more context
            messages = conversation[-10:] if len(conversation) >= 10 else conversation

        # Format messages
        formatted = ""
        for msg in messages:
            role = "User" if msg["role"] == "user" else "Assistant"
            formatted += f"{role}: {msg['content']}\n\n"

        return formatted
14. System Integration and Startup
pythonCopyif __name__ == "__main__":
    import asyncio
    asyncio.run(initialize_and_run_system())
15. Key Features and Advantages
15.1 Dynamic Parameter Adjustment

All search and processing parameters are determined at runtime by LLMs
Parameters adapt to query type, complexity, and system state
No hardcoded thresholds or strategies

15.2 Content-Aware Document Processing

Document analysis extracts rich metadata including structure and content type
Chunk boundaries respect natural document structure
LLM-enhanced chunk metadata captures semantic information

15.3 Advanced Research Capabilities

Multi-iteration progressive search for complex queries
Strategy selection based on query type and complexity
Gap analysis to identify missing information
Automatic query reformulation to fill information gaps

15.4 Sophisticated Context Management

Dynamic allocation of context window between knowledge and conversation
Relevance-based pruning of knowledge context
Conversation compression with summary generation
Context integration for follow-up questions

15.5 Resource Optimization

Adaptive batch sizing based on system load
Parallel processing with dynamic worker allocation
Rate limit awareness for API calls
Model fallback when approaching limits

15.6 Explainable AI Through Notes

LLM components leave explanatory notes about decisions
Complete research path tracking
Parameter adjustment history
Information sufficiency assessments

This comprehensive system provides a flexible and powerful foundation for building knowledge corpus research assistants that can work with any type of content, from academic literature to technical documentation, legal texts, or domain-specific knowledge bases.RetryESPlease take the opportunity to expand upon any particular details that haven't been fully flushed out. Please focus on making everything behind the scenes and dynamic controlled by the agents. Make sure that all of the variables are controllable by the agents and that their descriptions and roles and context and knowledge is robust enough for them to understand exactly what to do and how to do it.EditExpanded Dynamic Knowledge Corpus Research Assistant
I'll enhance the system with more robust dynamic controls, ensuring all aspects are agent-controllable with comprehensive context. Let me expand on several key areas to make the system more autonomous and self-directing.
1. Agent Metadata and Self-Configuration System
pythonCopyclass AgentMetadataManager:
    """
    Manages agent metadata and configuration, enabling agents to understand their roles,
    capabilities, and how to interact with other system components.
    """

    def __init__(self):
        self.agent_registry = {}
        self.capability_registry = {}
        self.interaction_patterns = {}
        self.configuration_history = {}

    async def register_agent(self, agent_id, agent_type, capabilities, llm_service):
        """Register an agent with the system and generate its self-configuration."""
        # Store basic agent information
        self.agent_registry[agent_id] = {
            "agent_id": agent_id,
            "agent_type": agent_type,
            "capabilities": capabilities,
            "created_at": time.time()
        }

        # Generate self-description and operating parameters
        self_config = await self._generate_self_configuration(agent_id, agent_type, capabilities, llm_service)
        self.agent_registry[agent_id].update(self_config)

        # Register capabilities
        for capability in capabilities:
            if capability not in self.capability_registry:
                self.capability_registry[capability] = []
            self.capability_registry[capability].append(agent_id)

        # Initialize configuration history
        self.configuration_history[agent_id] = [{
            "timestamp": time.time(),
            "configuration": self_config,
            "reason": "initial_configuration"
        }]

        return self_config

    async def _generate_self_configuration(self, agent_id, agent_type, capabilities, llm_service):
        """Use LLM to generate agent self-understanding and optimal operation parameters."""
        prompt = f"""
        As an AI system designer, create a comprehensive self-configuration for a {agent_type} agent
        with ID "{agent_id}" and the following capabilities: {', '.join(capabilities)}.

        Generate a detailed configuration that includes:

        1. A precise self-description of the agent's purpose and responsibilities
        2. Optimal operating parameters for this specific agent type
        3. Decision-making framework appropriate to its role
        4. Interaction protocols with other system components
        5. Self-evaluation metrics to assess performance

        Provide this as a structured JSON configuration that the agent will use to understand
        its own function and behavior within the larger knowledge corpus research system.
        """

        config_result = await llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(config_result)
        except:
            # Fallback basic configuration
            return {
                "self_description": f"{agent_type} agent for knowledge corpus research",
                "operating_parameters": {
                    "decision_temperature": 0.3,
                    "processing_depth": "medium",
                    "initiative_level": "medium"
                },
                "decision_framework": {
                    "evaluation_criteria": ["relevance", "accuracy", "efficiency"],
                    "priority_ordering": ["accuracy", "relevance", "efficiency"]
                }
            }

    async def update_agent_configuration(self, agent_id, updates, reason, llm_service):
        """Update an agent's configuration based on performance or system changes."""
        if agent_id not in self.agent_registry:
            raise ValueError(f"Agent {agent_id} not registered")

        # Get current configuration
        current_config = self.agent_registry[agent_id]

        # Generate updated configuration
        prompt = f"""
        The {current_config['agent_type']} agent with ID "{agent_id}" needs to update its configuration.

        CURRENT CONFIGURATION:
        {json.dumps(current_config, indent=2)}

        REASON FOR UPDATE:
        {reason}

        REQUESTED UPDATES:
        {json.dumps(updates, indent=2)}

        Generate a revised configuration that incorporates these updates while maintaining
        coherence with the agent's role and existing configuration. The updated configuration
        should be compatible with the agent's capabilities and enhance its effectiveness.

        Return the complete updated configuration as a JSON object.
        """

        updated_config_result = await llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        try:
            updated_config = json.loads(updated_config_result)

            # Store the update in history
            self.configuration_history[agent_id].append({
                "timestamp": time.time(),
                "configuration": updated_config,
                "reason": reason
            })

            # Update the registry
            self.agent_registry[agent_id] = updated_config

            return updated_config
        except:
            # If parsing fails, apply just the specific updates
            for key, value in updates.items():
                if isinstance(value, dict) and key in current_config and isinstance(current_config[key], dict):
                    # Merge nested dictionaries
                    current_config[key].update(value)
                else:
                    # Replace or add value
                    current_config[key] = value

            # Store the update in history
            self.configuration_history[agent_id].append({
                "timestamp": time.time(),
                "configuration": current_config,
                "reason": f"{reason} (manual update)"
            })

            return current_config

    async def generate_interaction_pattern(self, agent_id1, agent_id2, interaction_type, llm_service):
        """Generate an optimal interaction pattern between two agents."""
        if agent_id1 not in self.agent_registry or agent_id2 not in self.agent_registry:
            raise ValueError("One or both agents not registered")

        agent1 = self.agent_registry[agent_id1]
        agent2 = self.agent_registry[agent_id2]

        # Create or retrieve interaction key
        interaction_key = f"{agent_id1}:{agent_id2}:{interaction_type}"

        # Check if we already have this interaction pattern
        if interaction_key in self.interaction_patterns:
            return self.interaction_patterns[interaction_key]

        # Generate new interaction pattern
        prompt = f"""
        Design an optimal interaction pattern between these two agents:

        AGENT 1: {agent_id1} ({agent1['agent_type']})
        Capabilities: {', '.join(agent1.get('capabilities', []))}
        Self-description: {agent1.get('self_description', 'No description')}

        AGENT 2: {agent_id2} ({agent2['agent_type']})
        Capabilities: {', '.join(agent2.get('capabilities', []))}
        Self-description: {agent2.get('self_description', 'No description')}

        INTERACTION TYPE: {interaction_type}

        Create a detailed interaction protocol that specifies:
        1. Information exchange format
        2. Sequence of operations
        3. Decision boundaries (which agent decides what)
        4. Failure handling procedures
        5. Success criteria

        Return this as a structured JSON pattern that both agents can follow to optimize their collaboration.
        """

        pattern_result = await llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            pattern = json.loads(pattern_result)
            self.interaction_patterns[interaction_key] = pattern
            return pattern
        except:
            # Fallback basic pattern
            basic_pattern = {
                "information_exchange": {
                    "format": "json",
                    "required_fields": ["request_type", "payload", "metadata"]
                },
                "sequence": [
                    {"step": 1, "agent": agent_id1, "action": "initiate_request"},
                    {"step": 2, "agent": agent_id2, "action": "process_request"},
                    {"step": 3, "agent": agent_id2, "action": "return_results"},
                    {"step": 4, "agent": agent_id1, "action": "integrate_results"}
                ],
                "error_handling": {
                    "timeout": "retry_once_then_simplify",
                    "incomplete_result": "request_clarification"
                }
            }
            self.interaction_patterns[interaction_key] = basic_pattern
            return basic_pattern
2. Enhanced Orchestrator with Dynamic Agent Management
pythonCopyclass DynamicOrchestrator:
    """
    Enhanced orchestrator with dynamic agent management and adaptive workflow configuration.
    """

    def __init__(self, vector_db, llm_service=None, resource_monitor=None):
        self.vector_db = vector_db
        self.llm_service = llm_service or LLMService(default_model="gpt-4")
        self.resource_monitor = resource_monitor or ResourceMonitor()

        # Agent management
        self.agent_metadata = AgentMetadataManager()
        self.active_agents = {}

        # Session management
        self.sessions = {}

        # Workflow templates
        self.workflow_templates = {}

        # Performance tracking
        self.performance_metrics = {}

    async def initialize_system(self):
        """Initialize the orchestrator and register core agents."""
        # Register core agents
        await self._register_core_agents()

        # Generate standard workflow templates
        await self._generate_workflow_templates()

        # Initialize performance tracking
        self._initialize_performance_tracking()

    async def _register_core_agents(self):
        """Register and initialize all core system agents."""
        # Research Agent
        research_agent_id = "research_agent"
        research_agent = ResearchAgent(self.vector_db, self.llm_service, self.resource_monitor)
        await self.agent_metadata.register_agent(
            agent_id=research_agent_id,
            agent_type="research",
            capabilities=["vector_search", "query_analysis", "information_synthesis"],
            llm_service=self.llm_service
        )
        self.active_agents[research_agent_id] = research_agent

        # Response Generator
        response_agent_id = "response_generator"
        response_generator = ResponseGenerator(self.llm_service)
        await self.agent_metadata.register_agent(
            agent_id=response_agent_id,
            agent_type="response_generation",
            capabilities=["context_integration", "query_interpretation", "narrative_construction"],
            llm_service=self.llm_service
        )
        self.active_agents[response_agent_id] = response_generator

        # Memory Manager
        memory_manager_id = "memory_manager"
        memory_manager = {
            "conversation": ConversationMemoryManager(self.llm_service),
            "knowledge": KnowledgeContextManager(self.llm_service)
        }
        await self.agent_metadata.register_agent(
            agent_id=memory_manager_id,
            agent_type="memory_management",
            capabilities=["conversation_tracking", "knowledge_prioritization", "context_compression"],
            llm_service=self.llm_service
        )
        self.active_agents[memory_manager_id] = memory_manager

        # Multi-Strategy Searcher
        search_agent_id = "search_agent"
        search_agent = MultiStrategySearcher(self.vector_db, self.llm_service)
        await self.agent_metadata.register_agent(
            agent_id=search_agent_id,
            agent_type="search",
            capabilities=["query_type_detection", "multi_strategy_search", "result_aggregation"],
            llm_service=self.llm_service
        )
        self.active_agents[search_agent_id] = search_agent

    async def _generate_workflow_templates(self):
        """Generate workflow templates for common query types."""
        query_types = ["factual", "conceptual", "exploratory", "comparative", "clarification", "follow_up"]

        for query_type in query_types:
            workflow = await self._generate_workflow_for_query_type(query_type)
            self.workflow_templates[query_type] = workflow

    async def _generate_workflow_for_query_type(self, query_type):
        """Generate an optimal workflow template for a specific query type."""
        # Get all agent IDs and their capabilities
        agent_info = {}
        for agent_id, agent_data in self.agent_metadata.agent_registry.items():
            agent_info[agent_id] = {
                "type": agent_data["agent_type"],
                "capabilities": agent_data.get("capabilities", [])
            }

        prompt = f"""
        Design an optimal workflow for processing a {query_type} query in a knowledge corpus research system.

        AVAILABLE AGENTS:
        {json.dumps(agent_info, indent=2)}

        QUERY TYPE: {query_type}

        Create a detailed workflow that specifies:
        1. Sequence of agent operations
        2. Data flow between agents
        3. Decision points and criteria
        4. Parameter adjustment opportunities
        5. Success criteria for each stage

        The workflow should be adaptive to query complexity and available resources.
        Return as a structured JSON workflow that the orchestrator can execute.
        """

        workflow_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(workflow_result)
        except:
            # Fallback to basic workflow
            return {
                "steps": [
                    {"step": 1, "agent": "research_agent", "operation": "research", "required": True},
                    {"step": 2, "agent": "memory_manager", "operation": "update_knowledge", "required": True},
                    {"step": 3, "agent": "response_generator", "operation": "generate_response", "required": True}
                ],
                "decision_points": [
                    {"after_step": 1, "criterion": "information_sufficiency", "threshold": 0.7}
                ],
                "parameter_adjustments": [
                    {"step": 1, "parameter": "search_depth", "based_on": "query_complexity"}
                ]
            }

    async def process_query(self, query, session_id, conversation_history=None):
        """Process a user query with dynamic workflow selection and execution."""
        # Initialize session if new
        if session_id not in self.sessions:
            self.sessions[session_id] = await self._initialize_session(session_id)

        # Update conversation
        session = self.sessions[session_id]
        if conversation_history:
            session["conversation"] = conversation_history
        else:
            session["conversation"].append({"role": "user", "content": query})

        # Create execution context
        execution_context = {
            "query": query,
            "session_id": session_id,
            "timestamp": time.time(),
            "execution_id": f"exec_{int(time.time())}_{session_id}",
            "session": session
        }

        # Analyze query to determine optimal workflow
        workflow = await self._select_optimal_workflow(query, session_id)
        execution_context["workflow"] = workflow

        # Execute workflow
        result = await self._execute_workflow(workflow, execution_context)

        # Update session with response
        session["conversation"].append({"role": "assistant", "content": result["response"]})
        session["last_activity"] = time.time()

        # Update performance metrics
        self._update_performance_metrics(execution_context, result)

        return result["response"]

    async def _initialize_session(self, session_id):
        """Initialize a new session with default configuration."""
        # Get resource state for initial configuration
        resource_state = self.resource_monitor.get_current_state()

        # Generate session configuration with LLM
        prompt = f"""
        Create an optimal initial configuration for a new knowledge corpus research session.

        SYSTEM RESOURCE STATE:
        {json.dumps(resource_state, indent=2)}

        AVAILABLE AGENTS:
        {json.dumps(list(self.active_agents.keys()), indent=2)}

        Generate a session configuration that includes:
        1. Default parameter values for each agent
        2. Resource allocation strategy
        3. Context management settings
        4. Starting relevance thresholds

        The configuration should be balanced for a new user with unknown preferences.
        """

        config_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        try:
            session_config = json.loads(config_result)
        except:
            # Fallback basic configuration
            session_config = {
                "parameters": {
                    "research_agent": {
                        "top_k": 10,
                        "max_iterations": 2,
                        "similarity_threshold": 0.7
                    },
                    "response_generator": {
                        "detail_level": 7,
                        "include_citations": True
                    }
                },
                "context_management": {
                    "conversation_importance": 5,
                    "knowledge_importance": 8,
                    "max_conversation_tokens": 4000,
                    "max_knowledge_tokens": 8000
                }
            }

        # Create the session
        return {
            "session_id": session_id,
            "created_at": time.time(),
            "last_activity": time.time(),
            "conversation": [],
            "knowledge_context": [],
            "configuration": session_config,
            "metrics": {
                "total_queries": 0,
                "avg_response_time": 0,
                "information_sufficiency": []
            }
        }

    async def _select_optimal_workflow(self, query, session_id):
        """Select the optimal workflow for this query based on analysis."""
        session = self.sessions[session_id]
        conversation = session["conversation"]

        # Create query analysis prompt
        context_text = self._format_conversation(conversation[-5:] if len(conversation) > 5 else conversation)

        prompt = f"""
        Analyze this query to determine the optimal processing workflow.

        QUERY: {query}

        CONVERSATION CONTEXT:
        {context_text}

        Determine:
        1. The primary query type (factual, conceptual, exploratory, comparative, clarification, follow_up)
        2. Query complexity (1-10)
        3. Expected search difficulty (1-10)
        4. The relative importance of conversation history vs. new knowledge (percentage split)

        Return your analysis as a JSON object with these determinations and a brief explanation.
        """

        analysis_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        try:
            analysis = json.loads(analysis_result)
            query_type = analysis.get("primary_query_type", "factual").lower()

            # Get the workflow template
            if query_type in self.workflow_templates:
                workflow = deepcopy(self.workflow_templates[query_type])
            else:
                workflow = deepcopy(self.workflow_templates["factual"])

            # Customize workflow based on analysis
            workflow["query_analysis"] = analysis
            workflow = await self._customize_workflow(workflow, analysis, session)

            return workflow

        except:
            # Fallback to basic factual workflow
            return deepcopy(self.workflow_templates["factual"])

    async def _customize_workflow(self, workflow, analysis, session):
        """Customize workflow based on query analysis and session configuration."""
        # Apply complexity-based adjustments
        complexity = analysis.get("query_complexity", 5)

        if complexity >= 8:
            # For highly complex queries, add more research iterations
            for step in workflow["steps"]:
                if step.get("agent") == "research_agent" and step.get("operation") == "research":
                    step["parameters"] = step.get("parameters", {})
                    step["parameters"]["max_iterations"] = 4
                    step["parameters"]["top_k"] = 15

        # Apply search difficulty adjustments
        search_difficulty = analysis.get("expected_search_difficulty", 5)

        if search_difficulty >= 7:
            # For difficult searches, add progressive search refinement
            has_search_step = False
            for step in workflow["steps"]:
                if step.get("agent") == "search_agent":
                    has_search_step = True
                    step["parameters"] = step.get("parameters", {})
                    step["parameters"]["use_progressive_refinement"] = True
                    break

            if not has_search_step:
                # Add search step if not present
                search_step = {
                    "step": len(workflow["steps"]) + 1,
                    "agent": "search_agent",
                    "operation": "search",
                    "required": True,
                    "parameters": {
                        "use_progressive_refinement": True
                    }
                }

                # Insert before research step
                research_index = None
                for i, step in enumerate(workflow["steps"]):
                    if step.get("agent") == "research_agent":
                        research_index = i
                        break

                if research_index is not None:
                    workflow["steps"].insert(research_index, search_step)
                else:
                    workflow["steps"].append(search_step)

        # Apply conversation history importance
        history_importance = analysis.get("conversation_history_importance", 50)

        if history_importance > 70:
            # Prioritize conversation context
            for step in workflow["steps"]:
                if step.get("agent") == "memory_manager":
                    step["parameters"] = step.get("parameters", {})
                    step["parameters"]["conversation_importance"] = 8
                    step["parameters"]["knowledge_importance"] = 5

        return workflow

    async def _execute_workflow(self, workflow, context):
        """Execute a workflow with the given context."""
        start_time = time.time()
        results = {"execution_log": []}

        # Execute each step in sequence
        for step in workflow["steps"]:
            step_result = await self._execute_step(step, context, results)
            results["execution_log"].append({
                "step": step,
                "success": step_result["success"],
                "execution_time": step_result["execution_time"]
            })

            # Add step output to results
            if "output" in step_result:
                output_key = f"{step['agent']}_{step['operation']}_output"
                results[output_key] = step_result["output"]

            # Check for decision points after this step
            decision_points = [d for d in workflow.get("decision_points", []) if d.get("after_step") == step.get("step")]
            for decision in decision_points:
                decision_result = await self._evaluate_decision(decision, context, results)
                results["execution_log"].append({
                    "decision": decision,
                    "result": decision_result
                })

                # If decision indicates to skip remaining steps, break
                if decision_result.get("skip_remaining", False):
                    break

        # Generate final response if not already set
        if "response" not in results and "response_generator_generate_response_output" in results:
            results["response"] = results["response_generator_generate_response_output"]

        # Calculate total execution time
        results["total_execution_time"] = time.time() - start_time

        return results

    async def _execute_step(self, step, context, previous_results):
        """Execute a single workflow step."""
        start_time = time.time()
        result = {
            "success": False,
            "execution_time": 0
        }

        try:
            agent_id = step["agent"]
            operation = step["operation"]
            parameters = step.get("parameters", {})

            # Get the agent
            if agent_id not in self.active_agents:
                raise ValueError(f"Agent {agent_id} not found")

            agent = self.active_agents[agent_id]

            # Execute the operation
            if agent_id == "research_agent" and operation == "research":
                # Research operation
                research_results = await agent.research(
                    query=context["query"],
                    context=context["session"]["conversation"],
                    initial_params=parameters
                )
                result["output"] = research_results
                result["success"] = True

            elif agent_id == "memory_manager" and operation == "update_knowledge":
                # Update knowledge context
                knowledge_manager = agent["knowledge"]
                research_results = previous_results.get("research_agent_research_output", {})
                await knowledge_manager.update_knowledge(
                    session_id=context["session_id"],
                    new_chunks=research_results.get("chunks", []),
                    query=context["query"]
                )
                result["success"] = True

            elif agent_id == "response_generator" and operation == "generate_response":
                # Generate response
                memory_manager = self.active_agents["memory_manager"]
                knowledge_chunks = await memory_manager["knowledge"].get_relevant_knowledge(
                    session_id=context["session_id"],
                    query=context["query"],
                    max_chunks=parameters.get("max_chunks", 10)
                )

                response = await agent.generate_response(
                    query=context["query"],
                    knowledge_chunks=knowledge_chunks,
                    conversation_history=context["session"]["conversation"],
                    response_params=parameters
                )
                result["output"] = response
                result["success"] = True

            elif agent_id == "search_agent" and operation == "search":
                # Execute search
                query_type = context["workflow"].get("query_analysis", {}).get("primary_query_type", "auto")
                search_results, metadata = await agent.search(
                    query=context["query"],
                    query_type=query_type,
                    max_results=parameters.get("max_results", 20)
                )
                result["output"] = search_results
                result["metadata"] = metadata
                result["success"] = True

            # Add other operations as needed

        except Exception as e:
            result["error"] = str(e)

        # Record execution time
        result["execution_time"] = time.time() - start_time

        # If step failed but is required, try fallback
        if not result["success"] and step.get("required", False):
            fallback_result = await self._execute_fallback(step, context, previous_results)
            if fallback_result["success"]:
                return fallback_result

        return result

    async def _execute_fallback(self, failed_step, context, previous_results):
        """Execute a fallback for a failed required step."""
        # Generate fallback strategy
        prompt = f"""
        A required step in the workflow has failed and needs a fallback strategy.

        FAILED STEP:
        {json.dumps(failed_step, indent=2)}

        ERROR:
        {previous_results.get("error", "Unknown error")}

        CONTEXT:
        - Query: {context["query"]}
        - Session ID: {context["session_id"]}

        Generate a fallback strategy that will:
        1. Accomplish the essential parts of the failed step
        2. Use simpler or alternative approaches
        3. Specify which agent and operation to use instead
        4. Provide simplified parameters

        Return as a JSON object with the fallback step specification.
        """

        fallback_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            fallback_step = json.loads(fallback_result)
            return await self._execute_step(fallback_step, context, previous_results)
        except:
            # If fallback fails, create minimal success result
            if failed_step["agent"] == "research_agent":
                return {
                    "success": True,
                    "execution_time": 0.1,
                    "output": {"chunks": [], "information_sufficiency": 0}
                }
            elif failed_step["agent"] == "response_generator":
                return {
                    "success": True,
                    "execution_time": 0.1,
                    "output": "I'm sorry, but I couldn't find relevant information to answer your question. Could you try rephrasing or asking something else?"
                }
            else:
                return {
                    "success": True,
                    "execution_time": 0.1,
                    "output": None
                }

    async def _evaluate_decision(self, decision, context, results):
        """Evaluate a decision point in the workflow."""
        criterion = decision.get("criterion", "")
        threshold = decision.get("threshold", 0.5)

        if criterion == "information_sufficiency":
            # Check research results for sufficiency
            research_results = results.get("research_agent_research_output", {})
            sufficiency = research_results.get("information_sufficiency", 0)

            return {
                "criterion_value": sufficiency,
                "threshold": threshold,
                "passed": sufficiency >= threshold,
                "skip_remaining": False
            }

        elif criterion == "query_answered":
            # Check if query is already answered in conversation history
            conversation = context["session"]["conversation"]

            # Create prompt to check if query is already answered
            query = context["query"]
            recent_conversation = conversation[-6:] if len(conversation) > 6 else conversation

            prompt = f"""
            Determine if this query has already been answered in the recent conversation history.

            QUERY: {query}

            RECENT CONVERSATION:
            {self._format_conversation(recent_conversation)}

            Return a JSON with:
            1. already_answered: true/false
            2. confidence: 0.0-1.0 (how confident you are in this assessment)
            3. location: message index where it was answered, if applicable
            """

            assessment_result = await self.llm_service.generate_completion(
                prompt=prompt,
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            try:
                assessment = json.loads(assessment_result)
                return {
                    "criterion_value": assessment.get("confidence", 0) if assessment.get("already_answered", False) else 0,
                    "threshold": threshold,
                    "passed": assessment.get("already_answered", False) and assessment.get("confidence", 0) >= threshold,
                    "skip_remaining": assessment.get("already_answered", False) and assessment.get("confidence", 0) >= threshold
                }
            except:
                return {"passed": False, "skip_remaining": False}

        # Default response for unknown criteria
        return {"passed": False, "skip_remaining": False}
3. Dynamic Parameter Optimizer Agent
pythonCopyclass ParameterOptimizerAgent:
    """
    Agent dedicated to dynamically optimizing system parameters based on performance metrics,
    query types, and resource availability.
    """

    def __init__(self, llm_service=None, resource_monitor=None):
        self.llm_service = llm_service or LLMService()
        self.resource_monitor = resource_monitor or ResourceMonitor()
        self.parameter_history = {}
        self.optimization_notes = {}
        self.parameter_groups = self._define_parameter_groups()

    def _define_parameter_groups(self):
        """Define parameter groups and theirRetryClaude hit the max length for a message and has paused its response. You can write Continue to keep the chat going.EScontinueEdit3. Dynamic Parameter Optimizer Agent (continued)
pythonCopy    def _define_parameter_groups(self):
        """Define parameter groups and their relationships, constraints, and impact domains."""
        return {
            "search_parameters": {
                "parameters": ["top_k", "similarity_threshold", "max_iterations", "strategy_weight"],
                "impact_domains": ["accuracy", "recall", "response_time"],
                "constraints": [
                    {"type": "range", "parameter": "top_k", "min": 3, "max": 50},
                    {"type": "range", "parameter": "similarity_threshold", "min": 0.5, "max": 0.95},
                    {"type": "range", "parameter": "max_iterations", "min": 1, "max": 5}
                ],
                "trade_offs": [
                    {"increase": "top_k", "decreases": ["response_time"], "increases": ["recall"]}
                ]
            },
            "context_parameters": {
                "parameters": ["knowledge_importance", "conversation_importance", "knowledge_tokens", "conversation_tokens"],
                "impact_domains": ["coherence", "relevance", "memory_usage"],
                "constraints": [
                    {"type": "sum", "parameters": ["knowledge_importance", "conversation_importance"], "value": 10},
                    {"type": "range", "parameter": "knowledge_tokens", "min": 2000, "max": 30000},
                    {"type": "range", "parameter": "conversation_tokens", "min": 1000, "max": 10000}
                ]
            },
            "response_parameters": {
                "parameters": ["detail_level", "temperature", "max_tokens", "citation_style"],
                "impact_domains": ["satisfaction", "accuracy", "response_time"],
                "constraints": [
                    {"type": "range", "parameter": "detail_level", "min": 1, "max": 10},
                    {"type": "range", "parameter": "temperature", "min": 0.0, "max": 1.0}
                ]
            },
            "resource_parameters": {
                "parameters": ["batch_size", "parallel_processes", "model_selection"],
                "impact_domains": ["throughput", "cost", "response_time"],
                "constraints": [
                    {"type": "resource_dependent", "parameter": "batch_size"},
                    {"type": "resource_dependent", "parameter": "parallel_processes"}
                ]
            }
        }

    async def optimize_parameters(self, parameter_group, current_values, optimization_context):
        """
        Optimize parameters for a specific group based on context and performance history.

        Args:
            parameter_group: Name of the parameter group to optimize
            current_values: Current parameter values
            optimization_context: Context data for optimization (query type, performance metrics, etc.)

        Returns:
            Optimized parameter values with explanations
        """
        # Retrieve group definition
        if parameter_group not in self.parameter_groups:
            return {"error": f"Unknown parameter group: {parameter_group}", "values": current_values}

        group_def = self.parameter_groups[parameter_group]

        # Get resource state
        resource_state = self.resource_monitor.get_current_state() if self.resource_monitor else {}

        # Extract relevant performance history
        history = self._extract_relevant_history(parameter_group, optimization_context)

        # Generate optimization prompt
        prompt = self._create_optimization_prompt(
            parameter_group,
            group_def,
            current_values,
            optimization_context,
            resource_state,
            history
        )

        # Get LLM optimization recommendation
        optimization_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"},
            note_id=f"parameter_optimization_{parameter_group}_{int(time.time())}"
        )

        try:
            optimization = json.loads(optimization_result)

            # Validate optimized values against constraints
            validated_values = self._validate_parameters(optimization.get("values", {}), group_def)
            optimization["values"] = validated_values

            # Record optimization in history
            self._record_optimization(parameter_group, current_values, validated_values, optimization_context)

            # Store explanation notes
            if "explanations" in optimization:
                self.optimization_notes[f"{parameter_group}_{int(time.time())}"] = optimization["explanations"]

            return optimization

        except json.JSONDecodeError:
            # Return current values on failure
            return {"values": current_values, "error": "Failed to parse optimization result"}

    def _create_optimization_prompt(self, group_name, group_def, current_values, context, resource_state, history):
        """Create a detailed prompt for parameter optimization."""
        parameters = group_def.get("parameters", [])
        impact_domains = group_def.get("impact_domains", [])
        constraints = group_def.get("constraints", [])
        trade_offs = group_def.get("trade_offs", [])

        # Format current values
        current_values_str = json.dumps({p: current_values.get(p) for p in parameters if p in current_values}, indent=2)

        # Format constraints
        constraints_str = ""
        for constraint in constraints:
            if constraint["type"] == "range":
                constraints_str += f"- {constraint['parameter']} must be between {constraint['min']} and {constraint['max']}\n"
            elif constraint["type"] == "sum":
                constraints_str += f"- The sum of {', '.join(constraint['parameters'])} must equal {constraint['value']}\n"
            elif constraint["type"] == "resource_dependent":
                constraints_str += f"- {constraint['parameter']} depends on available system resources\n"

        # Format trade-offs
        tradeoffs_str = ""
        for tradeoff in trade_offs:
            increases = tradeoff.get("increases", [])
            decreases = tradeoff.get("decreases", [])
            tradeoffs_str += f"- Increasing {tradeoff['increase']} will "
            if increases:
                tradeoffs_str += f"increase {', '.join(increases)} "
            if increases and decreases:
                tradeoffs_str += "but "
            if decreases:
                tradeoffs_str += f"decrease {', '.join(decreases)}"
            tradeoffs_str += "\n"

        # Format context
        context_str = json.dumps(context, indent=2)

        # Format resource state
        resource_str = json.dumps(resource_state, indent=2)

        # Format history
        history_str = ""
        for entry in history:
            history_str += f"- Previous optimization ({entry['timestamp']}):\n"
            history_str += f"  Before: {json.dumps(entry['before'], indent=2)}\n"
            history_str += f"  After: {json.dumps(entry['after'], indent=2)}\n"
            history_str += f"  Performance change: {entry.get('performance_change', 'Unknown')}\n\n"

        # Build the prompt
        prompt = f"""
        You are a parameter optimization expert for a knowledge corpus research system.
        Optimize the parameters in the "{group_name}" group based on the context and history provided.

        PARAMETER GROUP: {group_name}

        PARAMETERS TO OPTIMIZE:
        {', '.join(parameters)}

        CURRENT VALUES:
        {current_values_str}

        IMPACT DOMAINS:
        {', '.join(impact_domains)}

        CONSTRAINTS:
        {constraints_str}

        TRADE-OFFS:
        {tradeoffs_str}

        OPTIMIZATION CONTEXT:
        {context_str}

        SYSTEM RESOURCE STATE:
        {resource_str}

        OPTIMIZATION HISTORY:
        {history_str if history_str else "No previous optimizations for this group."}

        Based on this information, determine the optimal values for each parameter.
        Consider the specific query type, resource state, and performance history.

        Return your optimization as a JSON object with:
        1. "values": A dictionary of parameter names to optimized values
        2. "explanations": A dictionary explaining the rationale for each parameter change
        3. "expected_impact": Expected impact on each relevant domain
        """

        return prompt

    def _validate_parameters(self, values, group_def):
        """Validate parameter values against their constraints."""
        validated = {}
        constraints = group_def.get("constraints", [])

        # Apply range constraints
        for constraint in constraints:
            if constraint["type"] == "range" and constraint["parameter"] in values:
                param = constraint["parameter"]
                validated[param] = max(constraint["min"], min(constraint["max"], values[param]))

        # Apply sum constraints
        for constraint in constraints:
            if constraint["type"] == "sum":
                params = constraint["parameters"]
                target_sum = constraint["value"]

                # Check if we have all the parameters in this constraint
                if all(p in values for p in params):
                    current_sum = sum(values[p] for p in params)

                    # If sum doesn't match, scale values proportionally
                    if current_sum != target_sum and current_sum > 0:
                        scale_factor = target_sum / current_sum
                        for p in params:
                            validated[p] = values[p] * scale_factor

        # Copy remaining values
        for param, value in values.items():
            if param not in validated:
                validated[param] = value

        return validated

    def _extract_relevant_history(self, parameter_group, context):
        """Extract relevant history for this parameter group and context."""
        if parameter_group not in self.parameter_history:
            return []

        history = self.parameter_history[parameter_group]

        # Filter by relevance to current context
        relevant_history = []
        query_type = context.get("query_type")

        for entry in history:
            entry_query_type = entry.get("context", {}).get("query_type")

            # Include if same query type or if recent (last 5 entries)
            if entry_query_type == query_type or len(relevant_history) < 5:
                relevant_history.append(entry)

        # Sort by recency and return most recent 10
        relevant_history.sort(key=lambda x: x.get("timestamp", 0), reverse=True)
        return relevant_history[:10]

    def _record_optimization(self, parameter_group, before_values, after_values, context):
        """Record an optimization in the history."""
        if parameter_group not in self.parameter_history:
            self.parameter_history[parameter_group] = []

        entry = {
            "timestamp": time.time(),
            "before": before_values,
            "after": after_values,
            "context": context
        }

        self.parameter_history[parameter_group].append(entry)

        # Keep history at a reasonable size
        if len(self.parameter_history[parameter_group]) > 100:
            self.parameter_history[parameter_group] = self.parameter_history[parameter_group][-100:]

    async def evaluate_parameter_impact(self, parameter_group, before_values, after_values, performance_metrics):
        """Evaluate the impact of parameter changes on performance."""
        if parameter_group not in self.parameter_groups:
            return {"error": f"Unknown parameter group: {parameter_group}"}

        group_def = self.parameter_groups[parameter_group]
        impact_domains = group_def.get("impact_domains", [])

        # Create evaluation prompt
        prompt = f"""
        Evaluate the impact of parameter changes on system performance.

        PARAMETER GROUP: {parameter_group}

        BEFORE VALUES:
        {json.dumps(before_values, indent=2)}

        AFTER VALUES:
        {json.dumps(after_values, indent=2)}

        PERFORMANCE METRICS:
        {json.dumps(performance_metrics, indent=2)}

        IMPACT DOMAINS:
        {', '.join(impact_domains)}

        Analyze how these parameter changes affected performance in each impact domain.
        Calculate specific improvements or regressions based on the metrics.

        Return your analysis as a JSON object with:
        1. "domain_impacts": A dictionary mapping each impact domain to a numeric score (-10 to +10)
        2. "overall_impact": An overall impact score (-10 to +10)
        3. "successful_changes": List of parameter changes that improved performance
        4. "detrimental_changes": List of parameter changes that worsened performance
        5. "recommendations": Suggestions for future parameter adjustments
        """

        evaluation_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        try:
            evaluation = json.loads(evaluation_result)

            # Update history with performance change
            for entry in self.parameter_history.get(parameter_group, []):
                if (entry.get("before") == before_values and
                    entry.get("after") == after_values and
                    "performance_change" not in entry):
                    entry["performance_change"] = evaluation.get("overall_impact")
                    break

            return evaluation

        except json.JSONDecodeError:
            return {
                "domain_impacts": {domain: 0 for domain in impact_domains},
                "overall_impact": 0,
                "error": "Failed to parse evaluation result"
            }
4. Enhanced Research Agent with Self-Improvement Capabilities
pythonCopyclass EnhancedResearchAgent:
    """
    Advanced research agent with self-improvement capabilities, dynamic strategy selection,
    and comprehensive understanding of research methodologies.
    """

    def __init__(self, vector_db, llm_service=None, resource_monitor=None):
        self.vector_db = vector_db
        self.llm_service = llm_service or LLMService(default_model="gpt-4")
        self.resource_monitor = resource_monitor or ResourceMonitor()

        # Research memory and learning
        self.research_memory = {}
        self.strategy_effectiveness = {}
        self.query_patterns = {}

        # Self-improvement
        self.improvement_history = []
        self.strategy_templates = {}

        # State tracking
        self.active_research_sessions = {}
        self.notes = {}

    async def initialize(self):
        """Initialize agent and generate initial research strategies."""
        await self._generate_research_strategies()

    async def _generate_research_strategies(self):
        """Generate initial set of research strategies for different query types."""
        query_types = [
            "factual", "conceptual", "comparative", "historical",
            "procedural", "exploratory", "investigative", "analytical"
        ]

        for query_type in query_types:
            strategy = await self._generate_strategy_for_query_type(query_type)
            self.strategy_templates[query_type] = strategy

    async def _generate_strategy_for_query_type(self, query_type):
        """Generate research strategy template for a specific query type."""
        prompt = f"""
        Design an optimal research strategy for a {query_type} query in a knowledge corpus.

        Create a comprehensive research strategy that specifies:

        1. Initial search approach (parameters, priorities, metadata focus)
        2. Information evaluation criteria
        3. Multi-iteration research plan
        4. Information synthesis approach
        5. Specialized techniques for {query_type} queries
        6. Potential pitfalls and mitigations
        7. Success criteria

        The strategy should be adaptable to different complexities and domains
        while optimized for {query_type} information needs.

        Return as a structured JSON research strategy template.
        """

        strategy_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(strategy_result)
        except:
            # Fallback basic strategy
            return {
                "query_type": query_type,
                "initial_search": {
                    "top_k": 10,
                    "similarity_threshold": 0.7,
                    "priority": "relevance"
                },
                "iterations": 2,
                "evaluation_criteria": ["relevance", "completeness", "authority"],
                "success_criteria": {
                    "min_information_sufficiency": 0.7,
                    "min_sources": 3
                }
            }

    async def research(self, query, context=None, initial_params=None):
        """
        Conduct research with dynamic parameter adjustment and strategy selection.

        Args:
            query: User's question
            context: Optional conversation context
            initial_params: Optional initial parameters

        Returns:
            Research results
        """
        # Generate a unique ID for this research session
        research_id = f"research_{int(time.time())}"

        # Analyze query to determine type and optimal strategy
        query_analysis = await self._analyze_query(query, context)

        # Create research plan based on query analysis and selected strategy
        research_plan = await self._create_research_plan(query, query_analysis, initial_params)

        # Record plan and analysis
        self.notes[f"{research_id}_query_analysis"] = query_analysis
        self.notes[f"{research_id}_research_plan"] = research_plan

        # Execute initial search
        initial_results = await self._execute_search(
            query=query,
            params=research_plan["search_parameters"]
        )

        # Analyze results to determine next steps
        analysis = await self._analyze_results(query, initial_results, research_plan, context)

        # Record analysis
        self.notes[f"{research_id}_initial_analysis"] = analysis

        # Initialize active research session
        self.active_research_sessions[research_id] = {
            "query": query,
            "query_analysis": query_analysis,
            "plan": research_plan,
            "results": initial_results,
            "analysis": analysis,
            "iterations": 1,
            "iteration_analyses": [analysis],
            "all_results": initial_results,
            "seen_chunk_ids": {result.metadata["chunk_id"] for result in initial_results}
        }

        # Check if we need further research
        if (analysis["information_sufficiency"] >= research_plan["success_criteria"]["min_information_sufficiency"]
            or not analysis["follow_up_queries"] or research_plan["max_iterations"] <= 1):
            # Initial research is sufficient
            self.notes[f"{research_id}_conclusion"] = "Initial research sufficient"
            results = self._prepare_final_results(research_id)

            # Update strategy effectiveness
            self._update_strategy_effectiveness(
                query_type=query_analysis["query_type"],
                success_level=analysis["information_sufficiency"],
                iterations=1
            )

            return results

        # Conduct iterative research
        session = self.active_research_sessions[research_id]

        for iteration in range(2, research_plan["max_iterations"] + 1):
            # Adjust parameters based on previous results
            adjusted_params = await self._adjust_search_parameters(
                research_id=research_id,
                iteration=iteration,
                previous_analysis=analysis
            )

            # Record parameter adjustments
            self.notes[f"{research_id}_iteration_{iteration}_params"] = adjusted_params

            # Execute follow-up queries
            iteration_results = []

            for follow_up_query in analysis["follow_up_queries"]:
                # Execute search
                query_results = await self._execute_search(
                    query=follow_up_query,
                    params=adjusted_params
                )

                # Filter out already seen chunks
                new_results = [
                    result for result in query_results
                    if result.metadata["chunk_id"] not in session["seen_chunk_ids"]
                ]

                # Update seen chunks
                for result in new_results:
                    session["seen_chunk_ids"].add(result.metadata["chunk_id"])

                # Add to iteration results
                iteration_results.extend(new_results)

            # Record iteration results
            self.notes[f"{research_id}_iteration_{iteration}_results_count"] = len(iteration_results)

            # If no new results, break
            if not iteration_results:
                self.notes[f"{research_id}_conclusion"] = "No new results found in iteration"
                break

            # Add new results to overall collection
            session["all_results"].extend(iteration_results)

            # Analyze combined results
            analysis = await self._analyze_results(
                query=query,
                results=session["all_results"],
                research_plan=research_plan,
                context=context
            )

            # Update session
            session["analysis"] = analysis
            session["iteration_analyses"].append(analysis)
            session["iterations"] = iteration

            # Record analysis
            self.notes[f"{research_id}_iteration_{iteration}_analysis"] = analysis

            # Check if we've reached sufficient information
            if analysis["information_sufficiency"] >= research_plan["success_criteria"]["min_information_sufficiency"]:
                self.notes[f"{research_id}_conclusion"] = "Sufficient information reached"
                break

        # Prepare final results
        results = self._prepare_final_results(research_id)

        # Update strategy effectiveness
        self._update_strategy_effectiveness(
            query_type=query_analysis["query_type"],
            success_level=analysis["information_sufficiency"],
            iterations=session["iterations"]
        )

        # Learn from this research session
        await self._learn_from_research_session(research_id)

        return results

    async def _analyze_query(self, query, context=None):
        """Analyze query to determine type, complexity, and optimal strategy."""
        # Format context if available
        context_text = ""
        if context:
            context_text = "CONVERSATION CONTEXT:\n"
            for msg in context[-5:]:  # Use last 5 messages
                role = "User" if msg["role"] == "user" else "Assistant"
                context_text += f"{role}: {msg['content'][:200]}...\n"

        # Create analysis prompt
        prompt = f"""
        Analyze this query to determine the optimal research strategy.

        QUERY:
        {query}

        {context_text}

        Provide a detailed analysis including:

        1. Query type (factual, conceptual, comparative, historical, procedural, exploratory, investigative, analytical)
        2. Complexity level (1-10)
        3. Subject domain (general knowledge, specialized field, technical, etc.)
        4. Key entities that should be researched
        5. Expected information structure (facts, relationships, processes, etc.)
        6. Potential information sources within a knowledge corpus
        7. Special considerations for this query

        Return your analysis as a structured JSON object.
        """

        analysis_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(analysis_result)
        except:
            # Fallback basic analysis
            return {
                "query_type": "factual",
                "complexity": 5,
                "subject_domain": "general knowledge",
                "key_entities": [query],
                "information_structure": "facts",
                "potential_sources": ["general documents"]
            }

    async def _create_research_plan(self, query, query_analysis, initial_params=None):
        """Create a research plan based on query analysis and strategy templates."""
        # Get query type
        query_type = query_analysis.get("query_type", "factual").lower()

        # Get the strategy template
        strategy = None
        if query_type in self.strategy_templates:
            strategy = deepcopy(self.strategy_templates[query_type])
        else:
            # Fallback to factual or the first available strategy
            strategy = deepcopy(self.strategy_templates.get(
                "factual",
                next(iter(self.strategy_templates.values())) if self.strategy_templates else {}
            ))

        # Adjust strategy based on query complexity
        complexity = query_analysis.get("complexity", 5)

        # Create base research plan from strategy
        research_plan = {
            "query": query,
            "query_type": query_type,
            "complexity": complexity,
            "subject_domain": query_analysis.get("subject_domain", "general"),
            "key_entities": query_analysis.get("key_entities", []),
            "max_iterations": min(5, strategy.get("iterations", 2) + (complexity // 3)),
            "search_parameters": {
                "top_k": strategy.get("initial_search", {}).get("top_k", 10),
                "similarity_threshold": strategy.get("initial_search", {}).get("similarity_threshold", 0.7),
                "filters": {}
            },
            "evaluation_criteria": strategy.get("evaluation_criteria", ["relevance", "completeness"]),
            "success_criteria": strategy.get("success_criteria", {
                "min_information_sufficiency": 0.7,
                "min_sources": 3
            })
        }

        # Adjust for complexity
        if complexity >= 8:
            research_plan["max_iterations"] = min(5, research_plan["max_iterations"] + 1)
            research_plan["search_parameters"]["top_k"] = min(30, research_plan["search_parameters"]["top_k"] + 5)
            research_plan["success_criteria"]["min_information_sufficiency"] = max(
                0.6, research_plan["success_criteria"]["min_information_sufficiency"] - 0.1
            )
        elif complexity <= 3:
            research_plan["max_iterations"] = max(1, research_plan["max_iterations"] - 1)
            research_plan["search_parameters"]["top_k"] = max(5, research_plan["search_parameters"]["top_k"] - 3)
            research_plan["success_criteria"]["min_information_sufficiency"] = min(
                0.9, research_plan["success_criteria"]["min_information_sufficiency"] + 0.1
            )

        # Apply domain-specific adjustments
        domain = query_analysis.get("subject_domain", "general").lower()
        if "technical" in domain or "specialized" in domain:
            research_plan["search_parameters"]["similarity_threshold"] = min(
                0.95, research_plan["search_parameters"]["similarity_threshold"] + 0.05
            )

        # Apply filters based on key entities if appropriate
        if query_type in ["comparative", "historical"] and len(query_analysis.get("key_entities", [])) > 0:
            entities = query_analysis["key_entities"]
            research_plan["search_parameters"]["filters"]["key_entities"] = entities

        # Override with initial params if provided
        if initial_params:
            # Deep merge
            for key, value in initial_params.items():
                if isinstance(value, dict) and key in research_plan and isinstance(research_plan[key], dict):
                    research_plan[key].update(value)
                else:
                    research_plan[key] = value

        return research_plan

    async def _execute_search(self, query, params):
        """Execute a search with the given parameters."""
        # Extract search parameters
        top_k = params.get("top_k", 10)
        similarity_threshold = params.get("similarity_threshold", 0.7)
        filters = params.get("filters", {})

        # Convert filters to vector DB format
        filter_dict = {}
        for key, value in filters.items():
            if isinstance(value, list):
                filter_dict[key] = {"$in": value}
            else:
                filter_dict[key] = value

        # Execute search
        try:
            results = await self.vector_db.similarity_search(
                query=query,
                top_k=top_k,
                threshold=similarity_threshold,
                filter_dict=filter_dict if filter_dict else None
            )
            return results
        except Exception as e:
            # Log error
            self.notes[f"search_error_{int(time.time())}"] = str(e)

            # Fallback search with simplified parameters
            try:
                return await self.vector_db.similarity_search(
                    query=query,
                    top_k=5,
                    threshold=0.6
                )
            except:
                # If all fails, return empty results
                return []

    async def _analyze_results(self, query, results, research_plan, context=None):
        """Analyze search results to determine next steps."""
        # Format results for analysis
        formatted_results = self._format_results_for_analysis(results)

        # Format context if available
        context_text = ""
        if context:
            context_text = "CONVERSATION CONTEXT:\n"
            for msg in context[-3:]:  # Use last 3 messages
                role = "User" if msg["role"] == "user" else "Assistant"
                context_text += f"{role}: {msg['content'][:100]}...\n"

        # Get evaluation criteria
        criteria = research_plan.get("evaluation_criteria", ["relevance", "completeness"])
        criteria_str = ", ".join(criteria)

        # Create analysis prompt
        prompt = f"""
        Analyze these search results for the query: "{query}"

        {context_text}

        SEARCH RESULTS:
        {formatted_results}

        EVALUATION CRITERIA:
        {criteria_str}

        QUERY TYPE:
        {research_plan.get("query_type", "factual")}

        Provide a comprehensive analysis of these results including:

        1. Information sufficiency score (0.0-1.0) indicating how well the results answer the query
        2. Key findings from the results, organized by subtopic
        3. Information gaps that need to be filled
        4. Specific follow-up queries to address those gaps (max 3)
        5. Relevance assessment of each result
        6. Metadata patterns observed in relevant results
        7. Preliminary synthesis of the information found

        Return your analysis as a structured JSON object.
        """

        analysis_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(analysis_result)
        except:
            # Fallback basic analysis
            return {
                "information_sufficiency": 0.5,
                "key_findings": ["Unable to parse key findings"],
                "information_gaps": ["Unable to parse information gaps"],
                "follow_up_queries": [f"more about {query}"],
                "relevance_assessment": {},
                "metadata_patterns": {},
                "preliminary_synthesis": f"Information about {query}"
            }

    async def _adjust_search_parameters(self, research_id, iteration, previous_analysis):
        """Adjust search parameters based on previous results analysis."""
        session = self.active_research_sessions[research_id]

        # Get current parameters
        current_params = session["plan"]["search_parameters"]

        # Extract information from analysis
        information_sufficiency = previous_analysis.get("information_sufficiency", 0)
        gaps = previous_analysis.get("information_gaps", [])
        irrelevant_count = len(previous_analysis.get("relevance_assessment", {}).get("irrelevant", []))
        total_results = len(session["all_results"])

        # Create parameter adjustment prompt
        prompt = f"""
        Adjust search parameters for iteration {iteration} based on previous results.

        CURRENT PARAMETERS:
        {json.dumps(current_params, indent=2)}RetryClaude hit the max length for a message and has paused its response. You can write Continue to keep the chat going.EScontinueEdit4. Enhanced Research Agent with Self-Improvement Capabilities (continued)
pythonCopy        # Create parameter adjustment prompt
        prompt = f"""
        Adjust search parameters for iteration {iteration} based on previous results.

        CURRENT PARAMETERS:
        {json.dumps(current_params, indent=2)}

        PREVIOUS RESULTS ANALYSIS:
        - Information sufficiency: {information_sufficiency}
        - Information gaps: {', '.join(gaps)}
        - Irrelevant results: {irrelevant_count} out of {total_results}

        QUERY TYPE:
        {session["query_analysis"].get("query_type", "factual")}

        QUERY COMPLEXITY:
        {session["query_analysis"].get("complexity", 5)}

        Determine optimal parameter adjustments to:
        1. Address identified information gaps
        2. Improve result relevance
        3. Discover new relevant information

        Return adjusted parameters as a JSON object with the same structure as the current parameters.
        Include explanations for each significant parameter change.
        """

        adjustment_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        try:
            adjustment = json.loads(adjustment_result)

            # Extract adjusted parameters, explanations, and ensure proper structure
            adjusted_params = adjustment.get("parameters", adjustment)
            if "explanations" in adjustment:
                self.notes[f"{research_id}_iteration_{iteration}_adjustment_explanations"] = adjustment["explanations"]

            return adjusted_params

        except:
            # Fallback adjustments based on heuristics
            adjusted_params = deepcopy(current_params)

            # If low information sufficiency, broaden search
            if information_sufficiency < 0.3:
                adjusted_params["top_k"] = min(30, current_params.get("top_k", 10) + 5)
                adjusted_params["similarity_threshold"] = max(0.5, current_params.get("similarity_threshold", 0.7) - 0.1)

            # If high irrelevant count, increase relevance threshold
            if irrelevant_count > total_results * 0.4:
                adjusted_params["similarity_threshold"] = min(0.9, current_params.get("similarity_threshold", 0.7) + 0.05)

            return adjusted_params

    def _prepare_final_results(self, research_id):
        """Prepare final results object from a research session."""
        if research_id not in self.active_research_sessions:
            return {"error": "Research session not found", "chunks": []}

        session = self.active_research_sessions[research_id]

        # Get final analysis
        analysis = session["analysis"]

        # Organize and deduplicate results
        organized_results = self._organize_results(session["all_results"], analysis)

        # Build final result object
        result = {
            "research_id": research_id,
            "query": session["query"],
            "query_type": session["query_analysis"].get("query_type", "factual"),
            "chunks": organized_results,
            "chunk_count": len(organized_results),
            "key_findings": analysis.get("key_findings", []),
            "information_gaps": analysis.get("information_gaps", []),
            "information_sufficiency": analysis.get("information_sufficiency", 0),
            "iterations": session["iterations"],
            "synthesis": analysis.get("preliminary_synthesis", ""),
            "execution_metrics": {
                "total_iterations": session["iterations"],
                "total_results_found": len(session["all_results"]),
                "unique_chunks": len(session["seen_chunk_ids"])
            }
        }

        # Store in research memory
        self.research_memory[research_id] = {
            "timestamp": time.time(),
            "query": session["query"],
            "query_analysis": session["query_analysis"],
            "plan": session["plan"],
            "results": result,
            "notes": {k: v for k, v in self.notes.items() if k.startswith(f"{research_id}_")}
        }

        return result

    def _organize_results(self, results, analysis):
        """Organize results by relevance and subtopic."""
        # Extract relevance assessments if available
        relevance_assessment = analysis.get("relevance_assessment", {})
        highly_relevant = set(relevance_assessment.get("highly_relevant", []))
        relevant = set(relevance_assessment.get("relevant", []))

        # Get subtopic organization if available
        subtopic_organization = analysis.get("subtopic_organization", {})

        # Sort results by relevance
        sorted_results = []

        # Add highly relevant results first
        for i, result in enumerate(results):
            chunk_id = result.metadata.get("chunk_id", f"chunk_{i}")
            if i in highly_relevant or chunk_id in highly_relevant:
                result.metadata["relevance"] = "high"
                sorted_results.append(result)

        # Add relevant results next
        for i, result in enumerate(results):
            chunk_id = result.metadata.get("chunk_id", f"chunk_{i}")
            if (i in relevant or chunk_id in relevant) and result not in sorted_results:
                result.metadata["relevance"] = "medium"
                sorted_results.append(result)

        # Add remaining results
        for result in results:
            if result not in sorted_results:
                result.metadata["relevance"] = "low"
                sorted_results.append(result)

        # Add subtopic metadata if available
        if subtopic_organization:
            for subtopic, indices in subtopic_organization.items():
                for idx in indices:
                    if 0 <= idx < len(sorted_results):
                        sorted_results[idx].metadata["subtopic"] = subtopic

        return sorted_results

    def _update_strategy_effectiveness(self, query_type, success_level, iterations):
        """Update strategy effectiveness metrics."""
        if query_type not in self.strategy_effectiveness:
            self.strategy_effectiveness[query_type] = {
                "total_uses": 0,
                "success_sum": 0,
                "iteration_sum": 0,
                "high_success_count": 0,
                "low_success_count": 0
            }

        metrics = self.strategy_effectiveness[query_type]
        metrics["total_uses"] += 1
        metrics["success_sum"] += success_level
        metrics["iteration_sum"] += iterations

        if success_level >= 0.8:
            metrics["high_success_count"] += 1
        elif success_level <= 0.4:
            metrics["low_success_count"] += 1

    async def _learn_from_research_session(self, research_id):
        """Learn from completed research session to improve strategies."""
        if research_id not in self.research_memory:
            return

        session = self.research_memory[research_id]

        # Check if this is a good learning opportunity (very high or low success)
        success_level = session["results"]["information_sufficiency"]
        query_type = session["query_analysis"]["query_type"]

        # Only learn from notable sessions
        if not (success_level >= 0.9 or success_level <= 0.3):
            return

        # Create learning prompt
        prompt = f"""
        Analyze this research session and identify strategy improvements.

        QUERY:
        {session["query"]}

        QUERY ANALYSIS:
        {json.dumps(session["query_analysis"], indent=2)}

        RESEARCH PLAN:
        {json.dumps(session["plan"], indent=2)}

        RESULTS SUMMARY:
        - Success level: {success_level}
        - Iterations: {session["results"]["iterations"]}
        - Information gaps: {', '.join(session["results"].get("information_gaps", []))}

        Identify specific strategy improvements for {query_type} queries based on this session.
        Consider what worked well or poorly in:
        1. Initial parameter selection
        2. Query analysis accuracy
        3. Iteration and refinement approach
        4. Success criteria appropriateness

        Return specific strategy updates that should be applied to the template for {query_type} queries.
        Format your response as a JSON object with the changes to make to the strategy template.
        """

        learning_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            strategy_updates = json.loads(learning_result)

            # Apply updates to strategy template
            if query_type in self.strategy_templates:
                template = self.strategy_templates[query_type]

                # Apply updates (deep merge)
                updated_template = self._deep_merge(template, strategy_updates)
                self.strategy_templates[query_type] = updated_template

                # Record improvement
                self.improvement_history.append({
                    "timestamp": time.time(),
                    "query_type": query_type,
                    "based_on_session": research_id,
                    "success_level": success_level,
                    "updates": strategy_updates
                })

        except:
            # Don't update if parsing fails
            pass

    def _deep_merge(self, dict1, dict2):
        """Deep merge two dictionaries."""
        result = deepcopy(dict1)

        for key, value in dict2.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value

        return result

    def _format_results_for_analysis(self, results):
        """Format search results for LLM analysis."""
        formatted = ""
        for i, result in enumerate(results):
            metadata = result.metadata
            source_info = f"{metadata.get('document_name', 'Unknown')}"

            # Add location information if available
            location_parts = []
            for field in ["chapter", "section", "page"]:
                if field in metadata and metadata[field]:
                    location_parts.append(f"{field.capitalize()}: {metadata[field]}")

            if location_parts:
                source_info += f" ({', '.join(location_parts)})"

            # Include document type if available
            if "document_type" in metadata:
                source_info += f" | Type: {metadata['document_type']}"

            # Format content
            content = result.page_content
            if len(content) > 500:
                content = content[:500] + "..."

            # Add to formatted output
            formatted += f"[Result {i}] {source_info}\n{content}\n\n"

        return formatted
5. Dynamic Metadata Extraction and Management
pythonCopyclass DynamicMetadataExtractor:
    """
    Extracts rich metadata from documents and chunks using LLMs, with
    domain-adaptive capabilities and contextual understanding.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()
        self.domain_schemas = {}
        self.extraction_patterns = {}
        self.metadata_stats = {}

    async def initialize(self):
        """Initialize the extractor with base schemas and extraction patterns."""
        # Create base domain schemas
        await self._create_base_domain_schemas()

        # Generate extraction patterns
        await self._generate_extraction_patterns()

    async def _create_base_domain_schemas(self):
        """Create base metadata schemas for different document domains."""
        domains = [
            "academic", "legal", "technical", "scientific", "historical",
            "literary", "educational", "business", "medical", "religious"
        ]

        for domain in domains:
            schema = await self._generate_domain_schema(domain)
            self.domain_schemas[domain] = schema

    async def _generate_domain_schema(self, domain):
        """Generate metadata schema for a specific document domain."""
        prompt = f"""
        Create a comprehensive metadata schema for {domain} documents.

        Design a schema that captures all important metadata fields for documents in the {domain} domain.
        Include:

        1. Essential identification fields (titles, identifiers, etc.)
        2. Domain-specific classification fields
        3. Structural metadata fields
        4. Content-type metadata
        5. Relationship metadata
        6. Temporal metadata
        7. Authority/source metadata

        For each field, specify:
        - Field name
        - Field description
        - Field type (string, number, date, list, etc.)
        - Whether the field is required or optional
        - Examples of valid values

        Return the schema as a structured JSON object.
        """

        schema_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(schema_result)
        except:
            # Fallback basic schema
            return {
                "fields": {
                    "title": {"type": "string", "required": True},
                    "author": {"type": "string", "required": False},
                    "date": {"type": "date", "required": False},
                    "domain_specific_type": {"type": "string", "required": False},
                    "content_type": {"type": "string", "required": False}
                }
            }

    async def _generate_extraction_patterns(self):
        """Generate extraction patterns for different content types."""
        content_types = [
            "book_chapter", "article", "legal_document", "research_paper",
            "technical_manual", "correspondence", "reference_material"
        ]

        for content_type in content_types:
            pattern = await self._generate_content_type_pattern(content_type)
            self.extraction_patterns[content_type] = pattern

    async def _generate_content_type_pattern(self, content_type):
        """Generate extraction pattern for a specific content type."""
        prompt = f"""
        Create an extraction pattern for identifying and extracting metadata from {content_type} content.

        Design a pattern that specifies:

        1. Textual indicators for identifying this content type
        2. Common metadata locations within the text
        3. Regular expression patterns for extracting common fields
        4. Contextual clues for inferring metadata
        5. Common formatting patterns

        The pattern will be used to dynamically extract metadata from documents
        that match this content type. Be specific and comprehensive.

        Return the pattern as a structured JSON object.
        """

        pattern_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(pattern_result)
        except:
            # Fallback basic pattern
            return {
                "indicators": [content_type.replace("_", " ")],
                "metadata_locations": ["beginning", "end"],
                "regex_patterns": {},
                "contextual_clues": {}
            }

    async def extract_document_metadata(self, document_path, content):
        """Extract comprehensive metadata from a document."""
        # Take representative sample for analysis
        sample = self._create_representative_sample(content)

        # Determine document domain and content type
        classification = await self._classify_document(document_path, sample)

        # Get appropriate schema and pattern
        domain = classification.get("domain", "general")
        content_type = classification.get("content_type", "general")

        schema = self.domain_schemas.get(domain, self.domain_schemas.get("academic", {}))
        pattern = self.extraction_patterns.get(content_type, {})

        # Create extraction prompt
        prompt = self._create_metadata_extraction_prompt(
            document_path=document_path,
            sample=sample,
            classification=classification,
            schema=schema,
            pattern=pattern
        )

        # Get LLM extraction
        extraction_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.1,
            response_format={"type": "json_object"}
        )

        try:
            metadata = json.loads(extraction_result)

            # Record metadata stats
            self._update_metadata_stats(domain, content_type, metadata)

            return {
                "metadata": metadata,
                "domain": domain,
                "content_type": content_type,
                "extraction_confidence": metadata.get("extraction_confidence", 0.7)
            }

        except:
            # Fallback basic metadata
            filename = os.path.basename(document_path)
            return {
                "metadata": {
                    "title": filename,
                    "extraction_confidence": 0.3
                },
                "domain": "unknown",
                "content_type": "unknown",
                "extraction_confidence": 0.3
            }

    async def extract_chunk_metadata(self, chunk_text, document_metadata, document_domain):
        """Extract detailed metadata specific to a chunk."""
        # Get domain schema
        schema = self.domain_schemas.get(document_domain, self.domain_schemas.get("academic", {}))

        # Create extraction prompt
        prompt = f"""
        Extract detailed metadata for this text chunk from a {document_domain} document.

        TEXT CHUNK:
        {chunk_text[:1000]}...

        DOCUMENT METADATA:
        {json.dumps(document_metadata, indent=2)}

        Extract the following metadata specific to this chunk:

        1. Main topics or subjects covered
        2. Named entities mentioned (people, places, organizations, etc.)
        3. Time references (dates, periods, events)
        4. Location references
        5. Domain-specific concepts
        6. Structural position (introduction, conclusion, etc.)
        7. Any identifiable section/chapter/part information
        8. References to other works or documents
        9. Numerical data or measurements
        10. Key terminology with domain significance

        Return the extracted metadata as a JSON object. Only include fields where you found information.
        Include a confidence score (0.0-1.0) for each extraction.
        """

        extraction_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.1,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(extraction_result)
        except:
            # Fallback basic metadata
            return {
                "main_topics": [],
                "extraction_confidence": 0.3
            }

    async def _classify_document(self, document_path, sample):
        """Classify document domain and content type."""
        filename = os.path.basename(document_path)

        prompt = f"""
        Classify this document's domain and content type based on the sample.

        FILENAME: {filename}

        DOCUMENT SAMPLE:
        {sample}

        Determine:
        1. The primary domain (academic, legal, technical, scientific, historical, literary, educational, business, medical, religious)
        2. The content type (book_chapter, article, legal_document, research_paper, technical_manual, correspondence, reference_material)
        3. The level of confidence in your classification (0.0-1.0)

        Return your classification as a JSON object with domain, content_type, and confidence fields.
        """

        classification_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.1,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(classification_result)
        except:
            # Fallback classification
            return {
                "domain": "general",
                "content_type": "general",
                "confidence": 0.5
            }

    def _create_representative_sample(self, content):
        """Create a representative sample from the content."""
        # Extract beginning, middle, and end
        content_length = len(content)

        begin_size = min(1000, content_length // 3)
        middle_start = max(0, (content_length // 2) - (begin_size // 2))
        end_start = max(0, content_length - begin_size)

        beginning = content[:begin_size]
        middle = content[middle_start:middle_start + begin_size]
        end = content[end_start:]

        return f"--- BEGINNING ---\n{beginning}\n\n--- MIDDLE ---\n{middle}\n\n--- END ---\n{end}"

    def _create_metadata_extraction_prompt(self, document_path, sample, classification, schema, pattern):
        """Create a detailed prompt for metadata extraction."""
        filename = os.path.basename(document_path)
        domain = classification.get("domain", "general")
        content_type = classification.get("content_type", "general")

        # Format schema fields
        schema_fields = schema.get("fields", {})
        fields_str = ""
        for field_name, field_info in schema_fields.items():
            required = "Required" if field_info.get("required", False) else "Optional"
            field_type = field_info.get("type", "string")
            fields_str += f"- {field_name}: {field_type} ({required})\n"

        # Format extraction patterns
        patterns_str = ""
        if "regex_patterns" in pattern:
            patterns_str = "EXTRACTION PATTERNS:\n"
            for field, regex in pattern.get("regex_patterns", {}).items():
                patterns_str += f"- {field}: {regex}\n"

        # Build prompt
        prompt = f"""
        Extract comprehensive metadata from this {domain} {content_type} document.

        FILENAME: {filename}

        DOCUMENT SAMPLE:
        {sample}

        METADATA SCHEMA FIELDS:
        {fields_str}

        {patterns_str}

        Extract as many metadata fields as possible from the document sample.
        Use both explicit statements and contextual clues to infer metadata values.
        For each field, include a confidence score (0.0-1.0) indicating your certainty.

        Return the extracted metadata as a JSON object following the schema structure.
        Include an overall extraction_confidence field (0.0-1.0) with your assessment.
        """

        return prompt

    def _update_metadata_stats(self, domain, content_type, metadata):
        """Update metadata statistics for analysis."""
        # Update domain stats
        if domain not in self.metadata_stats:
            self.metadata_stats[domain] = {
                "count": 0,
                "fields": {},
                "content_types": {},
                "avg_confidence": 0
            }

        domain_stats = self.metadata_stats[domain]
        domain_stats["count"] += 1

        # Update content type stats
        if content_type not in domain_stats["content_types"]:
            domain_stats["content_types"][content_type] = 0
        domain_stats["content_types"][content_type] += 1

        # Update field stats
        for field, value in metadata.items():
            if field not in domain_stats["fields"]:
                domain_stats["fields"][field] = 0
            domain_stats["fields"][field] += 1

        # Update confidence
        confidence = metadata.get("extraction_confidence", 0.7)
        old_avg = domain_stats.get("avg_confidence", 0)
        old_count = domain_stats["count"] - 1

        if old_count > 0:
            domain_stats["avg_confidence"] = (old_avg * old_count + confidence) / domain_stats["count"]
        else:
            domain_stats["avg_confidence"] = confidence
6. Advanced Context Management
pythonCopyclass ContextEngine:
    """
    Advanced context management system with dynamic allocation, optimal organization,
    and adaptive retrieval of knowledge and conversation context.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()
        self.conversation_memory = ConversationMemoryManager(self.llm_service)
        self.knowledge_context = KnowledgeContextManager(self.llm_service)
        self.context_stats = {}
        self.context_configurations = {}

    async def initialize_session(self, session_id):
        """Initialize a new context session with optimal configuration."""
        # Generate optimal configuration for new session
        configuration = await self._generate_context_configuration(session_id)
        self.context_configurations[session_id] = configuration

        # Initialize context stats
        self.context_stats[session_id] = {
            "created_at": time.time(),
            "last_accessed": time.time(),
            "query_count": 0,
            "knowledge_usage": [],
            "knowledge_relevance": []
        }

        return configuration

    async def _generate_context_configuration(self, session_id):
        """Generate optimal context configuration for a new session."""
        prompt = f"""
        Create an optimal context management configuration for a knowledge corpus research session.

        Design a configuration that specifies:

        1. Context window allocation between different components:
           - Knowledge context (facts, concepts from the corpus)
           - Conversation history (user-assistant exchanges)
           - System instructions (guidance for response generation)

        2. Prioritization strategies for different content types:
           - How to prioritize facts vs concepts vs examples
           - How to prioritize recent vs foundational information
           - How to prioritize specific vs general information

        3. Dynamic adjustment triggers:
           - When to increase knowledge context allocation
           - When to increase conversation history allocation
           - When to compress or summarize context

        The configuration should be optimized for maintaining coherent conversation
        while providing accurate information from the knowledge corpus.

        Return the configuration as a structured JSON object.
        """

        config_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(config_result)
        except:
            # Fallback basic configuration
            return {
                "allocation": {
                    "knowledge_context": 0.7,
                    "conversation_history": 0.2,
                    "system_instructions": 0.1
                },
                "prioritization": {
                    "knowledge_recency_weight": 0.6,
                    "knowledge_relevance_weight": 0.8,
                    "conversation_recency_weight": 0.7
                },
                "dynamic_adjustment": {
                    "knowledge_increase_threshold": 0.8,
                    "conversation_increase_threshold": 0.8,
                    "compression_threshold": 0.9
                }
            }

    async def update_context(self, session_id, new_chunks, query, message):
        """Update context with new knowledge chunks and conversation message."""
        # Check if session exists, create if not
        if session_id not in self.context_configurations:
            await self.initialize_session(session_id)

        # Update context stats
        if session_id in self.context_stats:
            self.context_stats[session_id]["last_accessed"] = time.time()
            self.context_stats[session_id]["query_count"] += 1

        # Update knowledge context
        if new_chunks:
            self.knowledge_context.update_knowledge(session_id, new_chunks, query)

        # Update conversation memory
        if message:
            self.conversation_memory.add_message(session_id, message)

        # Check if context adjustment is needed
        await self._check_context_adjustment(session_id, query)

    async def get_context(self, session_id, query, max_tokens=None):
        """
        Get optimally formatted context for the current query.

        Args:
            session_id: Session identifier
            query: Current query
            max_tokens: Maximum tokens for context

        Returns:
            Formatted context for prompt construction
        """
        # Check if session exists
        if session_id not in self.context_configurations:
            await self.initialize_session(session_id)

        # Get configuration
        config = self.context_configurations[session_id]

        # Calculate token allocations
        if not max_tokens:
            max_tokens = 32000  # Default to large model context size

        knowledge_tokens = int(max_tokens * config["allocation"]["knowledge_context"])
        conversation_tokens = int(max_tokens * config["allocation"]["conversation_history"])

        # Get relevant knowledge
        relevant_knowledge = await self.knowledge_context.get_relevant_knowledge(
            session_id, query, knowledge_tokens
        )

        # Get relevant conversation history
        relevant_history = await self.conversation_memory.get_relevant_history(
            session_id, query, conversation_tokens
        )

        # Format knowledge context
        knowledge_text = self._format_knowledge_context(
            relevant_knowledge,
            include_citations=True,
            prioritization=config["prioritization"]
        )

        # Format conversation history
        conversation_text = self._format_conversation_history(
            relevant_history,
            recency_weight=config["prioritization"]["conversation_recency_weight"]
        )

        # Log context usage stats
        if session_id in self.context_stats:
            self.context_stats[session_id]["knowledge_usage"].append(len(relevant_knowledge))

            # Calculate average relevance (if available)
            relevance_sum = 0
            relevance_count = 0
            for chunk in relevant_knowledge:
                if "relevance_score" in chunk.metadata:
                    relevance_sum += chunk.metadata["relevance_score"]
                    relevance_count += 1

            if relevance_count > 0:
                self.context_stats[session_id]["knowledge_relevance"].append(relevance_sum / relevance_count)

        return {
            "knowledge_context": knowledge_text,
            "conversation_history": conversation_text,
            "knowledge_chunks": relevant_knowledge,
            "conversation_messages": relevant_history,
            "tokens": {
                "knowledge": knowledge_tokens,
                "conversation": conversation_tokens
            }
        }

    async def _check_context_adjustment(self, session_id, query):
        """Check if context configuration needs adjustment."""
        if session_id not in self.context_configurations:
            return

        config = self.context_configurations[session_id]
        stats = self.context_stats.get(session_id, {})

        # Only adjust after sufficient queries
        query_count = stats.get("query_count", 0)
        if query_count < 5:
            return

        # Check knowledge relevance
        relevance_scores = stats.get("knowledge_relevance", [])
        if relevance_scores and len(relevance_scores) >= 3:
            avg_relevance = sum(relevance_scores[-3:]) / 3

            if avg_relevance < 0.5 and config["allocation"]["knowledge_context"] > 0.5:
                # Knowledge not relevant enough, maybe reduce allocation
                await self._adjust_context_allocation(session_id, "knowledge_context", -0.05)
            elif avg_relevance > 0.8 and avg_relevance > config["prioritization"]["knowledge_relevance_weight"]:
                # Knowledge very relevant, maybe increase weight
                config["prioritization"]["knowledge_relevance_weight"] = min(0.95, avg_relevance + 0.05)

        # Check compression need
        if query_count % 10 == 0:  # Check every 10 queries
            knowledge_usage = stats.get("knowledge_usage", [])
            if knowledge_usage and sum(knowledge_usage[-3:]) / 3 > 15:
                # Using lots of knowledge chunks, might need compression
                threshold = config["dynamic_adjustment"]["compressionRetryClaude hit the max length for a message and has paused its response. You can write Continue to keep the chat going.EScontinueEdit6. Advanced Context Management (continued)
pythonCopy        # Check compression need
        if query_count % 10 == 0:  # Check every 10 queries
            knowledge_usage = stats.get("knowledge_usage", [])
            if knowledge_usage and sum(knowledge_usage[-3:]) / 3 > 15:
                # Using lots of knowledge chunks, might need compression
                threshold = config["dynamic_adjustment"]["compression_threshold"]

                # Check if compression needed
                if query_count / 10 > threshold:
                    await self._compress_context(session_id)

    async def _adjust_context_allocation(self, session_id, component, change):
        """Adjust context allocation for a component."""
        if session_id not in self.context_configurations:
            return

        config = self.context_configurations[session_id]
        allocation = config["allocation"]

        # Calculate new allocation
        new_allocation = allocation.copy()
        new_allocation[component] = max(0.1, min(0.9, allocation[component] + change))

        # Adjust other components proportionally
        total_other = sum(v for k, v in allocation.items() if k != component)
        if total_other > 0:
            for k in allocation:
                if k != component:
                    new_allocation[k] = allocation[k] * (1 - new_allocation[component]) / total_other

        # Ensure sum is 1
        sum_allocation = sum(new_allocation.values())
        if abs(sum_allocation - 1.0) > 0.001:
            # Normalize
            for k in new_allocation:
                new_allocation[k] /= sum_allocation

        # Update configuration
        config["allocation"] = new_allocation

    async def _compress_context(self, session_id):
        """Compress context to optimize token usage."""
        # This could involve:
        # 1. Summarizing conversation history
        # 2. Consolidating similar knowledge chunks
        # 3. Removing redundant information

        # For now, just trigger conversation memory compression
        if session_id in self.context_stats:
            self.context_stats[session_id]["compression_count"] = self.context_stats[session_id].get("compression_count", 0) + 1

    def _format_knowledge_context(self, chunks, include_citations=True, prioritization=None):
        """Format knowledge chunks for inclusion in context."""
        if not chunks:
            return "No relevant knowledge available."

        # Sort chunks by relevance if available
        if prioritization:
            relevance_weight = prioritization.get("knowledge_relevance_weight", 0.8)
            recency_weight = prioritization.get("knowledge_recency_weight", 0.6)

            # Create a scoring function
            def chunk_score(chunk):
                relevance = chunk.metadata.get("relevance_score", 0.5)
                recency = chunk.metadata.get("recency_score", 0.5)
                return (relevance * relevance_weight) + (recency * recency_weight)

            # Sort chunks by score
            chunks = sorted(chunks, key=chunk_score, reverse=True)

        formatted = ""
        for i, chunk in enumerate(chunks):
            metadata = chunk.metadata
            source_info = f"{metadata.get('document_name', 'Unknown')}"

            # Add location information if available
            location_parts = []
            for field in ["chapter", "section", "page"]:
                if field in metadata and metadata[field]:
                    location_parts.append(f"{field.capitalize()}: {metadata[field]}")

            if location_parts:
                source_info += f" ({', '.join(location_parts)})"

            # Add citation if requested
            if include_citations:
                formatted += f"[{i+1}] {source_info}\n{chunk.page_content}\n\n"
            else:
                formatted += f"{source_info}\n{chunk.page_content}\n\n"

        return formatted

    def _format_conversation_history(self, messages, recency_weight=0.7):
        """Format conversation history for inclusion in context."""
        if not messages:
            return "No previous conversation."

        formatted = ""
        for msg in messages:
            role = "User" if msg["role"] == "user" else "Assistant"
            formatted += f"{role}: {msg['content']}\n\n"

        return formatted
7. Response Generation System with Self-Evaluation
pythonCopyclass EnhancedResponseGenerator:
    """
    Advanced response generation system with self-evaluation, contextual awareness,
    and response quality assessment.
    """

    def __init__(self, llm_service=None):
        self.llm_service = llm_service or LLMService()
        self.response_templates = {}
        self.quality_metrics = {}
        self.feedback_history = {}

    async def initialize(self):
        """Initialize generator with response templates."""
        await self._generate_response_templates()

    async def _generate_response_templates(self):
        """Generate templates for different response types."""
        response_types = [
            "factual", "conceptual", "comparative", "procedural",
            "exploratory", "analytical", "clarification", "summary"
        ]

        for response_type in response_types:
            template = await self._generate_template(response_type)
            self.response_templates[response_type] = template

    async def _generate_template(self, response_type):
        """Generate a template for a specific response type."""
        prompt = f"""
        Create a comprehensive template for generating {response_type} responses.

        Design a template that specifies:

        1. Overall structure for a {response_type} response
        2. Key components that should be included
        3. Tone and style guidelines
        4. Appropriate level of technical detail
        5. Citation and reference handling
        6. Adaptation guidelines for different complexity levels

        The template should optimize for clarity, accuracy, and helpfulness
        while being adaptable to different domains and user needs.

        Return the template as a structured JSON object.
        """

        template_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(template_result)
        except:
            # Fallback basic template
            return {
                "response_type": response_type,
                "structure": ["introduction", "main_content", "conclusion"],
                "components": {
                    "introduction": "Brief context setting",
                    "main_content": "Primary information addressing the query",
                    "conclusion": "Summary or next steps"
                },
                "tone": "informative and helpful",
                "technical_detail": "adapt to query complexity",
                "citation_style": "numbered references"
            }

    async def generate_response(self, query, context, response_params):
        """
        Generate a response based on context and parameters.

        Args:
            query: User's question
            context: Context information (knowledge and conversation)
            response_params: Parameters for response generation

        Returns:
            Generated response with metadata
        """
        # Determine response type
        response_type = response_params.get("response_type", "factual")

        # Get appropriate template
        template = self.response_templates.get(
            response_type,
            self.response_templates.get("factual", {})
        )

        # Create generation prompt
        prompt = self._create_generation_prompt(query, context, template, response_params)

        # Generate response
        response_text = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=response_params.get("temperature", 0.7),
            max_tokens=response_params.get("max_tokens", None)
        )

        # Evaluate response if requested
        evaluation = None
        if response_params.get("evaluate_response", False):
            evaluation = await self._evaluate_response(
                query=query,
                response=response_text,
                context=context,
                params=response_params
            )

        # Return response with metadata
        return {
            "response": response_text,
            "response_type": response_type,
            "template_used": template,
            "evaluation": evaluation
        }

    def _create_generation_prompt(self, query, context, template, params):
        """Create a detailed prompt for response generation."""
        # Extract context components
        knowledge_context = context.get("knowledge_context", "No knowledge context available.")
        conversation_history = context.get("conversation_history", "No conversation history.")

        # Extract template components
        structure = template.get("structure", [])
        components = template.get("components", {})
        tone = template.get("tone", "informative")
        technical_detail = template.get("technical_detail", "adapt to query complexity")

        # Format template guidelines
        template_guidelines = f"RESPONSE GUIDELINES:\n"
        template_guidelines += f"- Type: {template.get('response_type', 'factual')} response\n"
        template_guidelines += f"- Tone: {tone}\n"
        template_guidelines += f"- Technical detail: {technical_detail}\n"

        if structure:
            template_guidelines += f"- Structure: {', '.join(structure)}\n"

        # Format parameters as instructions
        param_guidelines = "PARAMETER GUIDELINES:\n"
        for param, value in params.items():
            if param in ["detail_level", "include_citations", "include_examples"]:
                param_guidelines += f"- {param.replace('_', ' ').title()}: {value}\n"

        # Build full prompt
        prompt = f"""You are a knowledgeable assistant with expertise in researching and explaining information from a knowledge corpus.

{template_guidelines}

{param_guidelines}

KNOWLEDGE CONTEXT:
{knowledge_context}

CONVERSATION HISTORY:
{conversation_history}

USER QUERY:
{query}

Based on the provided knowledge context and conversation history, generate a {template.get('response_type', 'factual')} response to the user's query.
Follow the response guidelines and structure while drawing primarily from the knowledge context.
If the knowledge context doesn't contain sufficient information to answer the query, acknowledge this limitation.
"""

        return prompt

    async def _evaluate_response(self, query, response, context, params):
        """Evaluate response quality across multiple dimensions."""
        # Create evaluation prompt
        prompt = f"""
        Evaluate the quality of this response to the user's query.

        USER QUERY:
        {query}

        RESPONSE:
        {response}

        KNOWLEDGE CONTEXT (EXCERPTS):
        {self._get_context_summary(context)}

        Evaluate the response across these dimensions:
        1. Factual Accuracy (1-10): How well the response accurately reflects the knowledge context
        2. Completeness (1-10): How thoroughly the response addresses all aspects of the query
        3. Clarity (1-10): How clear and easy to understand the response is
        4. Relevance (1-10): How well the response focuses on what was asked
        5. Citation Quality (1-10): How well the response attributes information to sources

        For each dimension, provide:
        - A numeric score (1-10)
        - A brief explanation of the score
        - A specific suggestion for improvement (if score < 8)

        Return your evaluation as a JSON object.
        """

        evaluation_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.2,
            response_format={"type": "json_object"}
        )

        try:
            evaluation = json.loads(evaluation_result)

            # Calculate overall score
            scores = [
                evaluation.get("factual_accuracy", {}).get("score", 0),
                evaluation.get("completeness", {}).get("score", 0),
                evaluation.get("clarity", {}).get("score", 0),
                evaluation.get("relevance", {}).get("score", 0),
                evaluation.get("citation_quality", {}).get("score", 0)
            ]

            # Add overall score
            evaluation["overall_score"] = sum(scores) / len(scores)

            # Update quality metrics
            response_type = params.get("response_type", "factual")
            if response_type not in self.quality_metrics:
                self.quality_metrics[response_type] = {
                    "count": 0,
                    "average_scores": {
                        "factual_accuracy": 0,
                        "completeness": 0,
                        "clarity": 0,
                        "relevance": 0,
                        "citation_quality": 0,
                        "overall": 0
                    }
                }

            # Update metrics
            metrics = self.quality_metrics[response_type]
            metrics["count"] += 1

            # Update running averages
            for i, dimension in enumerate(["factual_accuracy", "completeness", "clarity", "relevance", "citation_quality"]):
                old_avg = metrics["average_scores"][dimension]
                old_count = metrics["count"] - 1

                if old_count > 0:
                    metrics["average_scores"][dimension] = (old_avg * old_count + scores[i]) / metrics["count"]
                else:
                    metrics["average_scores"][dimension] = scores[i]

            # Update overall average
            old_avg = metrics["average_scores"]["overall"]
            old_count = metrics["count"] - 1

            if old_count > 0:
                metrics["average_scores"]["overall"] = (old_avg * old_count + evaluation["overall_score"]) / metrics["count"]
            else:
                metrics["average_scores"]["overall"] = evaluation["overall_score"]

            return evaluation

        except:
            # Return basic evaluation on parsing failure
            return {
                "overall_score": 7,
                "evaluation_error": "Failed to parse evaluation result"
            }

    def _get_context_summary(self, context):
        """Get a summary of the knowledge context for evaluation."""
        if "knowledge_chunks" not in context or not context["knowledge_chunks"]:
            return "No knowledge context available."

        # Take first few chunks as sample
        sample_chunks = context["knowledge_chunks"][:3]
        summary = ""

        for i, chunk in enumerate(sample_chunks):
            summary += f"[{i+1}] {chunk.page_content[:200]}...\n\n"

        return summary

    async def process_feedback(self, response_id, feedback, query, response):
        """Process user feedback to improve response generation."""
        # Store feedback in history
        if response_id not in self.feedback_history:
            self.feedback_history[response_id] = []

        self.feedback_history[response_id].append({
            "timestamp": time.time(),
            "feedback": feedback,
            "query": query,
            "response": response
        })

        # Analyze feedback to extract improvement insights
        if len(self.feedback_history) % 10 == 0:  # Every 10 feedback items
            await self._analyze_feedback_trends()

    async def _analyze_feedback_trends(self):
        """Analyze feedback trends to identify improvement opportunities."""
        # Only proceed if we have enough feedback
        if len(self.feedback_history) < 10:
            return

        # Group feedback by response type
        feedback_by_type = {}
        for response_id, feedback_list in self.feedback_history.items():
            for feedback_item in feedback_list:
                response_type = feedback_item.get("response_type", "unknown")

                if response_type not in feedback_by_type:
                    feedback_by_type[response_type] = []

                feedback_by_type[response_type].append(feedback_item)

        # Analyze each response type with sufficient feedback
        for response_type, feedback_items in feedback_by_type.items():
            if len(feedback_items) >= 5:
                await self._update_template_from_feedback(response_type, feedback_items)

    async def _update_template_from_feedback(self, response_type, feedback_items):
        """Update response template based on feedback analysis."""
        if response_type not in self.response_templates:
            return

        # Format feedback for analysis
        feedback_str = ""
        for i, item in enumerate(feedback_items[-10:]):  # Use most recent 10
            feedback_str += f"[{i+1}] Query: {item['query']}\n"
            feedback_str += f"Response: {item['response'][:200]}...\n"
            feedback_str += f"Feedback: {item['feedback']}\n\n"

        # Get current template
        current_template = self.response_templates[response_type]

        # Create analysis prompt
        prompt = f"""
        Analyze user feedback for {response_type} responses and suggest template improvements.

        CURRENT TEMPLATE:
        {json.dumps(current_template, indent=2)}

        RECENT FEEDBACK:
        {feedback_str}

        Based on this feedback, suggest specific improvements to the response template.
        Consider changes to:
        - Structure
        - Components
        - Tone and style
        - Technical detail level
        - Citation approach

        Return your suggestions as a JSON object with the same structure as the current template,
        modified based on feedback patterns.
        """

        suggestion_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            suggested_template = json.loads(suggestion_result)

            # Update template
            self.response_templates[response_type] = suggested_template

        except:
            # Don't update if parsing fails
            pass
8. Data Analysis Agent for Documents
pythonCopyclass DataAnalysisAgent:
    """
    Specialized agent for analyzing and extracting insights from document collections,
    with self-directed exploration capabilities.
    """

    def __init__(self, vector_db, llm_service=None):
        self.vector_db = vector_db
        self.llm_service = llm_service or LLMService()
        self.analysis_memory = {}
        self.insight_repository = {}
        self.exploration_strategies = {}

    async def initialize(self):
        """Initialize the agent with exploration strategies."""
        await self._generate_exploration_strategies()

    async def _generate_exploration_strategies(self):
        """Generate exploration strategies for different analysis goals."""
        analysis_goals = [
            "concept_mapping", "trend_analysis", "comparative_analysis",
            "argument_extraction", "evidence_gathering", "narrative_analysis",
            "perspective_identification", "methodology_mapping"
        ]

        for goal in analysis_goals:
            strategy = await self._generate_strategy(goal)
            self.exploration_strategies[goal] = strategy

    async def _generate_strategy(self, analysis_goal):
        """Generate an exploration strategy for a specific analysis goal."""
        prompt = f"""
        Create a comprehensive exploration strategy for {analysis_goal} in a knowledge corpus.

        Design a strategy that specifies:

        1. Key exploration patterns for {analysis_goal}
        2. Search query formulation approach
        3. Information relationship mapping
        4. Progressive discovery process
        5. Insight aggregation methodology
        6. Verification and cross-checking procedures

        The strategy should enable systematic exploration of the knowledge corpus
        to achieve {analysis_goal} with minimal query iterations.

        Return the strategy as a structured JSON object.
        """

        strategy_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(strategy_result)
        except:
            # Fallback basic strategy
            return {
                "analysis_goal": analysis_goal,
                "search_patterns": ["direct_queries", "related_concept_exploration"],
                "relationship_mapping": "concept_linking",
                "discovery_process": ["initial_exploration", "targeted_investigation", "synthesis"],
                "verification_approach": "cross_reference"
            }

    async def analyze_topic(self, topic, analysis_goals=None, initial_params=None):
        """
        Conduct a comprehensive analysis of a topic in the knowledge corpus.

        Args:
            topic: Topic to analyze
            analysis_goals: Specific analysis goals (defaults to concept_mapping if None)
            initial_params: Optional initial parameters

        Returns:
            Analysis results with insights and supporting evidence
        """
        # Generate a unique ID for this analysis
        analysis_id = f"analysis_{int(time.time())}"

        # Set default analysis goals if none provided
        if not analysis_goals:
            analysis_goals = ["concept_mapping"]

        # Create analysis plan
        analysis_plan = await self._create_analysis_plan(topic, analysis_goals, initial_params)

        # Initialize analysis session
        self.analysis_memory[analysis_id] = {
            "topic": topic,
            "analysis_goals": analysis_goals,
            "plan": analysis_plan,
            "execution_log": [],
            "discovered_concepts": set(),
            "evidence": {},
            "insights": [],
            "start_time": time.time()
        }

        # Execute analysis plan
        for step in analysis_plan["steps"]:
            step_result = await self._execute_analysis_step(analysis_id, step)
            self.analysis_memory[analysis_id]["execution_log"].append({
                "step": step,
                "result": step_result
            })

            # Break if step indicates analysis should end
            if step_result.get("should_end_analysis", False):
                break

        # Synthesize insights
        synthesis = await self._synthesize_analysis(analysis_id)
        self.analysis_memory[analysis_id]["synthesis"] = synthesis

        # Prepare final results
        results = self._prepare_analysis_results(analysis_id)

        # Store important insights in repository
        self._update_insight_repository(topic, results)

        return results

    async def _create_analysis_plan(self, topic, analysis_goals, initial_params=None):
        """Create a comprehensive analysis plan."""
        # Get strategies for selected goals
        strategies = []
        for goal in analysis_goals:
            if goal in self.exploration_strategies:
                strategies.append(self.exploration_strategies[goal])

        # Format strategies for prompt
        strategies_str = json.dumps(strategies, indent=2) if strategies else "No predefined strategies available."

        prompt = f"""
        Create a comprehensive analysis plan for exploring the topic "{topic}" in a knowledge corpus.

        ANALYSIS GOALS:
        {', '.join(analysis_goals)}

        AVAILABLE STRATEGIES:
        {strategies_str}

        Design a step-by-step analysis plan that:
        1. Begins with broad exploration of the main topic
        2. Progressively narrows and deepens investigation
        3. Maps relationships between discovered concepts
        4. Validates findings through cross-referencing
        5. Synthesizes insights across the knowledge corpus

        Each step should specify:
        - The specific search or analysis action to perform
        - Parameters for that action (search terms, filters, etc.)
        - The expected information to gather
        - Criteria for evaluating results
        - Decision points for the next steps

        Return the plan as a structured JSON object with clear, executable steps.
        """

        plan_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            plan = json.loads(plan_result)

            # Apply initial params if provided
            if initial_params:
                # Merge parameters
                if "parameters" in plan and isinstance(plan["parameters"], dict):
                    plan["parameters"].update(initial_params)
                else:
                    plan["parameters"] = initial_params

            return plan

        except:
            # Fallback basic plan
            return {
                "topic": topic,
                "goals": analysis_goals,
                "steps": [
                    {
                        "step_id": 1,
                        "action": "search",
                        "query": topic,
                        "parameters": {"top_k": 10}
                    },
                    {
                        "step_id": 2,
                        "action": "extract_concepts",
                        "from_results": True
                    },
                    {
                        "step_id": 3,
                        "action": "search_concepts",
                        "parameters": {"top_k": 5}
                    },
                    {
                        "step_id": 4,
                        "action": "synthesize_insights"
                    }
                ]
            }

    async def _execute_analysis_step(self, analysis_id, step):
        """Execute a single analysis step."""
        if analysis_id not in self.analysis_memory:
            return {"error": "Analysis session not found"}

        session = self.analysis_memory[analysis_id]

        # Execute based on action type
        action = step.get("action", "").lower()

        if action == "search":
            return await self._execute_search_action(session, step)

        elif action == "extract_concepts":
            return await self._execute_concept_extraction(session, step)

        elif action == "search_concepts":
            return await self._execute_concept_search(session, step)

        elif action == "map_relationships":
            return await self._execute_relationship_mapping(session, step)

        elif action == "validate_findings":
            return await self._execute_validation(session, step)

        elif action == "synthesize_insights":
            return await self._execute_insight_synthesis(session, step)

        else:
            return {
                "error": f"Unknown action type: {action}",
                "status": "failed"
            }

    async def _execute_search_action(self, session, step):
        """Execute a search action within analysis."""
        # Get query and parameters
        query = step.get("query", session["topic"])
        parameters = step.get("parameters", {})

        # Execute search
        try:
            results = await self.vector_db.similarity_search(
                query=query,
                top_k=parameters.get("top_k", 10),
                threshold=parameters.get("threshold", 0.7),
                filter_dict=parameters.get("filters")
            )

            # Process results
            processed_results = []
            for result in results:
                # Add to evidence repository
                chunk_id = result.metadata.get("chunk_id", str(uuid.uuid4()))
                session["evidence"][chunk_id] = {
                    "content": result.page_content,
                    "metadata": result.metadata,
                    "found_in_step": step.get("step_id", 0),
                    "query": query
                }

                # Add to processed results
                processed_results.append({
                    "chunk_id": chunk_id,
                    "content_preview": result.page_content[:200],
                    "metadata": {k: v for k, v in result.metadata.items() if k != "chunk_id"}
                })

            return {
                "status": "success",
                "query": query,
                "results_count": len(results),
                "results_preview": processed_results
            }

        except Exception as e:
            return {
                "status": "failed",
                "error": str(e)
            }

    async def _execute_concept_extraction(self, session, step):
        """Extract concepts from previously found evidence."""
        # Determine source of content for extraction
        evidence_dict = session["evidence"]

        if step.get("from_results", True) and evidence_dict:
            # Concatenate evidence for analysis
            content = ""
            for chunk_id, evidence in list(evidence_dict.items())[:10]:  # Limit to most recent 10
                content += f"{evidence['content']}\n\n"

            # Create extraction prompt
            prompt = f"""
            Analyze this content and extract key concepts related to the topic "{session['topic']}".

            CONTENT:
            {content[:4000]}  # Limit to 4000 chars

            Extract:
            1. Main concepts directly related to the topic
            2. Secondary concepts that are connected to main concepts
            3. Terminology and definitions specific to this domain
            4. Evidence relationships between concepts

            For each concept, provide:
            - The concept name
            - A brief description based on the content
            - Supporting evidence from the content
            - Relationships to other concepts (if present)

            Return your extraction as a structured JSON object.
            """

            extraction_result = await self.llm_service.generate_completion(
                prompt=prompt,
                temperature=0.2,
                response_format={"type": "json_object"}
            )

            try:
                extraction = json.loads(extraction_result)

                # Update discovered concepts
                if "concepts" in extraction:
                    for concept in extraction["concepts"]:
                        concept_name = concept.get("name", "")
                        if concept_name:
                            session["discovered_concepts"].add(concept_name)

                return {
                    "status": "success",
                    "extraction": extraction,
                    "concepts_found": len(extraction.get("concepts", []))
                }

            except:
                return {
                    "status": "partial_success",
                    "error": "Failed to parse extraction result",
                    "raw_extraction": extraction_result[:200]
                }
        else:
            return {
                "status": "failed",
                "error": "No evidence available for concept extraction"
            }

    async def _execute_concept_search(self, session, step):
        """Search for discovered concepts to gather more evidence."""
        discovered_concepts = list(session["discovered_concepts"])

        if not discovered_concepts:
            return {
                "status": "skipped",
                "reason": "No concepts discovered for searching",
                "should_end_analysis": False
            }

        # Limit to most promising concepts if many discovered
        if len(discovered_concepts) > 5:
            # Create concept prioritization prompt
            concept_list = ", ".join(discovered_concepts)

            prompt = f"""
            From these concepts related to "{session['topic']}", identify the 5 most important
            for deeper investigation.

            CONCEPTS:
            {concept_list}

            Select the 5 concepts that:
            1. Are most central to understanding the topic
            2. Represent diverse aspects of the topic
            3. Would yield the most valuable insights if investigated further

            Return a JSON array of just the 5 concept names, no additional text.
            """

            priority_result = await self.llm_service.generate_completion(
                prompt=prompt,
                temperature=0.2,
                response_format={"type": "json_object"}
            )

            try:
                priority_concepts = json.loads(priority_result)
                if isinstance(priority_concepts, list) and len(priority_concepts) <= 5:
                    concepts_to_search = priority_concepts
                else:
                    concepts_to_search = discovered_concepts[:5]
            except:
                concepts_to_search = discovered_concepts[:5]
        else:
            concepts_to_search = discovered_concepts

        # Search for each concept
        results = {}
        parameters = step.get("parameters", {})

        for concept in concepts_to_search:
            # Formulate query that includes original topic
            query = f"{concept} {session['topic']}"

            # Execute search
            try:
                concept_results = await self.vector_db.similarity_search(
                    query=query,
                    top_k=parameters.get("top_k", 5),
                    threshold=parameters.get("threshold", 0.7)
                )

                # Process and store results
                processed_results = []
                forRetryClaude hit the max length for a message and has paused its response. You can write Continue to keep the chat going.EScontinueEdit8. Data Analysis Agent for Documents (continued)
pythonCopy                # Process and store results
                processed_results = []
                for result in concept_results:
                    # Add to evidence repository
                    chunk_id = result.metadata.get("chunk_id", str(uuid.uuid4()))
                    session["evidence"][chunk_id] = {
                        "content": result.page_content,
                        "metadata": result.metadata,
                        "found_in_step": step.get("step_id", 0),
                        "query": query,
                        "concept": concept
                    }

                    # Add to processed results
                    processed_results.append({
                        "chunk_id": chunk_id,
                        "content_preview": result.page_content[:200],
                        "metadata": {k: v for k, v in result.metadata.items() if k != "chunk_id"}
                    })

                results[concept] = {
                    "status": "success",
                    "results_count": len(concept_results),
                    "results_preview": processed_results
                }

            except Exception as e:
                results[concept] = {
                    "status": "failed",
                    "error": str(e)
                }

        return {
            "status": "success",
            "concepts_searched": len(results),
            "concept_results": results,
            "should_end_analysis": False
        }

    async def _execute_relationship_mapping(self, session, step):
        """Map relationships between concepts based on evidence."""
        discovered_concepts = list(session["discovered_concepts"])
        evidence_dict = session["evidence"]

        if not discovered_concepts or len(discovered_concepts) < 2:
            return {
                "status": "skipped",
                "reason": "Not enough concepts for relationship mapping",
                "should_end_analysis": False
            }

        if not evidence_dict:
            return {
                "status": "skipped",
                "reason": "No evidence available for relationship mapping",
                "should_end_analysis": False
            }

        # Collect evidence for concepts
        concept_evidence = {}
        for chunk_id, evidence in evidence_dict.items():
            # Check which concepts appear in this evidence
            content = evidence["content"].lower()
            for concept in discovered_concepts:
                if concept.lower() in content:
                    if concept not in concept_evidence:
                        concept_evidence[concept] = []
                    concept_evidence[concept].append({
                        "chunk_id": chunk_id,
                        "content": evidence["content"],
                        "metadata": evidence["metadata"]
                    })

        # Filter concepts with enough evidence
        concepts_with_evidence = [c for c in concept_evidence if len(concept_evidence[c]) > 0]

        if len(concepts_with_evidence) < 2:
            return {
                "status": "skipped",
                "reason": "Not enough concepts with evidence for relationship mapping",
                "should_end_analysis": False
            }

        # Create relationship mapping prompt
        concepts_str = ", ".join(concepts_with_evidence)

        # Format evidence for each concept (limited to keep prompt size reasonable)
        evidence_str = ""
        for concept in concepts_with_evidence[:5]:  # Limit to 5 concepts
            evidence_str += f"EVIDENCE FOR '{concept}':\n"
            for i, evid in enumerate(concept_evidence[concept][:2]):  # Limit to 2 pieces of evidence per concept
                evidence_str += f"[{i+1}] {evid['content'][:300]}...\n\n"

        prompt = f"""
        Map relationships between these concepts related to "{session['topic']}" based on the evidence.

        CONCEPTS:
        {concepts_str}

        EVIDENCE:
        {evidence_str}

        Identify relationships between concepts, such as:
        - Hierarchical relationships (is a type of, is part of)
        - Causal relationships (causes, influences, prevents)
        - Temporal relationships (precedes, follows, during)
        - Functional relationships (serves as, enables, supports)
        - Contrastive relationships (opposes, contradicts, differs from)

        For each relationship, provide:
        - The two concepts being related
        - The nature of the relationship
        - Supporting evidence from the text
        - Confidence level (high, medium, low)

        Return the relationship map as a structured JSON object.
        """

        mapping_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            relationships = json.loads(mapping_result)

            return {
                "status": "success",
                "relationships": relationships,
                "concepts_mapped": len(concepts_with_evidence),
                "should_end_analysis": False
            }

        except:
            return {
                "status": "partial_success",
                "error": "Failed to parse relationship mapping result",
                "raw_mapping": mapping_result[:200],
                "should_end_analysis": False
            }

    async def _execute_validation(self, session, step):
        """Validate findings through cross-referencing and verification."""
        # Check if we have insights to validate
        if not session.get("insights"):
            return {
                "status": "skipped",
                "reason": "No insights available for validation",
                "should_end_analysis": False
            }

        insights = session["insights"]
        evidence_dict = session["evidence"]

        # Choose insights to validate (prioritize those with lower confidence)
        insights_to_validate = sorted(insights, key=lambda x: x.get("confidence", 0.5))[:5]

        validation_results = []
        for insight in insights_to_validate:
            # Create validation prompt
            prompt = f"""
            Validate the following insight about "{session['topic']}" against the evidence.

            INSIGHT:
            {insight.get('statement', 'No statement provided')}

            VALIDATION CRITERIA:
            - Factual accuracy: Does the evidence support the factual claims?
            - Completeness: Does the insight represent the full picture from the evidence?
            - Nuance: Does the insight capture important distinctions or qualifications?
            - Balance: Does the insight consider multiple perspectives if present in evidence?

            EVIDENCE:
            """

            # Add supporting evidence
            supporting_evidence_ids = insight.get("supporting_evidence", [])
            for i, evidence_id in enumerate(supporting_evidence_ids):
                if evidence_id in evidence_dict:
                    evidence = evidence_dict[evidence_id]
                    prompt += f"[{i+1}] {evidence['content'][:300]}...\n\n"

            # Add random other evidence for cross-validation
            other_evidence_ids = [e_id for e_id in evidence_dict if e_id not in supporting_evidence_ids]
            random.shuffle(other_evidence_ids)
            for i, evidence_id in enumerate(other_evidence_ids[:3]):  # Add up to 3 random pieces
                evidence = evidence_dict[evidence_id]
                prompt += f"[Random {i+1}] {evidence['content'][:300]}...\n\n"

            prompt += """
            Provide a detailed validation assessment including:
            - Validation score (0-10) for each criterion
            - Overall validation score (0-10)
            - Specific evidence that supports or contradicts the insight
            - Suggested revisions to improve accuracy or completeness

            Return your assessment as a structured JSON object.
            """

            validation_result = await self.llm_service.generate_completion(
                prompt=prompt,
                temperature=0.2,
                response_format={"type": "json_object"}
            )

            try:
                validation = json.loads(validation_result)

                # Add to validation results
                validation_results.append({
                    "insight": insight.get("statement", "No statement provided"),
                    "validation": validation,
                    "insight_id": insight.get("id", "unknown")
                })

                # Update insight confidence if overall score is provided
                if "overall_score" in validation:
                    for i, orig_insight in enumerate(session["insights"]):
                        if orig_insight.get("id") == insight.get("id"):
                            session["insights"][i]["validation_score"] = validation["overall_score"]
                            session["insights"][i]["confidence"] = max(0.1, min(1.0, validation["overall_score"] / 10))

            except:
                validation_results.append({
                    "insight": insight.get("statement", "No statement provided"),
                    "error": "Failed to parse validation result",
                    "insight_id": insight.get("id", "unknown")
                })

        return {
            "status": "success",
            "validation_results": validation_results,
            "insights_validated": len(validation_results),
            "should_end_analysis": False
        }

    async def _execute_insight_synthesis(self, session, step):
        """Synthesize insights from gathered evidence and concept relationships."""
        evidence_dict = session["evidence"]
        discovered_concepts = list(session["discovered_concepts"])

        if not evidence_dict:
            return {
                "status": "failed",
                "reason": "No evidence available for insight synthesis",
                "should_end_analysis": True
            }

        # Get previous relationship mapping if available
        relationship_mapping = None
        for log_entry in session["execution_log"]:
            if log_entry["step"].get("action") == "map_relationships" and log_entry["result"].get("status") == "success":
                relationship_mapping = log_entry["result"].get("relationships")
                break

        # Format evidence for synthesis (sample from different queries/concepts)
        evidence_by_concept = {}
        evidence_by_query = {}

        for chunk_id, evidence in evidence_dict.items():
            # Organize by concept
            concept = evidence.get("concept")
            if concept:
                if concept not in evidence_by_concept:
                    evidence_by_concept[concept] = []
                evidence_by_concept[concept].append(evidence)

            # Organize by query
            query = evidence.get("query")
            if query:
                if query not in evidence_by_query:
                    evidence_by_query[query] = []
                evidence_by_query[query].append(evidence)

        # Sample evidence to include in prompt
        sampled_evidence = []

        # First, include evidence from each concept
        for concept, evidence_list in evidence_by_concept.items():
            if evidence_list:
                sampled_evidence.append(random.choice(evidence_list))

        # Then, include evidence from different queries if not already included
        for query, evidence_list in evidence_by_query.items():
            if len(sampled_evidence) >= 15:  # Limit to keep prompt size reasonable
                break

            candidate = random.choice(evidence_list)
            if candidate not in sampled_evidence:
                sampled_evidence.append(candidate)

        # Format evidence for prompt
        evidence_str = ""
        for i, evidence in enumerate(sampled_evidence[:15]):  # Limit to 15 pieces
            evidence_str += f"[{i+1}] "
            if "concept" in evidence:
                evidence_str += f"[Concept: {evidence['concept']}] "
            evidence_str += f"{evidence['content'][:250]}...\n\n"

        # Create synthesis prompt
        prompt = f"""
        Synthesize insights about "{session['topic']}" based on the evidence and discovered concepts.

        TOPIC: {session['topic']}

        DISCOVERED CONCEPTS:
        {', '.join(discovered_concepts)}

        EVIDENCE SAMPLES:
        {evidence_str}

        ANALYSIS GOALS:
        {', '.join(session['analysis_goals'])}

        Based on all information gathered, generate:

        1. Key insights about the topic (5-7 insights)
        2. How concepts relate to form a coherent understanding
        3. Areas of consensus across different sources
        4. Areas of disagreement or uncertainty
        5. Implications or applications of these insights
        6. Gaps in the current understanding

        For each insight, provide:
        - A clear statement of the insight
        - Supporting concepts and evidence
        - Confidence level (high, medium, low)
        - Limitations or qualifications

        Return your synthesis as a structured JSON object.
        """

        # Include relationship mapping if available
        if relationship_mapping:
            prompt += f"\n\nRELATIONSHIP MAPPING:\n{json.dumps(relationship_mapping, indent=2)}"

        synthesis_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        try:
            synthesis = json.loads(synthesis_result)

            # Extract insights and add IDs
            if "insights" in synthesis:
                for i, insight in enumerate(synthesis["insights"]):
                    insight["id"] = f"insight_{i}_{int(time.time())}"

                    # Convert text confidence to numeric
                    confidence_text = insight.get("confidence", "medium").lower()
                    if confidence_text == "high":
                        insight["confidence"] = 0.9
                    elif confidence_text == "medium":
                        insight["confidence"] = 0.7
                    elif confidence_text == "low":
                        insight["confidence"] = 0.5
                    else:
                        insight["confidence"] = 0.6

                # Update session insights
                session["insights"] = synthesis["insights"]

            return {
                "status": "success",
                "synthesis": synthesis,
                "insights_generated": len(synthesis.get("insights", [])),
                "should_end_analysis": True
            }

        except:
            return {
                "status": "partial_success",
                "error": "Failed to parse synthesis result",
                "raw_synthesis": synthesis_result[:200],
                "should_end_analysis": True
            }

    async def _synthesize_analysis(self, analysis_id):
        """Create a comprehensive synthesis of the entire analysis."""
        if analysis_id not in self.analysis_memory:
            return {"error": "Analysis session not found"}

        session = self.analysis_memory[analysis_id]

        # Check if we already have insights
        insights = session.get("insights", [])

        # Create synthesis prompt
        prompt = f"""
        Create a comprehensive synthesis of the analysis on "{session['topic']}".

        TOPIC: {session['topic']}

        ANALYSIS GOALS:
        {', '.join(session['analysis_goals'])}

        DISCOVERED CONCEPTS:
        {', '.join(list(session['discovered_concepts'])[:20])}  # Limit to 20

        INSIGHTS:
        """

        # Add insights
        for i, insight in enumerate(insights):
            confidence = insight.get("confidence", 0.7)
            confidence_text = "high" if confidence > 0.8 else "medium" if confidence > 0.5 else "low"

            prompt += f"[{i+1}] {insight.get('statement', 'No statement provided')} "
            prompt += f"(Confidence: {confidence_text})\n"

            # Add support or limitations if available
            if "supporting_evidence" in insight:
                prompt += f"- Support: {insight.get('supporting_evidence', 'Not specified')}\n"
            if "limitations" in insight:
                prompt += f"- Limitations: {insight.get('limitations', 'Not specified')}\n"

            prompt += "\n"

        prompt += """
        Create a comprehensive synthesis that:

        1. Presents a cohesive narrative about the topic
        2. Organizes insights into a logical structure
        3. Highlights key findings and their significance
        4. Addresses areas of certainty and uncertainty
        5. Identifies connections between different concepts
        6. Suggests implications and applications
        7. Recommends areas for further exploration

        The synthesis should integrate all insights into a coherent whole
        while acknowledging limitations in the current understanding.

        Return your synthesis as a structured JSON object.
        """

        synthesis_result = await self.llm_service.generate_completion(
            prompt=prompt,
            temperature=0.4,
            response_format={"type": "json_object"}
        )

        try:
            return json.loads(synthesis_result)
        except:
            # Fallback basic synthesis
            return {
                "narrative": f"Analysis of {session['topic']} revealed several insights but synthesis failed.",
                "key_findings": [i.get("statement", "Unnamed insight") for i in insights[:5]],
                "synthesis_error": "Failed to parse synthesis result"
            }

    def _prepare_analysis_results(self, analysis_id):
        """Prepare final analysis results."""
        if analysis_id not in self.analysis_memory:
            return {"error": "Analysis session not found"}

        session = self.analysis_memory[analysis_id]

        # Calculate execution time
        execution_time = time.time() - session["start_time"]

        # Count unique evidence sources
        evidence_sources = set()
        for evidence in session["evidence"].values():
            if "metadata" in evidence and "document_name" in evidence["metadata"]:
                evidence_sources.add(evidence["metadata"]["document_name"])

        # Prepare results
        results = {
            "analysis_id": analysis_id,
            "topic": session["topic"],
            "analysis_goals": session["analysis_goals"],
            "execution_time": execution_time,
            "concepts_discovered": list(session["discovered_concepts"]),
            "evidence_count": len(session["evidence"]),
            "unique_sources": len(evidence_sources),
            "insights": session.get("insights", []),
            "synthesis": session.get("synthesis", {})
        }

        return results

    def _update_insight_repository(self, topic, results):
        """Update insight repository with key findings."""
        # Clean topic for storage
        clean_topic = topic.lower().strip()

        # Get insights
        insights = results.get("insights", [])
        if not insights:
            return

        # Filter to high-confidence insights
        high_confidence_insights = [
            insight for insight in insights
            if insight.get("confidence", 0) > 0.7 or insight.get("validation_score", 0) > 7
        ]

        # Update repository
        if clean_topic not in self.insight_repository:
            self.insight_repository[clean_topic] = {
                "last_updated": time.time(),
                "insights": high_confidence_insights,
                "related_concepts": results.get("concepts_discovered", [])[:10],
                "analysis_count": 1
            }
        else:
            # Merge with existing insights
            existing = self.insight_repository[clean_topic]

            # Update metadata
            existing["last_updated"] = time.time()
            existing["analysis_count"] += 1

            # Merge concepts
            existing_concepts = set(existing["related_concepts"])
            new_concepts = set(results.get("concepts_discovered", [])[:10])
            existing["related_concepts"] = list(existing_concepts.union(new_concepts))

            # Merge insights (avoid duplicates)
            existing_statements = {i.get("statement") for i in existing["insights"]}
            for insight in high_confidence_insights:
                if insight.get("statement") not in existing_statements:
                    existing["insights"].append(insight)
                    existing_statements.add(insight.get("statement"))
9. System Integration and Application Setup
pythonCopyasync def build_and_initialize_system(input_folder="./input", output_folder="./output"):
    """Build and initialize the complete knowledge corpus research system."""
    import os
    import time
    import asyncio

    # Create output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    print("Initializing resource monitor...")
    resource_monitor = ResourceMonitor()

    print("Initializing LLM service...")
    llm_service = LLMService(resource_monitor=resource_monitor)

    print("Initializing embedding service...")
    embedding_service = EmbeddingService(resource_monitor=resource_monitor)

    # Check if vector store already exists
    vector_store_path = os.path.join(output_folder, "vector_store")
    vector_store_exists = os.path.exists(vector_store_path)

    if not vector_store_exists:
        print("Processing documents...")
        await process_documents(input_folder, output_folder, llm_service, embedding_service, resource_monitor)

    # Load vector database
    print("Loading vector database...")
    vector_db = VectorDatabaseManager(vector_store_path)
    vector_db.initialize()

    # Initialize agent metadata manager
    print("Initializing agent metadata manager...")
    agent_metadata = AgentMetadataManager()

    # Initialize orchestrator
    print("Initializing orchestrator...")
    orchestrator = DynamicOrchestrator(vector_db.db, llm_service, resource_monitor)
    await orchestrator.initialize_system()

    # Initialize parameter optimizer
    print("Initializing parameter optimizer...")
    parameter_optimizer = ParameterOptimizerAgent(llm_service, resource_monitor)

    # Initialize research agent
    print("Initializing research agent...")
    research_agent = EnhancedResearchAgent(vector_db.db, llm_service, resource_monitor)
    await research_agent.initialize()

    # Initialize response generator
    print("Initializing response generator...")
    response_generator = EnhancedResponseGenerator(llm_service)
    await response_generator.initialize()

    # Initialize context engine
    print("Initializing context engine...")
    context_engine = ContextEngine(llm_service)

    # Initialize data analysis agent
    print("Initializing data analysis agent...")
    data_analysis_agent = DataAnalysisAgent(vector_db.db, llm_service)
    await data_analysis_agent.initialize()

    # Register all agents with orchestrator
    print("Registering agents with orchestrator...")
    # (Orchestrator already initializes and registers core agents)

    # Build system object with all components
    system = {
        "orchestrator": orchestrator,
        "vector_db": vector_db,
        "llm_service": llm_service,
        "research_agent": research_agent,
        "response_generator": response_generator,
        "parameter_optimizer": parameter_optimizer,
        "context_engine": context_engine,
        "data_analysis_agent": data_analysis_agent,
        "resource_monitor": resource_monitor,
        "agent_metadata": agent_metadata
    }

    print("System initialization complete!")
    return system

async def process_documents(input_folder, output_folder, llm_service, embedding_service, resource_monitor):
    """Process all documents in the input folder with dynamic analysis and metadata extraction."""
    import os

    start_time = time.time()

    # Initialize metadata extractor
    metadata_extractor = DynamicMetadataExtractor(llm_service)
    await metadata_extractor.initialize()

    # Initialize chunk processor
    chunk_processor = ChunkProcessor(llm_service)

    # Initialize multilevel chunk builder
    chunk_builder = MultilevelChunkBuilder(llm_service)

    # Discover all documents
    documents = []
    for root, _, files in os.walk(input_folder):
        for filename in files:
            if filename.endswith(('.txt', '.pdf', '.html', '.md', '.json', '.csv')):
                file_path = os.path.join(root, filename)
                rel_path = os.path.relpath(file_path, input_folder)

                documents.append({
                    "path": file_path,
                    "rel_path": rel_path,
                    "name": filename
                })

    print(f"Found {len(documents)} documents")

    # Process documents in batches
    batch_size = 5
    all_level0_chunks = []
    document_analyses = []

    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]

        print(f"Processing document batch {i // batch_size + 1}/{(len(documents) + batch_size - 1) // batch_size}")

        # Process each document in the batch
        for doc in batch:
            # Load document content
            try:
                with open(doc["path"], 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                try:
                    with open(doc["path"], 'r', encoding='latin-1') as f:
                        content = f.read()
                except:
                    print(f"Error reading {doc['path']}, skipping")
                    continue

            # Extract metadata
            doc_analysis = await metadata_extractor.extract_document_metadata(doc["path"], content)
            document_analyses.append((doc, doc_analysis))

            # Process document into chunks
            doc_chunks = await chunk_processor.process_document(doc["path"], content, doc_analysis)
            all_level0_chunks.extend(doc_chunks)

            print(f"Created {len(doc_chunks)} Level 0 chunks for {doc['name']}")

        # Allow system to catch up between batches
        await asyncio.sleep(1)

    print(f"Created total of {len(all_level0_chunks)} Level 0 chunks")

    # Get model information for embedding
    embedding_model_info = await embedding_service.get_model_info()

    # Build multilevel chunks
    print("Building multilevel chunks...")
    all_chunks = await chunk_builder.build_multilevel_chunks(
        all_level0_chunks,
        embedding_model_info["max_tokens"]
    )

    # Count total chunks
    total_chunks = sum(len(chunks) for level, chunks in all_chunks.items())
    print(f"Created total of {total_chunks} chunks across {len(all_chunks)} levels")

    # Flatten chunks for embedding
    flat_chunks = []
    for level_chunks in all_chunks.values():
        flat_chunks.extend(level_chunks)

    # Generate embeddings
    print(f"Generating embeddings for {len(flat_chunks)} chunks...")

    # Get system resource state for batch optimization
    resources = resource_monitor.get_current_state()
    optimizations = resource_monitor.optimize_resources()

    embeddings = await embedding_service.embed_documents(
        flat_chunks,
        batch_size=optimizations.get("embedding_batch_size", 500),
        optimize=True
    )

    print(f"Generated {len(embeddings)} embeddings")

    # Initialize vector database
    vector_store_path = os.path.join(output_folder, "vector_store")
    vector_db = VectorDatabaseManager(vector_store_path)
    vector_db.initialize()

    # Convert chunks to documents and add to vector database
    print("Adding documents to vector database...")
    from langchain.schema import Document

    # Prepare documents and embeddings lists
    documents = []
    embeddings_list = []
    metadatas = []
    ids = []

    for chunk in flat_chunks:
        chunk_id = chunk["id"]

        # Skip if we don't have an embedding
        if chunk_id not in embeddings:
            continue

        # Create Document
        doc = Document(
            page_content=chunk["text"],
            metadata=chunk["metadata"]
        )

        documents.append(doc)
        embeddings_list.append(embeddings[chunk_id])
        metadatas.append(chunk["metadata"])
        ids.append(chunk_id)

    # Add documents in batches
    batch_size = optimizations.get("processing_batch_size", 500)
    for i in range(0, len(documents), batch_size):
        end_idx = min(i + batch_size, len(documents))

        vector_db.add_documents(
            documents=documents[i:end_idx],
            embeddings=embeddings_list[i:end_idx]
        )

        print(f"Added {end_idx}/{len(documents)} documents to vector database")

    # Record completion time
    end_time = time.time()
    print(f"Document processing completed in {end_time - start_time:.2f} seconds")
10. Web Interface and API
pythonCopyasync def create_web_interface():
    """Create a web interface and API for the knowledge corpus system."""
    from fastapi import FastAPI, WebSocket, HTTPException, Request, Depends
    from fastapi.responses import HTMLResponse, JSONResponse
    from fastapi.staticfiles import StaticFiles
    from fastapi.templating import Jinja2Templates
    import uvicorn
    import uuid
    import json

    # Initialize the system
    system = await build_and_initialize_system()

    # Extract components
    orchestrator = system["orchestrator"]
    data_analysis_agent = system["data_analysis_agent"]

    # Create FastAPI app
    app = FastAPI(title="Knowledge Corpus Research Assistant")

    # Mount static files
    app.mount("/static", StaticFiles(directory="static"), name="static")

    # Setup templates
    templates = Jinja2Templates(directory="templates")

    # Active sessions
    active_sessions = {}

    # Routes
    @app.get("/", response_class=HTMLResponse)
    async def get_home(request: Request):
        return templates.TemplateResponse("index.html", {"request": request})

    @app.get("/analyze", response_class=HTMLResponse)
    async def get_analyze(request: Request):
        return templates.TemplateResponse("analyze.html", {"request": request})

    @app.post("/api/session")
    async def create_session():
        session_id = str(uuid.uuid4())
        active_sessions[session_id] = {
            "id": session_id,
            "created_at": time.time(),
            "last_activity": time.time(),
            "conversation": []
        }

        # Initialize context for this session
        await system["context_engine"].initialize_session(session_id)

        return {"session_id": session_id}

    @app.post("/api/query")
    async def process_query(request: Request):
        data = await request.json()
        session_id = data.get("session_id")
        query = data.get("query")

        if not session_id or not query:
            raise HTTPException(status_code=400, detail="Missing session_id or query")

        if session_id not in active_sessions:
            raise HTTPException(status_code=404, detail="Session not found")

        # Update session activity
        active_sessions[session_id]["last_activity"] = time.time()

        # Process query
        try:
            response = await orchestrator.process_query(query, session_id)

            # Update conversation
            active_sessions[session_id]["conversation"].append({
                "role": "user",
                "content": query
            })
            active_sessions[session_id]["conversation"].append({
                "role": "assistant",
                "content": response
            })

            return {
                "response": response,
                "session_id": session_id
            }

        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")

    @app.post("/api/analyze")
    async def analyze_topic(request: Request):
        data = await request.json()
        topic = data.get("topic")
        goals = data.get("goals", ["concept_mapping"])

        if not topic:
            raise HTTPException(status_code=400RetryClaude hit the max length for a message and has paused its response. You can write Continue to keep the chat going.EScontinueEdit10. Web Interface and API (continued)
pythonCopy    @app.post("/api/analyze")
    async def analyze_topic(request: Request):
        data = await request.json()
        topic = data.get("topic")
        goals = data.get("goals", ["concept_mapping"])

        if not topic:
            raise HTTPException(status_code=400, detail="Missing topic")

        # Process analysis request
        try:
            analysis_results = await data_analysis_agent.analyze_topic(topic, goals)

            return {
                "results": analysis_results,
                "topic": topic
            }

        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error analyzing topic: {str(e)}")

    @app.websocket("/chat")
    async def websocket_endpoint(websocket: WebSocket):
        await websocket.accept()

        # Initialize session
        session_id = str(uuid.uuid4())
        active_sessions[session_id] = {
            "id": session_id,
            "created_at": time.time(),
            "last_activity": time.time(),
            "conversation": []
        }

        # Initialize context for this session
        await system["context_engine"].initialize_session(session_id)

        # Send session initialization confirmation
        await websocket.send_json({
            "type": "session_init",
            "session_id": session_id
        })

        try:
            while True:
                # Receive message
                message = await websocket.receive_text()
                data = json.loads(message)

                if data["type"] == "query":
                    # Process query
                    query = data["content"]

                    # Update session activity
                    active_sessions[session_id]["last_activity"] = time.time()

                    # Send acknowledgment
                    await websocket.send_json({
                        "type": "status",
                        "content": "Processing your question..."
                    })

                    # Process with orchestrator
                    response = await orchestrator.process_query(query, session_id)

                    # Update conversation
                    active_sessions[session_id]["conversation"].append({
                        "role": "user",
                        "content": query
                    })
                    active_sessions[session_id]["conversation"].append({
                        "role": "assistant",
                        "content": response
                    })

                    # Send response
                    await websocket.send_json({
                        "type": "response",
                        "content": response
                    })

                elif data["type"] == "analyze":
                    # Process analysis request
                    topic = data["topic"]
                    goals = data.get("goals", ["concept_mapping"])

                    # Send acknowledgment
                    await websocket.send_json({
                        "type": "status",
                        "content": f"Analyzing topic: {topic}..."
                    })

                    # Process with data analysis agent
                    analysis_results = await data_analysis_agent.analyze_topic(topic, goals)

                    # Send results
                    await websocket.send_json({
                        "type": "analysis_results",
                        "results": analysis_results
                    })

                elif data["type"] == "reset":
                    # Reset conversation
                    active_sessions[session_id]["conversation"] = []

                    # Re-initialize context
                    await system["context_engine"].initialize_session(session_id)

                    await websocket.send_json({
                        "type": "status",
                        "content": "Conversation reset."
                    })

        except Exception as e:
            print(f"Error in websocket: {e}")
            try:
                await websocket.send_json({
                    "type": "error",
                    "content": f"An error occurred: {str(e)}"
                })
            except:
                pass
            finally:
                await websocket.close()

    @app.on_event("startup")
    async def startup_event():
        # Clean up inactive sessions periodically
        asyncio.create_task(clean_inactive_sessions())

    async def clean_inactive_sessions():
        """Periodically clean up inactive sessions."""
        while True:
            await asyncio.sleep(3600)  # Check every hour

            current_time = time.time()
            sessions_to_remove = []

            for session_id, session in active_sessions.items():
                # Remove sessions inactive for more than 24 hours
                if current_time - session["last_activity"] > 86400:
                    sessions_to_remove.append(session_id)

            for session_id in sessions_to_remove:
                del active_sessions[session_id]

            print(f"Cleaned up {len(sessions_to_remove)} inactive sessions")

    # Return the FastAPI app for use with uvicorn
    return app

async def main():
    """Main entry point for the application."""
    # Create the web interface
    app = await create_web_interface()

    # Start the server
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
11. Key System Features and Advantages
11.1 Complete Dynamic Parameter Control
Every aspect of the system is dynamically controlled by intelligent agents:

Self-Configuring Agents

Each agent generates its own operational parameters
Agents understand their roles and capabilities through LLM-generated self-descriptions
Parameter boundaries and trade-offs are explicitly modeled


Dynamic Workflow Selection

Query analysis determines optimal processing workflows
Workflow steps adjust based on initial results
Decision points allow for processing path changes


Resource-Aware Operation

All components monitor and adapt to available resources
Batch sizes adjust based on CPU/memory availability
API rate limit awareness prevents throttling


Hierarchical Parameter Optimization

Global parameters managed by Orchestrator
Domain-specific parameters managed by specialized agents
Context-specific parameters fine-tuned for each query



11.2 Sophisticated Information Retrieval

Multi-Strategy Search

Different search strategies for different query types
Progressive refinement for complex information needs
Dynamic metadata filtering based on query context


Context-Aware Knowledge Management

Conversation and knowledge context balanced dynamically
Relevance-based knowledge prioritization
Context compression for efficient token usage


Self-Directed Research

Iterative deepening for complex topics
Automatic query reformulation based on information gaps
Cross-referencing for verification



11.3 Advanced Document Processing

Content-Aware Chunking

Document structure analysis determines optimal chunking
Natural boundaries respected through LLM guidance
Multi-level chunk hierarchy for comprehensive context


Rich Metadata Extraction

LLM-powered analysis of document content
Domain-specific schema generation
Concept and entity recognition at both document and chunk level


Document Relationship Mapping

Cross-document concept linking
Automatic relationship type detection
Structured knowledge representation



11.4 Advanced Analysis Capabilities

Topic Exploration

Structured concept discovery
Relationship mapping between concepts
Insight synthesis across documents


Self-Verification

Evidence gathering for factual claims
Source consistency checking
Confidence scoring for generated information


Narrative Construction

Coherent organization of discovered information
Gap identification and acknowledgment
Balanced presentation of perspectives



11.5 Self-Improvement Mechanisms

Learning from Interactions

Strategy effectiveness tracking
Parameter adjustment based on performance
Template refinement from user feedback


Continuous Adaptation

Query pattern recognition
Domain specialization through continued use
Insight repository for faster future responses


Robust Error Handling

Fallback mechanisms at multiple levels
Graceful degradation under resource constraints
Alternative processing paths when optimal routes fail



12. Implementation and Deployment Considerations
12.1 Resource Requirements

Compute:

8+ CPU cores recommended
16+ GB RAM for medium corpus size
GPU beneficial but not required


Storage:

Vector database scales with corpus size
Approximately 1GB per 100,000 chunks
SSD recommended for vector operations


API Usage:

LLM API costs scale with query volume and complexity
Embedding API costs scale with corpus size
Rate limiting mechanisms prevent unexpected costs



12.2 Scalability Considerations

Horizontal Scaling:

Document processing pipeline can scale horizontally
Multiple instances for concurrent user support
Shared vector database for consistency


Vertical Scaling:

Memory requirements increase with corpus size
Context length requirements increase with complex queries
Parameter optimization reduces resource needs


Caching Strategies:

Frequent query pattern recognition
Common request result caching
Pre-emptive insight generation



12.3 Maintenance and Updates

Corpus Expansion:

Incremental document processing
Background embedding generation
Automatic schema adaptation


LLM Model Updates:

Model-agnostic architecture
Smooth transition between model versions
Backward compatibility for stored parameters


Performance Monitoring:

Comprehensive logging of key performance indicators
Automated detection of degraded performance
Self-healing through parameter adjustment



13. Conclusion
This comprehensive system architecture provides a dynamic, self-configuring research assistant capable of working with any type of knowledge corpus. By leveraging LLM capabilities for parameter selection, metadata extraction, and intelligent processing, the system demonstrates human-like research capabilities while maintaining computational efficiency.
The system's flexibility comes from its completely dynamic architecture, where every component can adapt its behavior based on the specific query, content characteristics, and available resources. This self-directed operation allows it to handle diverse knowledge domains without specialized programming for each case.
The hierarchical organization of intelligence - from low-level document processing to high-level insight synthesis - creates a robust system that can tackle complex research tasks while providing transparent explanations of its findings. The self-verification and improvement mechanisms ensure that the system becomes more effective over time, learning from each interaction to better serve future research needs.